{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e69a11",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-16T06:48:19.859339Z",
     "iopub.status.busy": "2022-06-16T06:48:19.858919Z",
     "iopub.status.idle": "2022-06-16T06:48:19.869546Z",
     "shell.execute_reply": "2022-06-16T06:48:19.868847Z"
    },
    "papermill": {
     "duration": 0.024347,
     "end_time": "2022-06-16T06:48:19.871395",
     "exception": false,
     "start_time": "2022-06-16T06:48:19.847048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#os.remove('/kaggle/working/datasets/cityscapes.pth')\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#       print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18538a28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-16T06:48:19.890383Z",
     "iopub.status.busy": "2022-06-16T06:48:19.890187Z",
     "iopub.status.idle": "2022-06-16T06:48:21.752328Z",
     "shell.execute_reply": "2022-06-16T06:48:21.751581Z"
    },
    "papermill": {
     "duration": 1.874299,
     "end_time": "2022-06-16T06:48:21.754810",
     "exception": false,
     "start_time": "2022-06-16T06:48:19.880511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as functional\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d47af6ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-16T06:48:21.774304Z",
     "iopub.status.busy": "2022-06-16T06:48:21.774102Z",
     "iopub.status.idle": "2022-06-16T06:48:22.003195Z",
     "shell.execute_reply": "2022-06-16T06:48:22.002336Z"
    },
    "papermill": {
     "duration": 0.241202,
     "end_time": "2022-06-16T06:48:22.005322",
     "exception": false,
     "start_time": "2022-06-16T06:48:21.764120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = glob('/kaggle/input/cityscapes-image-pairs/cityscapes_data/train/*')\n",
    "valid_path = glob('/kaggle/input/cityscapes-image-pairs/cityscapes_data/val/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790c7051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-16T06:48:22.025724Z",
     "iopub.status.busy": "2022-06-16T06:48:22.025123Z",
     "iopub.status.idle": "2022-06-16T06:48:22.032390Z",
     "shell.execute_reply": "2022-06-16T06:48:22.031758Z"
    },
    "papermill": {
     "duration": 0.018693,
     "end_time": "2022-06-16T06:48:22.033953",
     "exception": false,
     "start_time": "2022-06-16T06:48:22.015260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cityscapes(Dataset):\n",
    "    def __init__(self, data_path, transform=None, target_transform=None):\n",
    "        super(Cityscapes, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        #self.datasets = np.array(data)\n",
    "        #self.images, self.targets = np.array_split(self.datasets, 2, axis=2)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image_pair = plt.imread(self.data_path[item])\n",
    "        image, target = image_pair[:, :int(image_pair.shape[1] / 2)], image_pair[:, int(image_pair.shape[1] / 2):]\n",
    "        #image = self.images[item]\n",
    "        #target = self.targets[item]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f84b70d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-16T06:48:22.053361Z",
     "iopub.status.busy": "2022-06-16T06:48:22.052909Z",
     "iopub.status.idle": "2022-06-16T06:48:22.077868Z",
     "shell.execute_reply": "2022-06-16T06:48:22.077224Z"
    },
    "papermill": {
     "duration": 0.036501,
     "end_time": "2022-06-16T06:48:22.079482",
     "exception": false,
     "start_time": "2022-06-16T06:48:22.042981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, image_channel=3, mid_channel=64):\n",
    "        super(UNet, self).__init__()\n",
    "        self.two_conv_block = nn.Sequential(\n",
    "            nn.Conv2d(image_channel, mid_channel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channel, mid_channel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.down_sample_1 = DownSampleConvBlock(mid_channel, mid_channel * 2, mid_channel * 2)\n",
    "        self.down_sample_2 = DownSampleConvBlock(mid_channel * 2, mid_channel * 4, mid_channel * 4)\n",
    "        self.down_sample_3 = DownSampleConvBlock(mid_channel * 4, mid_channel * 8, mid_channel * 8)\n",
    "        self.down_sample_4 = DownSampleConvBlock(mid_channel * 8, mid_channel * 16, mid_channel * 16)\n",
    "        self.up_sample_1 = UpSampleConvBlock(mid_channel * 16, mid_channel * 8, mid_channel * 8)\n",
    "        self.up_sample_2 = UpSampleConvBlock(mid_channel * 8, mid_channel * 4, mid_channel * 4)\n",
    "        self.up_sample_3 = UpSampleConvBlock(mid_channel * 4, mid_channel * 2, mid_channel * 2)\n",
    "        self.up_sample_4 = UpSampleConvBlock(mid_channel * 2, mid_channel, mid_channel)\n",
    "        # 降维\n",
    "        self.conv1x1 = nn.Conv2d(mid_channel, image_channel, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(image_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.two_conv_block(x)\n",
    "        x2 = self.down_sample_1(x1)\n",
    "        x3 = self.down_sample_2(x2)\n",
    "        x4 = self.down_sample_3(x3)\n",
    "        x5 = self.down_sample_4(x4)\n",
    "        x_u1 = self.up_sample_1(x5, x4)\n",
    "        x_u2 = self.up_sample_2(x_u1, x3)\n",
    "        x_u3 = self.up_sample_3(x_u2, x2)\n",
    "        x_u4 = self.up_sample_4(x_u3, x1)\n",
    "        return self.bn(self.conv1x1(x_u4))\n",
    "\n",
    "\n",
    "class BasicConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1):\n",
    "        super(BasicConvBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # the first conv block\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size, stride, padding, dilation),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # the second conv block\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size, stride, padding, dilation),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class DownSampleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, pool_size=2, kernel_size=3, stride=1, padding=1,\n",
    "                 dilation=1):\n",
    "        super(DownSampleConvBlock, self).__init__()\n",
    "        self.down_sample = nn.MaxPool2d(kernel_size=pool_size)\n",
    "        self.basic_conv_block = BasicConvBlock(in_channels, mid_channels, out_channels, kernel_size, stride, padding,\n",
    "                                               dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # down sample\n",
    "        x = self.down_sample(x)\n",
    "        # two conv block\n",
    "        x = self.basic_conv_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpSampleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, up_size=2, kernel_size=3, stride=1, padding=1,\n",
    "                 dilation=1):\n",
    "        super(UpSampleConvBlock, self).__init__()\n",
    "        self.up_sample = nn.ConvTranspose2d(in_channels, mid_channels, kernel_size=up_size, stride=up_size,\n",
    "                                            dilation=dilation)\n",
    "        self.basic_conv_block = BasicConvBlock(in_channels, mid_channels, out_channels, kernel_size, stride, padding,\n",
    "                                               dilation)\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        # up sample\n",
    "        x = self.up_sample(x)\n",
    "        # concat x and skip_x in the dimension of channel\n",
    "        x = torch.cat([x, skip_x], dim=1)\n",
    "        # two conv block\n",
    "        x = self.basic_conv_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AdaptiveFeatureFusionModule(nn.Module):\n",
    "    \"\"\"Adaptive Feature Fusion Module(AFFM)\n",
    "\n",
    "    Fusion multiple-scale feature maps, the count of feature maps is not fixed,\n",
    "    the value of counts must equal the size of feature_maps, the number of layers\n",
    "    in AFFM is determined by the parameter of counts.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counts):\n",
    "        super(AdaptiveFeatureFusionModule, self).__init__()\n",
    "        self.counts = counts\n",
    "        pass\n",
    "\n",
    "    def forward(self, feature_maps: tuple = None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b18a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-16T06:48:22.098612Z",
     "iopub.status.busy": "2022-06-16T06:48:22.098412Z",
     "iopub.status.idle": "2022-06-16T06:48:22.109429Z",
     "shell.execute_reply": "2022-06-16T06:48:22.108816Z"
    },
    "papermill": {
     "duration": 0.02288,
     "end_time": "2022-06-16T06:48:22.111233",
     "exception": false,
     "start_time": "2022-06-16T06:48:22.088353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dice_coeff(predict, target, reduce_batch_first=False, epsilon=1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert predict.size() == target.size()\n",
    "    if predict.dim() == 2 and reduce_batch_first:\n",
    "        raise ValueError(f'Dice: asked to reduce batch but got tensor without batch dimension (shape {predict.shape})')\n",
    "\n",
    "    if predict.dim() == 2 or reduce_batch_first:\n",
    "        inter = torch.dot(predict.reshape(-1), target.reshape(-1))\n",
    "        sets_sum = torch.sum(predict) + torch.sum(target)\n",
    "        if sets_sum.item() == 0:\n",
    "            sets_sum = 2 * inter\n",
    "        return (2 * inter + epsilon) / (sets_sum + epsilon)\n",
    "    else:\n",
    "        # compute and average metric for each batch element\n",
    "        dice = 0\n",
    "        for i in range(predict.shape[0]):\n",
    "            dice += dice_coeff(predict[i, ...], target[i, ...])\n",
    "        # return average dice loss value of a batch\n",
    "        return dice / predict.shape[0]\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(predict, target, reduce_batch_first=False, epsilon=1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    assert predict.size() == target.size()\n",
    "    dice = 0\n",
    "    for channel in range(predict.shape[1]):\n",
    "        dice += dice_coeff(predict[:, channel, ...], target[:, channel, ...], reduce_batch_first, epsilon)\n",
    "    return dice / predict.shape[1]\n",
    "\n",
    "\n",
    "def dice_loss(predict, target, multiclass=True, epsilon=1e-6):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    assert predict.size() == target.size()\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(predict, target, reduce_batch_first=True, epsilon=epsilon)\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, ep=1e-8):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.ep = ep\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        # the shape of predict must equal to the shape of target\n",
    "        value = dice_loss(predict, target, True, self.ep)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f7a6f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-16T06:48:22.130481Z",
     "iopub.status.busy": "2022-06-16T06:48:22.130274Z",
     "iopub.status.idle": "2022-06-16T06:48:22.138123Z",
     "shell.execute_reply": "2022-06-16T06:48:22.137436Z"
    },
    "papermill": {
     "duration": 0.019444,
     "end_time": "2022-06-16T06:48:22.139699",
     "exception": false,
     "start_time": "2022-06-16T06:48:22.120255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(network_model, valid_loader, loss, device):\n",
    "    network_model.eval()\n",
    "    v_loss_total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for j, (v_x, v_l) in enumerate(valid_loader):\n",
    "            v_x = v_x.to(device)\n",
    "            v_l = v_l.to(device)\n",
    "            v_predict = network_model(v_x)\n",
    "            loss_value = loss(v_predict, v_l)\n",
    "            v_loss_total += loss_value.item()\n",
    "    val_avg_loss = v_loss_total / len(valid_loader)\n",
    "    return val_avg_loss\n",
    "\n",
    "\n",
    "class SearchBestModel(object):\n",
    "    def __init__(self, min_delta=0, verbose=True):\n",
    "        super(SearchBestModel, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "        elif self.best_score - val_loss >= self.min_delta:\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print('performance reducing: counter {}'.format(self.counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80612b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-16T06:48:22.158609Z",
     "iopub.status.busy": "2022-06-16T06:48:22.158198Z",
     "iopub.status.idle": "2022-06-16T06:48:22.168683Z",
     "shell.execute_reply": "2022-06-16T06:48:22.168036Z"
    },
    "papermill": {
     "duration": 0.02194,
     "end_time": "2022-06-16T06:48:22.170291",
     "exception": false,
     "start_time": "2022-06-16T06:48:22.148351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, optimizer, loss, epoch, device):\n",
    "    loss_change_list = []\n",
    "    valid_loss_change = []\n",
    "    save_best = {}\n",
    "    save_last = {}\n",
    "    search_best_model = SearchBestModel()\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for index, (image, label) in enumerate(train_loader):\n",
    "            image = image.to(device)\n",
    "            label = label.to(device).to(torch.float32)\n",
    "\n",
    "            segment_mask = model(image)\n",
    "            loss_value = loss(segment_mask, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss = total_loss + loss_value.item()\n",
    "\n",
    "            print('epoch {} batch {}/{} loss = {:.4f}'.format(i + 1, index + 1, len(train_loader), loss_value.item()))\n",
    "        # save train loss change history used for model analyse\n",
    "        loss_change_list.append(total_loss / len(train_loader))\n",
    "        # use the dataset for validation to validate the trained model\n",
    "        valid_avg_loss = validate(model, valid_loader, loss, device)\n",
    "        # save valid loss change history used for model analyse\n",
    "        valid_loss_change.append(valid_avg_loss)\n",
    "\n",
    "        print('epoch {} train loss = {:.4f} valid loss = {:.4f}'.format(i + 1, total_loss / len(train_loader), valid_avg_loss))\n",
    "\n",
    "        # see if satisfy the conditions of early stopping\n",
    "        search_best_model(valid_avg_loss)\n",
    "        # if satisfy the conditions of early stopping, break the training process\n",
    "        if search_best_model.counter > 0:\n",
    "            continue\n",
    "        # if not satisfy the conditions of early stopping, it shows that\n",
    "        # the model in this epoch is the best, save the params of current model.\n",
    "        save_best['model_state_dict'] = model.state_dict()\n",
    "        # save optimizer used for re-train\n",
    "        save_best['optimizer_state_dict'] = optimizer.state_dict()\n",
    "        # save the epoch of current best model\n",
    "        save_best['epoch'] = i\n",
    "    # save loss change history of training and validation\n",
    "    save_last['train_loss_change'] = loss_change_list\n",
    "    save_last['valid_loss_change'] = valid_loss_change\n",
    "    save_last['model_state_dict'] = model.state_dict()\n",
    "    save_last['optimizer_state_dict'] = optimizer.state_dict()\n",
    "    save_last['trained_epoch'] = epoch\n",
    "    torch.save(save_best, './unet-best.pth')\n",
    "    torch.save(save_last, './unet-last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591b839f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-16T06:48:22.189141Z",
     "iopub.status.busy": "2022-06-16T06:48:22.188565Z",
     "iopub.status.idle": "2022-06-16T07:10:08.240986Z",
     "shell.execute_reply": "2022-06-16T07:10:08.240093Z"
    },
    "papermill": {
     "duration": 1306.064617,
     "end_time": "2022-06-16T07:10:08.243530",
     "exception": false,
     "start_time": "2022-06-16T06:48:22.178913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch 1/185 loss = 1.1917\n",
      "epoch 1 batch 2/185 loss = 1.0670\n",
      "epoch 1 batch 3/185 loss = 1.0306\n",
      "epoch 1 batch 4/185 loss = 1.0131\n",
      "epoch 1 batch 5/185 loss = 0.9826\n",
      "epoch 1 batch 6/185 loss = 0.9560\n",
      "epoch 1 batch 7/185 loss = 0.9866\n",
      "epoch 1 batch 8/185 loss = 0.9288\n",
      "epoch 1 batch 9/185 loss = 0.9292\n",
      "epoch 1 batch 10/185 loss = 0.9151\n",
      "epoch 1 batch 11/185 loss = 0.9006\n",
      "epoch 1 batch 12/185 loss = 0.9042\n",
      "epoch 1 batch 13/185 loss = 0.9168\n",
      "epoch 1 batch 14/185 loss = 0.8970\n",
      "epoch 1 batch 15/185 loss = 0.8938\n",
      "epoch 1 batch 16/185 loss = 0.8932\n",
      "epoch 1 batch 17/185 loss = 0.8776\n",
      "epoch 1 batch 18/185 loss = 0.8792\n",
      "epoch 1 batch 19/185 loss = 0.8673\n",
      "epoch 1 batch 20/185 loss = 0.8793\n",
      "epoch 1 batch 21/185 loss = 0.8607\n",
      "epoch 1 batch 22/185 loss = 0.8687\n",
      "epoch 1 batch 23/185 loss = 0.8646\n",
      "epoch 1 batch 24/185 loss = 0.8374\n",
      "epoch 1 batch 25/185 loss = 0.8632\n",
      "epoch 1 batch 26/185 loss = 0.8405\n",
      "epoch 1 batch 27/185 loss = 0.8315\n",
      "epoch 1 batch 28/185 loss = 0.8346\n",
      "epoch 1 batch 29/185 loss = 0.8359\n",
      "epoch 1 batch 30/185 loss = 0.8170\n",
      "epoch 1 batch 31/185 loss = 0.8306\n",
      "epoch 1 batch 32/185 loss = 0.8078\n",
      "epoch 1 batch 33/185 loss = 0.8263\n",
      "epoch 1 batch 34/185 loss = 0.8164\n",
      "epoch 1 batch 35/185 loss = 0.8280\n",
      "epoch 1 batch 36/185 loss = 0.7983\n",
      "epoch 1 batch 37/185 loss = 0.8005\n",
      "epoch 1 batch 38/185 loss = 0.8037\n",
      "epoch 1 batch 39/185 loss = 0.7980\n",
      "epoch 1 batch 40/185 loss = 0.8011\n",
      "epoch 1 batch 41/185 loss = 0.7961\n",
      "epoch 1 batch 42/185 loss = 0.7858\n",
      "epoch 1 batch 43/185 loss = 0.7884\n",
      "epoch 1 batch 44/185 loss = 0.7982\n",
      "epoch 1 batch 45/185 loss = 0.7729\n",
      "epoch 1 batch 46/185 loss = 0.7658\n",
      "epoch 1 batch 47/185 loss = 0.7905\n",
      "epoch 1 batch 48/185 loss = 0.7780\n",
      "epoch 1 batch 49/185 loss = 0.7788\n",
      "epoch 1 batch 50/185 loss = 0.7674\n",
      "epoch 1 batch 51/185 loss = 0.7726\n",
      "epoch 1 batch 52/185 loss = 0.7609\n",
      "epoch 1 batch 53/185 loss = 0.7610\n",
      "epoch 1 batch 54/185 loss = 0.7687\n",
      "epoch 1 batch 55/185 loss = 0.7580\n",
      "epoch 1 batch 56/185 loss = 0.7543\n",
      "epoch 1 batch 57/185 loss = 0.7502\n",
      "epoch 1 batch 58/185 loss = 0.7600\n",
      "epoch 1 batch 59/185 loss = 0.7450\n",
      "epoch 1 batch 60/185 loss = 0.7437\n",
      "epoch 1 batch 61/185 loss = 0.7467\n",
      "epoch 1 batch 62/185 loss = 0.7510\n",
      "epoch 1 batch 63/185 loss = 0.7600\n",
      "epoch 1 batch 64/185 loss = 0.7144\n",
      "epoch 1 batch 65/185 loss = 0.7218\n",
      "epoch 1 batch 66/185 loss = 0.7451\n",
      "epoch 1 batch 67/185 loss = 0.7202\n",
      "epoch 1 batch 68/185 loss = 0.7268\n",
      "epoch 1 batch 69/185 loss = 0.7130\n",
      "epoch 1 batch 70/185 loss = 0.7109\n",
      "epoch 1 batch 71/185 loss = 0.7080\n",
      "epoch 1 batch 72/185 loss = 0.7331\n",
      "epoch 1 batch 73/185 loss = 0.7208\n",
      "epoch 1 batch 74/185 loss = 0.7107\n",
      "epoch 1 batch 75/185 loss = 0.7167\n",
      "epoch 1 batch 76/185 loss = 0.7047\n",
      "epoch 1 batch 77/185 loss = 0.6974\n",
      "epoch 1 batch 78/185 loss = 0.7011\n",
      "epoch 1 batch 79/185 loss = 0.6877\n",
      "epoch 1 batch 80/185 loss = 0.6926\n",
      "epoch 1 batch 81/185 loss = 0.6924\n",
      "epoch 1 batch 82/185 loss = 0.6754\n",
      "epoch 1 batch 83/185 loss = 0.6990\n",
      "epoch 1 batch 84/185 loss = 0.6929\n",
      "epoch 1 batch 85/185 loss = 0.6865\n",
      "epoch 1 batch 86/185 loss = 0.6747\n",
      "epoch 1 batch 87/185 loss = 0.6868\n",
      "epoch 1 batch 88/185 loss = 0.6824\n",
      "epoch 1 batch 89/185 loss = 0.6766\n",
      "epoch 1 batch 90/185 loss = 0.6671\n",
      "epoch 1 batch 91/185 loss = 0.6921\n",
      "epoch 1 batch 92/185 loss = 0.6745\n",
      "epoch 1 batch 93/185 loss = 0.6823\n",
      "epoch 1 batch 94/185 loss = 0.6615\n",
      "epoch 1 batch 95/185 loss = 0.6719\n",
      "epoch 1 batch 96/185 loss = 0.6615\n",
      "epoch 1 batch 97/185 loss = 0.6708\n",
      "epoch 1 batch 98/185 loss = 0.6607\n",
      "epoch 1 batch 99/185 loss = 0.6569\n",
      "epoch 1 batch 100/185 loss = 0.6670\n",
      "epoch 1 batch 101/185 loss = 0.6445\n",
      "epoch 1 batch 102/185 loss = 0.6446\n",
      "epoch 1 batch 103/185 loss = 0.6441\n",
      "epoch 1 batch 104/185 loss = 0.6465\n",
      "epoch 1 batch 105/185 loss = 0.6536\n",
      "epoch 1 batch 106/185 loss = 0.6554\n",
      "epoch 1 batch 107/185 loss = 0.6512\n",
      "epoch 1 batch 108/185 loss = 0.6443\n",
      "epoch 1 batch 109/185 loss = 0.6237\n",
      "epoch 1 batch 110/185 loss = 0.6481\n",
      "epoch 1 batch 111/185 loss = 0.6583\n",
      "epoch 1 batch 112/185 loss = 0.6354\n",
      "epoch 1 batch 113/185 loss = 0.6308\n",
      "epoch 1 batch 114/185 loss = 0.6382\n",
      "epoch 1 batch 115/185 loss = 0.6222\n",
      "epoch 1 batch 116/185 loss = 0.6152\n",
      "epoch 1 batch 117/185 loss = 0.6286\n",
      "epoch 1 batch 118/185 loss = 0.6290\n",
      "epoch 1 batch 119/185 loss = 0.6291\n",
      "epoch 1 batch 120/185 loss = 0.6207\n",
      "epoch 1 batch 121/185 loss = 0.6063\n",
      "epoch 1 batch 122/185 loss = 0.6094\n",
      "epoch 1 batch 123/185 loss = 0.6035\n",
      "epoch 1 batch 124/185 loss = 0.6013\n",
      "epoch 1 batch 125/185 loss = 0.5948\n",
      "epoch 1 batch 126/185 loss = 0.6233\n",
      "epoch 1 batch 127/185 loss = 0.6064\n",
      "epoch 1 batch 128/185 loss = 0.6148\n",
      "epoch 1 batch 129/185 loss = 0.5946\n",
      "epoch 1 batch 130/185 loss = 0.5923\n",
      "epoch 1 batch 131/185 loss = 0.6048\n",
      "epoch 1 batch 132/185 loss = 0.5807\n",
      "epoch 1 batch 133/185 loss = 0.5780\n",
      "epoch 1 batch 134/185 loss = 0.5942\n",
      "epoch 1 batch 135/185 loss = 0.5739\n",
      "epoch 1 batch 136/185 loss = 0.5706\n",
      "epoch 1 batch 137/185 loss = 0.5853\n",
      "epoch 1 batch 138/185 loss = 0.5807\n",
      "epoch 1 batch 139/185 loss = 0.5758\n",
      "epoch 1 batch 140/185 loss = 0.5946\n",
      "epoch 1 batch 141/185 loss = 0.5703\n",
      "epoch 1 batch 142/185 loss = 0.5831\n",
      "epoch 1 batch 143/185 loss = 0.5696\n",
      "epoch 1 batch 144/185 loss = 0.5707\n",
      "epoch 1 batch 145/185 loss = 0.5811\n",
      "epoch 1 batch 146/185 loss = 0.5716\n",
      "epoch 1 batch 147/185 loss = 0.5830\n",
      "epoch 1 batch 148/185 loss = 0.5550\n",
      "epoch 1 batch 149/185 loss = 0.5521\n",
      "epoch 1 batch 150/185 loss = 0.5589\n",
      "epoch 1 batch 151/185 loss = 0.5631\n",
      "epoch 1 batch 152/185 loss = 0.5614\n",
      "epoch 1 batch 153/185 loss = 0.5501\n",
      "epoch 1 batch 154/185 loss = 0.5528\n",
      "epoch 1 batch 155/185 loss = 0.5573\n",
      "epoch 1 batch 156/185 loss = 0.5432\n",
      "epoch 1 batch 157/185 loss = 0.5398\n",
      "epoch 1 batch 158/185 loss = 0.5351\n",
      "epoch 1 batch 159/185 loss = 0.5407\n",
      "epoch 1 batch 160/185 loss = 0.5481\n",
      "epoch 1 batch 161/185 loss = 0.5603\n",
      "epoch 1 batch 162/185 loss = 0.5319\n",
      "epoch 1 batch 163/185 loss = 0.5396\n",
      "epoch 1 batch 164/185 loss = 0.5363\n",
      "epoch 1 batch 165/185 loss = 0.5209\n",
      "epoch 1 batch 166/185 loss = 0.5388\n",
      "epoch 1 batch 167/185 loss = 0.5259\n",
      "epoch 1 batch 168/185 loss = 0.5174\n",
      "epoch 1 batch 169/185 loss = 0.5282\n",
      "epoch 1 batch 170/185 loss = 0.5305\n",
      "epoch 1 batch 171/185 loss = 0.5489\n",
      "epoch 1 batch 172/185 loss = 0.5267\n",
      "epoch 1 batch 173/185 loss = 0.5310\n",
      "epoch 1 batch 174/185 loss = 0.5266\n",
      "epoch 1 batch 175/185 loss = 0.5125\n",
      "epoch 1 batch 176/185 loss = 0.5176\n",
      "epoch 1 batch 177/185 loss = 0.5256\n",
      "epoch 1 batch 178/185 loss = 0.5135\n",
      "epoch 1 batch 179/185 loss = 0.5050\n",
      "epoch 1 batch 180/185 loss = 0.4940\n",
      "epoch 1 batch 181/185 loss = 0.5232\n",
      "epoch 1 batch 182/185 loss = 0.5050\n",
      "epoch 1 batch 183/185 loss = 0.5110\n",
      "epoch 1 batch 184/185 loss = 0.5160\n",
      "epoch 1 batch 185/185 loss = 0.4962\n",
      "epoch 1 train loss = 0.6903 valid loss = 0.5523\n",
      "epoch 2 batch 1/185 loss = 0.5026\n",
      "epoch 2 batch 2/185 loss = 0.4987\n",
      "epoch 2 batch 3/185 loss = 0.5054\n",
      "epoch 2 batch 4/185 loss = 0.4833\n",
      "epoch 2 batch 5/185 loss = 0.4811\n",
      "epoch 2 batch 6/185 loss = 0.4717\n",
      "epoch 2 batch 7/185 loss = 0.4854\n",
      "epoch 2 batch 8/185 loss = 0.4969\n",
      "epoch 2 batch 9/185 loss = 0.4781\n",
      "epoch 2 batch 10/185 loss = 0.4789\n",
      "epoch 2 batch 11/185 loss = 0.4748\n",
      "epoch 2 batch 12/185 loss = 0.4799\n",
      "epoch 2 batch 13/185 loss = 0.4986\n",
      "epoch 2 batch 14/185 loss = 0.4853\n",
      "epoch 2 batch 15/185 loss = 0.4838\n",
      "epoch 2 batch 16/185 loss = 0.4884\n",
      "epoch 2 batch 17/185 loss = 0.4679\n",
      "epoch 2 batch 18/185 loss = 0.4720\n",
      "epoch 2 batch 19/185 loss = 0.4635\n",
      "epoch 2 batch 20/185 loss = 0.4743\n",
      "epoch 2 batch 21/185 loss = 0.4751\n",
      "epoch 2 batch 22/185 loss = 0.4629\n",
      "epoch 2 batch 23/185 loss = 0.4713\n",
      "epoch 2 batch 24/185 loss = 0.4690\n",
      "epoch 2 batch 25/185 loss = 0.4724\n",
      "epoch 2 batch 26/185 loss = 0.4576\n",
      "epoch 2 batch 27/185 loss = 0.4732\n",
      "epoch 2 batch 28/185 loss = 0.4611\n",
      "epoch 2 batch 29/185 loss = 0.4625\n",
      "epoch 2 batch 30/185 loss = 0.4472\n",
      "epoch 2 batch 31/185 loss = 0.4655\n",
      "epoch 2 batch 32/185 loss = 0.4754\n",
      "epoch 2 batch 33/185 loss = 0.4502\n",
      "epoch 2 batch 34/185 loss = 0.4607\n",
      "epoch 2 batch 35/185 loss = 0.4613\n",
      "epoch 2 batch 36/185 loss = 0.4488\n",
      "epoch 2 batch 37/185 loss = 0.4578\n",
      "epoch 2 batch 38/185 loss = 0.4424\n",
      "epoch 2 batch 39/185 loss = 0.4359\n",
      "epoch 2 batch 40/185 loss = 0.4436\n",
      "epoch 2 batch 41/185 loss = 0.4327\n",
      "epoch 2 batch 42/185 loss = 0.4472\n",
      "epoch 2 batch 43/185 loss = 0.4541\n",
      "epoch 2 batch 44/185 loss = 0.4215\n",
      "epoch 2 batch 45/185 loss = 0.4417\n",
      "epoch 2 batch 46/185 loss = 0.4284\n",
      "epoch 2 batch 47/185 loss = 0.4212\n",
      "epoch 2 batch 48/185 loss = 0.4467\n",
      "epoch 2 batch 49/185 loss = 0.4330\n",
      "epoch 2 batch 50/185 loss = 0.4203\n",
      "epoch 2 batch 51/185 loss = 0.4254\n",
      "epoch 2 batch 52/185 loss = 0.4335\n",
      "epoch 2 batch 53/185 loss = 0.4344\n",
      "epoch 2 batch 54/185 loss = 0.4358\n",
      "epoch 2 batch 55/185 loss = 0.4155\n",
      "epoch 2 batch 56/185 loss = 0.4275\n",
      "epoch 2 batch 57/185 loss = 0.4047\n",
      "epoch 2 batch 58/185 loss = 0.4280\n",
      "epoch 2 batch 59/185 loss = 0.4087\n",
      "epoch 2 batch 60/185 loss = 0.4161\n",
      "epoch 2 batch 61/185 loss = 0.4267\n",
      "epoch 2 batch 62/185 loss = 0.4104\n",
      "epoch 2 batch 63/185 loss = 0.4114\n",
      "epoch 2 batch 64/185 loss = 0.4111\n",
      "epoch 2 batch 65/185 loss = 0.4052\n",
      "epoch 2 batch 66/185 loss = 0.4175\n",
      "epoch 2 batch 67/185 loss = 0.4018\n",
      "epoch 2 batch 68/185 loss = 0.4037\n",
      "epoch 2 batch 69/185 loss = 0.4059\n",
      "epoch 2 batch 70/185 loss = 0.3927\n",
      "epoch 2 batch 71/185 loss = 0.4007\n",
      "epoch 2 batch 72/185 loss = 0.4068\n",
      "epoch 2 batch 73/185 loss = 0.4064\n",
      "epoch 2 batch 74/185 loss = 0.3994\n",
      "epoch 2 batch 75/185 loss = 0.3927\n",
      "epoch 2 batch 76/185 loss = 0.4004\n",
      "epoch 2 batch 77/185 loss = 0.3950\n",
      "epoch 2 batch 78/185 loss = 0.4043\n",
      "epoch 2 batch 79/185 loss = 0.4059\n",
      "epoch 2 batch 80/185 loss = 0.3922\n",
      "epoch 2 batch 81/185 loss = 0.3820\n",
      "epoch 2 batch 82/185 loss = 0.3894\n",
      "epoch 2 batch 83/185 loss = 0.3892\n",
      "epoch 2 batch 84/185 loss = 0.3822\n",
      "epoch 2 batch 85/185 loss = 0.3923\n",
      "epoch 2 batch 86/185 loss = 0.3824\n",
      "epoch 2 batch 87/185 loss = 0.3875\n",
      "epoch 2 batch 88/185 loss = 0.3885\n",
      "epoch 2 batch 89/185 loss = 0.3769\n",
      "epoch 2 batch 90/185 loss = 0.3860\n",
      "epoch 2 batch 91/185 loss = 0.3751\n",
      "epoch 2 batch 92/185 loss = 0.3677\n",
      "epoch 2 batch 93/185 loss = 0.3795\n",
      "epoch 2 batch 94/185 loss = 0.3840\n",
      "epoch 2 batch 95/185 loss = 0.3754\n",
      "epoch 2 batch 96/185 loss = 0.3767\n",
      "epoch 2 batch 97/185 loss = 0.3645\n",
      "epoch 2 batch 98/185 loss = 0.3568\n",
      "epoch 2 batch 99/185 loss = 0.3789\n",
      "epoch 2 batch 100/185 loss = 0.3665\n",
      "epoch 2 batch 101/185 loss = 0.3732\n",
      "epoch 2 batch 102/185 loss = 0.3660\n",
      "epoch 2 batch 103/185 loss = 0.3650\n",
      "epoch 2 batch 104/185 loss = 0.3776\n",
      "epoch 2 batch 105/185 loss = 0.3514\n",
      "epoch 2 batch 106/185 loss = 0.3629\n",
      "epoch 2 batch 107/185 loss = 0.3818\n",
      "epoch 2 batch 108/185 loss = 0.3556\n",
      "epoch 2 batch 109/185 loss = 0.3607\n",
      "epoch 2 batch 110/185 loss = 0.3624\n",
      "epoch 2 batch 111/185 loss = 0.3558\n",
      "epoch 2 batch 112/185 loss = 0.3535\n",
      "epoch 2 batch 113/185 loss = 0.3524\n",
      "epoch 2 batch 114/185 loss = 0.3547\n",
      "epoch 2 batch 115/185 loss = 0.3442\n",
      "epoch 2 batch 116/185 loss = 0.3452\n",
      "epoch 2 batch 117/185 loss = 0.3497\n",
      "epoch 2 batch 118/185 loss = 0.3490\n",
      "epoch 2 batch 119/185 loss = 0.3452\n",
      "epoch 2 batch 120/185 loss = 0.3419\n",
      "epoch 2 batch 121/185 loss = 0.3392\n",
      "epoch 2 batch 122/185 loss = 0.3422\n",
      "epoch 2 batch 123/185 loss = 0.3652\n",
      "epoch 2 batch 124/185 loss = 0.3419\n",
      "epoch 2 batch 125/185 loss = 0.3361\n",
      "epoch 2 batch 126/185 loss = 0.3403\n",
      "epoch 2 batch 127/185 loss = 0.3355\n",
      "epoch 2 batch 128/185 loss = 0.3354\n",
      "epoch 2 batch 129/185 loss = 0.3324\n",
      "epoch 2 batch 130/185 loss = 0.3332\n",
      "epoch 2 batch 131/185 loss = 0.3234\n",
      "epoch 2 batch 132/185 loss = 0.3383\n",
      "epoch 2 batch 133/185 loss = 0.3259\n",
      "epoch 2 batch 134/185 loss = 0.3311\n",
      "epoch 2 batch 135/185 loss = 0.3339\n",
      "epoch 2 batch 136/185 loss = 0.3261\n",
      "epoch 2 batch 137/185 loss = 0.3174\n",
      "epoch 2 batch 138/185 loss = 0.3287\n",
      "epoch 2 batch 139/185 loss = 0.3173\n",
      "epoch 2 batch 140/185 loss = 0.3285\n",
      "epoch 2 batch 141/185 loss = 0.3237\n",
      "epoch 2 batch 142/185 loss = 0.3177\n",
      "epoch 2 batch 143/185 loss = 0.3228\n",
      "epoch 2 batch 144/185 loss = 0.3111\n",
      "epoch 2 batch 145/185 loss = 0.3254\n",
      "epoch 2 batch 146/185 loss = 0.3098\n",
      "epoch 2 batch 147/185 loss = 0.3176\n",
      "epoch 2 batch 148/185 loss = 0.3009\n",
      "epoch 2 batch 149/185 loss = 0.3093\n",
      "epoch 2 batch 150/185 loss = 0.3115\n",
      "epoch 2 batch 151/185 loss = 0.3076\n",
      "epoch 2 batch 152/185 loss = 0.3051\n",
      "epoch 2 batch 153/185 loss = 0.3013\n",
      "epoch 2 batch 154/185 loss = 0.2975\n",
      "epoch 2 batch 155/185 loss = 0.3128\n",
      "epoch 2 batch 156/185 loss = 0.3179\n",
      "epoch 2 batch 157/185 loss = 0.3098\n",
      "epoch 2 batch 158/185 loss = 0.3071\n",
      "epoch 2 batch 159/185 loss = 0.3081\n",
      "epoch 2 batch 160/185 loss = 0.3001\n",
      "epoch 2 batch 161/185 loss = 0.3149\n",
      "epoch 2 batch 162/185 loss = 0.2973\n",
      "epoch 2 batch 163/185 loss = 0.3093\n",
      "epoch 2 batch 164/185 loss = 0.2959\n",
      "epoch 2 batch 165/185 loss = 0.2978\n",
      "epoch 2 batch 166/185 loss = 0.2901\n",
      "epoch 2 batch 167/185 loss = 0.3078\n",
      "epoch 2 batch 168/185 loss = 0.3018\n",
      "epoch 2 batch 169/185 loss = 0.2955\n",
      "epoch 2 batch 170/185 loss = 0.2956\n",
      "epoch 2 batch 171/185 loss = 0.2917\n",
      "epoch 2 batch 172/185 loss = 0.2861\n",
      "epoch 2 batch 173/185 loss = 0.2983\n",
      "epoch 2 batch 174/185 loss = 0.2830\n",
      "epoch 2 batch 175/185 loss = 0.2840\n",
      "epoch 2 batch 176/185 loss = 0.3009\n",
      "epoch 2 batch 177/185 loss = 0.2794\n",
      "epoch 2 batch 178/185 loss = 0.2844\n",
      "epoch 2 batch 179/185 loss = 0.2855\n",
      "epoch 2 batch 180/185 loss = 0.2859\n",
      "epoch 2 batch 181/185 loss = 0.2834\n",
      "epoch 2 batch 182/185 loss = 0.2868\n",
      "epoch 2 batch 183/185 loss = 0.2845\n",
      "epoch 2 batch 184/185 loss = 0.2796\n",
      "epoch 2 batch 185/185 loss = 0.2739\n",
      "epoch 2 train loss = 0.3808 valid loss = 0.3698\n",
      "epoch 3 batch 1/185 loss = 0.2789\n",
      "epoch 3 batch 2/185 loss = 0.2910\n",
      "epoch 3 batch 3/185 loss = 0.2856\n",
      "epoch 3 batch 4/185 loss = 0.2766\n",
      "epoch 3 batch 5/185 loss = 0.2742\n",
      "epoch 3 batch 6/185 loss = 0.2749\n",
      "epoch 3 batch 7/185 loss = 0.2780\n",
      "epoch 3 batch 8/185 loss = 0.2727\n",
      "epoch 3 batch 9/185 loss = 0.2713\n",
      "epoch 3 batch 10/185 loss = 0.2715\n",
      "epoch 3 batch 11/185 loss = 0.2709\n",
      "epoch 3 batch 12/185 loss = 0.2700\n",
      "epoch 3 batch 13/185 loss = 0.2700\n",
      "epoch 3 batch 14/185 loss = 0.2646\n",
      "epoch 3 batch 15/185 loss = 0.2571\n",
      "epoch 3 batch 16/185 loss = 0.2582\n",
      "epoch 3 batch 17/185 loss = 0.2652\n",
      "epoch 3 batch 18/185 loss = 0.2669\n",
      "epoch 3 batch 19/185 loss = 0.2656\n",
      "epoch 3 batch 20/185 loss = 0.2671\n",
      "epoch 3 batch 21/185 loss = 0.2512\n",
      "epoch 3 batch 22/185 loss = 0.2688\n",
      "epoch 3 batch 23/185 loss = 0.2626\n",
      "epoch 3 batch 24/185 loss = 0.2547\n",
      "epoch 3 batch 25/185 loss = 0.2453\n",
      "epoch 3 batch 26/185 loss = 0.2553\n",
      "epoch 3 batch 27/185 loss = 0.2560\n",
      "epoch 3 batch 28/185 loss = 0.2544\n",
      "epoch 3 batch 29/185 loss = 0.2546\n",
      "epoch 3 batch 30/185 loss = 0.2602\n",
      "epoch 3 batch 31/185 loss = 0.2542\n",
      "epoch 3 batch 32/185 loss = 0.2638\n",
      "epoch 3 batch 33/185 loss = 0.2564\n",
      "epoch 3 batch 34/185 loss = 0.2469\n",
      "epoch 3 batch 35/185 loss = 0.2588\n",
      "epoch 3 batch 36/185 loss = 0.2443\n",
      "epoch 3 batch 37/185 loss = 0.2604\n",
      "epoch 3 batch 38/185 loss = 0.2552\n",
      "epoch 3 batch 39/185 loss = 0.2471\n",
      "epoch 3 batch 40/185 loss = 0.2584\n",
      "epoch 3 batch 41/185 loss = 0.2441\n",
      "epoch 3 batch 42/185 loss = 0.2492\n",
      "epoch 3 batch 43/185 loss = 0.2387\n",
      "epoch 3 batch 44/185 loss = 0.2351\n",
      "epoch 3 batch 45/185 loss = 0.2479\n",
      "epoch 3 batch 46/185 loss = 0.2409\n",
      "epoch 3 batch 47/185 loss = 0.2502\n",
      "epoch 3 batch 48/185 loss = 0.2446\n",
      "epoch 3 batch 49/185 loss = 0.2429\n",
      "epoch 3 batch 50/185 loss = 0.2400\n",
      "epoch 3 batch 51/185 loss = 0.2303\n",
      "epoch 3 batch 52/185 loss = 0.2436\n",
      "epoch 3 batch 53/185 loss = 0.2334\n",
      "epoch 3 batch 54/185 loss = 0.2325\n",
      "epoch 3 batch 55/185 loss = 0.2229\n",
      "epoch 3 batch 56/185 loss = 0.2356\n",
      "epoch 3 batch 57/185 loss = 0.2264\n",
      "epoch 3 batch 58/185 loss = 0.2290\n",
      "epoch 3 batch 59/185 loss = 0.2231\n",
      "epoch 3 batch 60/185 loss = 0.2341\n",
      "epoch 3 batch 61/185 loss = 0.2294\n",
      "epoch 3 batch 62/185 loss = 0.2221\n",
      "epoch 3 batch 63/185 loss = 0.2277\n",
      "epoch 3 batch 64/185 loss = 0.2171\n",
      "epoch 3 batch 65/185 loss = 0.2370\n",
      "epoch 3 batch 66/185 loss = 0.2192\n",
      "epoch 3 batch 67/185 loss = 0.2269\n",
      "epoch 3 batch 68/185 loss = 0.2122\n",
      "epoch 3 batch 69/185 loss = 0.2275\n",
      "epoch 3 batch 70/185 loss = 0.2268\n",
      "epoch 3 batch 71/185 loss = 0.2211\n",
      "epoch 3 batch 72/185 loss = 0.2124\n",
      "epoch 3 batch 73/185 loss = 0.2236\n",
      "epoch 3 batch 74/185 loss = 0.2307\n",
      "epoch 3 batch 75/185 loss = 0.2190\n",
      "epoch 3 batch 76/185 loss = 0.2290\n",
      "epoch 3 batch 77/185 loss = 0.2196\n",
      "epoch 3 batch 78/185 loss = 0.2158\n",
      "epoch 3 batch 79/185 loss = 0.2177\n",
      "epoch 3 batch 80/185 loss = 0.2234\n",
      "epoch 3 batch 81/185 loss = 0.2159\n",
      "epoch 3 batch 82/185 loss = 0.2097\n",
      "epoch 3 batch 83/185 loss = 0.2096\n",
      "epoch 3 batch 84/185 loss = 0.2030\n",
      "epoch 3 batch 85/185 loss = 0.2025\n",
      "epoch 3 batch 86/185 loss = 0.2174\n",
      "epoch 3 batch 87/185 loss = 0.2252\n",
      "epoch 3 batch 88/185 loss = 0.2109\n",
      "epoch 3 batch 89/185 loss = 0.1986\n",
      "epoch 3 batch 90/185 loss = 0.2022\n",
      "epoch 3 batch 91/185 loss = 0.2078\n",
      "epoch 3 batch 92/185 loss = 0.2078\n",
      "epoch 3 batch 93/185 loss = 0.2080\n",
      "epoch 3 batch 94/185 loss = 0.2005\n",
      "epoch 3 batch 95/185 loss = 0.2036\n",
      "epoch 3 batch 96/185 loss = 0.2089\n",
      "epoch 3 batch 97/185 loss = 0.1975\n",
      "epoch 3 batch 98/185 loss = 0.2043\n",
      "epoch 3 batch 99/185 loss = 0.2200\n",
      "epoch 3 batch 100/185 loss = 0.2001\n",
      "epoch 3 batch 101/185 loss = 0.2014\n",
      "epoch 3 batch 102/185 loss = 0.2064\n",
      "epoch 3 batch 103/185 loss = 0.1986\n",
      "epoch 3 batch 104/185 loss = 0.2007\n",
      "epoch 3 batch 105/185 loss = 0.2018\n",
      "epoch 3 batch 106/185 loss = 0.2053\n",
      "epoch 3 batch 107/185 loss = 0.2077\n",
      "epoch 3 batch 108/185 loss = 0.2091\n",
      "epoch 3 batch 109/185 loss = 0.1971\n",
      "epoch 3 batch 110/185 loss = 0.1945\n",
      "epoch 3 batch 111/185 loss = 0.1955\n",
      "epoch 3 batch 112/185 loss = 0.2048\n",
      "epoch 3 batch 113/185 loss = 0.1970\n",
      "epoch 3 batch 114/185 loss = 0.1906\n",
      "epoch 3 batch 115/185 loss = 0.1949\n",
      "epoch 3 batch 116/185 loss = 0.1816\n",
      "epoch 3 batch 117/185 loss = 0.1881\n",
      "epoch 3 batch 118/185 loss = 0.1805\n",
      "epoch 3 batch 119/185 loss = 0.1860\n",
      "epoch 3 batch 120/185 loss = 0.1816\n",
      "epoch 3 batch 121/185 loss = 0.1801\n",
      "epoch 3 batch 122/185 loss = 0.1790\n",
      "epoch 3 batch 123/185 loss = 0.1836\n",
      "epoch 3 batch 124/185 loss = 0.1899\n",
      "epoch 3 batch 125/185 loss = 0.1880\n",
      "epoch 3 batch 126/185 loss = 0.1969\n",
      "epoch 3 batch 127/185 loss = 0.1795\n",
      "epoch 3 batch 128/185 loss = 0.1867\n",
      "epoch 3 batch 129/185 loss = 0.1851\n",
      "epoch 3 batch 130/185 loss = 0.1917\n",
      "epoch 3 batch 131/185 loss = 0.1764\n",
      "epoch 3 batch 132/185 loss = 0.1734\n",
      "epoch 3 batch 133/185 loss = 0.1785\n",
      "epoch 3 batch 134/185 loss = 0.1875\n",
      "epoch 3 batch 135/185 loss = 0.1818\n",
      "epoch 3 batch 136/185 loss = 0.1789\n",
      "epoch 3 batch 137/185 loss = 0.1775\n",
      "epoch 3 batch 138/185 loss = 0.1754\n",
      "epoch 3 batch 139/185 loss = 0.1765\n",
      "epoch 3 batch 140/185 loss = 0.1762\n",
      "epoch 3 batch 141/185 loss = 0.1766\n",
      "epoch 3 batch 142/185 loss = 0.1840\n",
      "epoch 3 batch 143/185 loss = 0.1744\n",
      "epoch 3 batch 144/185 loss = 0.1826\n",
      "epoch 3 batch 145/185 loss = 0.1773\n",
      "epoch 3 batch 146/185 loss = 0.1781\n",
      "epoch 3 batch 147/185 loss = 0.1787\n",
      "epoch 3 batch 148/185 loss = 0.1744\n",
      "epoch 3 batch 149/185 loss = 0.1692\n",
      "epoch 3 batch 150/185 loss = 0.1788\n",
      "epoch 3 batch 151/185 loss = 0.1659\n",
      "epoch 3 batch 152/185 loss = 0.1692\n",
      "epoch 3 batch 153/185 loss = 0.1632\n",
      "epoch 3 batch 154/185 loss = 0.1686\n",
      "epoch 3 batch 155/185 loss = 0.1638\n",
      "epoch 3 batch 156/185 loss = 0.1744\n",
      "epoch 3 batch 157/185 loss = 0.1626\n",
      "epoch 3 batch 158/185 loss = 0.1690\n",
      "epoch 3 batch 159/185 loss = 0.1695\n",
      "epoch 3 batch 160/185 loss = 0.1686\n",
      "epoch 3 batch 161/185 loss = 0.1659\n",
      "epoch 3 batch 162/185 loss = 0.1899\n",
      "epoch 3 batch 163/185 loss = 0.1724\n",
      "epoch 3 batch 164/185 loss = 0.1573\n",
      "epoch 3 batch 165/185 loss = 0.1668\n",
      "epoch 3 batch 166/185 loss = 0.1645\n",
      "epoch 3 batch 167/185 loss = 0.1624\n",
      "epoch 3 batch 168/185 loss = 0.1603\n",
      "epoch 3 batch 169/185 loss = 0.1623\n",
      "epoch 3 batch 170/185 loss = 0.1622\n",
      "epoch 3 batch 171/185 loss = 0.1614\n",
      "epoch 3 batch 172/185 loss = 0.1539\n",
      "epoch 3 batch 173/185 loss = 0.1594\n",
      "epoch 3 batch 174/185 loss = 0.1552\n",
      "epoch 3 batch 175/185 loss = 0.1527\n",
      "epoch 3 batch 176/185 loss = 0.1519\n",
      "epoch 3 batch 177/185 loss = 0.1603\n",
      "epoch 3 batch 178/185 loss = 0.1482\n",
      "epoch 3 batch 179/185 loss = 0.1526\n",
      "epoch 3 batch 180/185 loss = 0.1615\n",
      "epoch 3 batch 181/185 loss = 0.1558\n",
      "epoch 3 batch 182/185 loss = 0.1616\n",
      "epoch 3 batch 183/185 loss = 0.1595\n",
      "epoch 3 batch 184/185 loss = 0.1588\n",
      "epoch 3 batch 185/185 loss = 0.1567\n",
      "epoch 3 train loss = 0.2104 valid loss = 0.1729\n",
      "epoch 4 batch 1/185 loss = 0.1435\n",
      "epoch 4 batch 2/185 loss = 0.1516\n",
      "epoch 4 batch 3/185 loss = 0.1579\n",
      "epoch 4 batch 4/185 loss = 0.1596\n",
      "epoch 4 batch 5/185 loss = 0.1512\n",
      "epoch 4 batch 6/185 loss = 0.1424\n",
      "epoch 4 batch 7/185 loss = 0.1499\n",
      "epoch 4 batch 8/185 loss = 0.1522\n",
      "epoch 4 batch 9/185 loss = 0.1486\n",
      "epoch 4 batch 10/185 loss = 0.1511\n",
      "epoch 4 batch 11/185 loss = 0.1462\n",
      "epoch 4 batch 12/185 loss = 0.1404\n",
      "epoch 4 batch 13/185 loss = 0.1400\n",
      "epoch 4 batch 14/185 loss = 0.1409\n",
      "epoch 4 batch 15/185 loss = 0.1486\n",
      "epoch 4 batch 16/185 loss = 0.1570\n",
      "epoch 4 batch 17/185 loss = 0.1452\n",
      "epoch 4 batch 18/185 loss = 0.1431\n",
      "epoch 4 batch 19/185 loss = 0.1352\n",
      "epoch 4 batch 20/185 loss = 0.1369\n",
      "epoch 4 batch 21/185 loss = 0.1398\n",
      "epoch 4 batch 22/185 loss = 0.1463\n",
      "epoch 4 batch 23/185 loss = 0.1471\n",
      "epoch 4 batch 24/185 loss = 0.1482\n",
      "epoch 4 batch 25/185 loss = 0.1316\n",
      "epoch 4 batch 26/185 loss = 0.1364\n",
      "epoch 4 batch 27/185 loss = 0.1387\n",
      "epoch 4 batch 28/185 loss = 0.1356\n",
      "epoch 4 batch 29/185 loss = 0.1376\n",
      "epoch 4 batch 30/185 loss = 0.1375\n",
      "epoch 4 batch 31/185 loss = 0.1342\n",
      "epoch 4 batch 32/185 loss = 0.1391\n",
      "epoch 4 batch 33/185 loss = 0.1363\n",
      "epoch 4 batch 34/185 loss = 0.1415\n",
      "epoch 4 batch 35/185 loss = 0.1367\n",
      "epoch 4 batch 36/185 loss = 0.1346\n",
      "epoch 4 batch 37/185 loss = 0.1408\n",
      "epoch 4 batch 38/185 loss = 0.1392\n",
      "epoch 4 batch 39/185 loss = 0.1362\n",
      "epoch 4 batch 40/185 loss = 0.1291\n",
      "epoch 4 batch 41/185 loss = 0.1324\n",
      "epoch 4 batch 42/185 loss = 0.1327\n",
      "epoch 4 batch 43/185 loss = 0.1320\n",
      "epoch 4 batch 44/185 loss = 0.1350\n",
      "epoch 4 batch 45/185 loss = 0.1368\n",
      "epoch 4 batch 46/185 loss = 0.1307\n",
      "epoch 4 batch 47/185 loss = 0.1254\n",
      "epoch 4 batch 48/185 loss = 0.1317\n",
      "epoch 4 batch 49/185 loss = 0.1364\n",
      "epoch 4 batch 50/185 loss = 0.1366\n",
      "epoch 4 batch 51/185 loss = 0.1267\n",
      "epoch 4 batch 52/185 loss = 0.1305\n",
      "epoch 4 batch 53/185 loss = 0.1270\n",
      "epoch 4 batch 54/185 loss = 0.1263\n",
      "epoch 4 batch 55/185 loss = 0.1333\n",
      "epoch 4 batch 56/185 loss = 0.1252\n",
      "epoch 4 batch 57/185 loss = 0.1309\n",
      "epoch 4 batch 58/185 loss = 0.1213\n",
      "epoch 4 batch 59/185 loss = 0.1283\n",
      "epoch 4 batch 60/185 loss = 0.1262\n",
      "epoch 4 batch 61/185 loss = 0.1238\n",
      "epoch 4 batch 62/185 loss = 0.1257\n",
      "epoch 4 batch 63/185 loss = 0.1337\n",
      "epoch 4 batch 64/185 loss = 0.1287\n",
      "epoch 4 batch 65/185 loss = 0.1160\n",
      "epoch 4 batch 66/185 loss = 0.1204\n",
      "epoch 4 batch 67/185 loss = 0.1179\n",
      "epoch 4 batch 68/185 loss = 0.1177\n",
      "epoch 4 batch 69/185 loss = 0.1253\n",
      "epoch 4 batch 70/185 loss = 0.1167\n",
      "epoch 4 batch 71/185 loss = 0.1191\n",
      "epoch 4 batch 72/185 loss = 0.1149\n",
      "epoch 4 batch 73/185 loss = 0.1228\n",
      "epoch 4 batch 74/185 loss = 0.1159\n",
      "epoch 4 batch 75/185 loss = 0.1220\n",
      "epoch 4 batch 76/185 loss = 0.1171\n",
      "epoch 4 batch 77/185 loss = 0.1134\n",
      "epoch 4 batch 78/185 loss = 0.1241\n",
      "epoch 4 batch 79/185 loss = 0.1136\n",
      "epoch 4 batch 80/185 loss = 0.1166\n",
      "epoch 4 batch 81/185 loss = 0.1220\n",
      "epoch 4 batch 82/185 loss = 0.1180\n",
      "epoch 4 batch 83/185 loss = 0.1099\n",
      "epoch 4 batch 84/185 loss = 0.1131\n",
      "epoch 4 batch 85/185 loss = 0.1111\n",
      "epoch 4 batch 86/185 loss = 0.1162\n",
      "epoch 4 batch 87/185 loss = 0.1187\n",
      "epoch 4 batch 88/185 loss = 0.1112\n",
      "epoch 4 batch 89/185 loss = 0.1184\n",
      "epoch 4 batch 90/185 loss = 0.1112\n",
      "epoch 4 batch 91/185 loss = 0.1085\n",
      "epoch 4 batch 92/185 loss = 0.1153\n",
      "epoch 4 batch 93/185 loss = 0.1110\n",
      "epoch 4 batch 94/185 loss = 0.1050\n",
      "epoch 4 batch 95/185 loss = 0.1156\n",
      "epoch 4 batch 96/185 loss = 0.1090\n",
      "epoch 4 batch 97/185 loss = 0.1028\n",
      "epoch 4 batch 98/185 loss = 0.1172\n",
      "epoch 4 batch 99/185 loss = 0.1120\n",
      "epoch 4 batch 100/185 loss = 0.1022\n",
      "epoch 4 batch 101/185 loss = 0.1111\n",
      "epoch 4 batch 102/185 loss = 0.1112\n",
      "epoch 4 batch 103/185 loss = 0.1049\n",
      "epoch 4 batch 104/185 loss = 0.1050\n",
      "epoch 4 batch 105/185 loss = 0.1096\n",
      "epoch 4 batch 106/185 loss = 0.1022\n",
      "epoch 4 batch 107/185 loss = 0.1071\n",
      "epoch 4 batch 108/185 loss = 0.1082\n",
      "epoch 4 batch 109/185 loss = 0.1095\n",
      "epoch 4 batch 110/185 loss = 0.0999\n",
      "epoch 4 batch 111/185 loss = 0.1015\n",
      "epoch 4 batch 112/185 loss = 0.1039\n",
      "epoch 4 batch 113/185 loss = 0.1045\n",
      "epoch 4 batch 114/185 loss = 0.1049\n",
      "epoch 4 batch 115/185 loss = 0.1072\n",
      "epoch 4 batch 116/185 loss = 0.1058\n",
      "epoch 4 batch 117/185 loss = 0.1100\n",
      "epoch 4 batch 118/185 loss = 0.1054\n",
      "epoch 4 batch 119/185 loss = 0.1050\n",
      "epoch 4 batch 120/185 loss = 0.0940\n",
      "epoch 4 batch 121/185 loss = 0.0926\n",
      "epoch 4 batch 122/185 loss = 0.1082\n",
      "epoch 4 batch 123/185 loss = 0.0949\n",
      "epoch 4 batch 124/185 loss = 0.1072\n",
      "epoch 4 batch 125/185 loss = 0.1047\n",
      "epoch 4 batch 126/185 loss = 0.0996\n",
      "epoch 4 batch 127/185 loss = 0.0971\n",
      "epoch 4 batch 128/185 loss = 0.0973\n",
      "epoch 4 batch 129/185 loss = 0.1010\n",
      "epoch 4 batch 130/185 loss = 0.0883\n",
      "epoch 4 batch 131/185 loss = 0.0985\n",
      "epoch 4 batch 132/185 loss = 0.1008\n",
      "epoch 4 batch 133/185 loss = 0.0924\n",
      "epoch 4 batch 134/185 loss = 0.0986\n",
      "epoch 4 batch 135/185 loss = 0.1002\n",
      "epoch 4 batch 136/185 loss = 0.1062\n",
      "epoch 4 batch 137/185 loss = 0.0972\n",
      "epoch 4 batch 138/185 loss = 0.0922\n",
      "epoch 4 batch 139/185 loss = 0.0905\n",
      "epoch 4 batch 140/185 loss = 0.0932\n",
      "epoch 4 batch 141/185 loss = 0.0924\n",
      "epoch 4 batch 142/185 loss = 0.0909\n",
      "epoch 4 batch 143/185 loss = 0.1034\n",
      "epoch 4 batch 144/185 loss = 0.0866\n",
      "epoch 4 batch 145/185 loss = 0.0978\n",
      "epoch 4 batch 146/185 loss = 0.0935\n",
      "epoch 4 batch 147/185 loss = 0.0969\n",
      "epoch 4 batch 148/185 loss = 0.0914\n",
      "epoch 4 batch 149/185 loss = 0.0958\n",
      "epoch 4 batch 150/185 loss = 0.0983\n",
      "epoch 4 batch 151/185 loss = 0.0838\n",
      "epoch 4 batch 152/185 loss = 0.1002\n",
      "epoch 4 batch 153/185 loss = 0.0964\n",
      "epoch 4 batch 154/185 loss = 0.0935\n",
      "epoch 4 batch 155/185 loss = 0.0856\n",
      "epoch 4 batch 156/185 loss = 0.0916\n",
      "epoch 4 batch 157/185 loss = 0.0918\n",
      "epoch 4 batch 158/185 loss = 0.0913\n",
      "epoch 4 batch 159/185 loss = 0.0885\n",
      "epoch 4 batch 160/185 loss = 0.0840\n",
      "epoch 4 batch 161/185 loss = 0.0827\n",
      "epoch 4 batch 162/185 loss = 0.0867\n",
      "epoch 4 batch 163/185 loss = 0.0929\n",
      "epoch 4 batch 164/185 loss = 0.0933\n",
      "epoch 4 batch 165/185 loss = 0.0908\n",
      "epoch 4 batch 166/185 loss = 0.0930\n",
      "epoch 4 batch 167/185 loss = 0.0847\n",
      "epoch 4 batch 168/185 loss = 0.0919\n",
      "epoch 4 batch 169/185 loss = 0.0799\n",
      "epoch 4 batch 170/185 loss = 0.0850\n",
      "epoch 4 batch 171/185 loss = 0.0860\n",
      "epoch 4 batch 172/185 loss = 0.0802\n",
      "epoch 4 batch 173/185 loss = 0.0861\n",
      "epoch 4 batch 174/185 loss = 0.0877\n",
      "epoch 4 batch 175/185 loss = 0.0901\n",
      "epoch 4 batch 176/185 loss = 0.0787\n",
      "epoch 4 batch 177/185 loss = 0.0825\n",
      "epoch 4 batch 178/185 loss = 0.0844\n",
      "epoch 4 batch 179/185 loss = 0.0937\n",
      "epoch 4 batch 180/185 loss = 0.0748\n",
      "epoch 4 batch 181/185 loss = 0.0828\n",
      "epoch 4 batch 182/185 loss = 0.0865\n",
      "epoch 4 batch 183/185 loss = 0.0893\n",
      "epoch 4 batch 184/185 loss = 0.0803\n",
      "epoch 4 batch 185/185 loss = 0.0859\n",
      "epoch 4 train loss = 0.1137 valid loss = 0.0882\n",
      "epoch 5 batch 1/185 loss = 0.0797\n",
      "epoch 5 batch 2/185 loss = 0.0780\n",
      "epoch 5 batch 3/185 loss = 0.0843\n",
      "epoch 5 batch 4/185 loss = 0.0831\n",
      "epoch 5 batch 5/185 loss = 0.0773\n",
      "epoch 5 batch 6/185 loss = 0.0783\n",
      "epoch 5 batch 7/185 loss = 0.0799\n",
      "epoch 5 batch 8/185 loss = 0.0844\n",
      "epoch 5 batch 9/185 loss = 0.0755\n",
      "epoch 5 batch 10/185 loss = 0.0769\n",
      "epoch 5 batch 11/185 loss = 0.0737\n",
      "epoch 5 batch 12/185 loss = 0.0704\n",
      "epoch 5 batch 13/185 loss = 0.0778\n",
      "epoch 5 batch 14/185 loss = 0.0748\n",
      "epoch 5 batch 15/185 loss = 0.0826\n",
      "epoch 5 batch 16/185 loss = 0.0788\n",
      "epoch 5 batch 17/185 loss = 0.0710\n",
      "epoch 5 batch 18/185 loss = 0.0740\n",
      "epoch 5 batch 19/185 loss = 0.0775\n",
      "epoch 5 batch 20/185 loss = 0.0814\n",
      "epoch 5 batch 21/185 loss = 0.0807\n",
      "epoch 5 batch 22/185 loss = 0.0751\n",
      "epoch 5 batch 23/185 loss = 0.0755\n",
      "epoch 5 batch 24/185 loss = 0.0746\n",
      "epoch 5 batch 25/185 loss = 0.0806\n",
      "epoch 5 batch 26/185 loss = 0.0708\n",
      "epoch 5 batch 27/185 loss = 0.0712\n",
      "epoch 5 batch 28/185 loss = 0.0701\n",
      "epoch 5 batch 29/185 loss = 0.0780\n",
      "epoch 5 batch 30/185 loss = 0.0716\n",
      "epoch 5 batch 31/185 loss = 0.0713\n",
      "epoch 5 batch 32/185 loss = 0.0759\n",
      "epoch 5 batch 33/185 loss = 0.0736\n",
      "epoch 5 batch 34/185 loss = 0.0710\n",
      "epoch 5 batch 35/185 loss = 0.0705\n",
      "epoch 5 batch 36/185 loss = 0.0673\n",
      "epoch 5 batch 37/185 loss = 0.0659\n",
      "epoch 5 batch 38/185 loss = 0.0723\n",
      "epoch 5 batch 39/185 loss = 0.0743\n",
      "epoch 5 batch 40/185 loss = 0.0696\n",
      "epoch 5 batch 41/185 loss = 0.0746\n",
      "epoch 5 batch 42/185 loss = 0.0724\n",
      "epoch 5 batch 43/185 loss = 0.0730\n",
      "epoch 5 batch 44/185 loss = 0.0726\n",
      "epoch 5 batch 45/185 loss = 0.0681\n",
      "epoch 5 batch 46/185 loss = 0.0692\n",
      "epoch 5 batch 47/185 loss = 0.0709\n",
      "epoch 5 batch 48/185 loss = 0.0650\n",
      "epoch 5 batch 49/185 loss = 0.0718\n",
      "epoch 5 batch 50/185 loss = 0.0716\n",
      "epoch 5 batch 51/185 loss = 0.0686\n",
      "epoch 5 batch 52/185 loss = 0.0745\n",
      "epoch 5 batch 53/185 loss = 0.0702\n",
      "epoch 5 batch 54/185 loss = 0.0680\n",
      "epoch 5 batch 55/185 loss = 0.0631\n",
      "epoch 5 batch 56/185 loss = 0.0719\n",
      "epoch 5 batch 57/185 loss = 0.0604\n",
      "epoch 5 batch 58/185 loss = 0.0673\n",
      "epoch 5 batch 59/185 loss = 0.0570\n",
      "epoch 5 batch 60/185 loss = 0.0680\n",
      "epoch 5 batch 61/185 loss = 0.0710\n",
      "epoch 5 batch 62/185 loss = 0.0644\n",
      "epoch 5 batch 63/185 loss = 0.0637\n",
      "epoch 5 batch 64/185 loss = 0.0686\n",
      "epoch 5 batch 65/185 loss = 0.0737\n",
      "epoch 5 batch 66/185 loss = 0.0690\n",
      "epoch 5 batch 67/185 loss = 0.0623\n",
      "epoch 5 batch 68/185 loss = 0.0700\n",
      "epoch 5 batch 69/185 loss = 0.0622\n",
      "epoch 5 batch 70/185 loss = 0.0658\n",
      "epoch 5 batch 71/185 loss = 0.0621\n",
      "epoch 5 batch 72/185 loss = 0.0618\n",
      "epoch 5 batch 73/185 loss = 0.0630\n",
      "epoch 5 batch 74/185 loss = 0.0692\n",
      "epoch 5 batch 75/185 loss = 0.0695\n",
      "epoch 5 batch 76/185 loss = 0.0604\n",
      "epoch 5 batch 77/185 loss = 0.0593\n",
      "epoch 5 batch 78/185 loss = 0.0682\n",
      "epoch 5 batch 79/185 loss = 0.0619\n",
      "epoch 5 batch 80/185 loss = 0.0679\n",
      "epoch 5 batch 81/185 loss = 0.0567\n",
      "epoch 5 batch 82/185 loss = 0.0650\n",
      "epoch 5 batch 83/185 loss = 0.0630\n",
      "epoch 5 batch 84/185 loss = 0.0621\n",
      "epoch 5 batch 85/185 loss = 0.0660\n",
      "epoch 5 batch 86/185 loss = 0.0582\n",
      "epoch 5 batch 87/185 loss = 0.0661\n",
      "epoch 5 batch 88/185 loss = 0.0657\n",
      "epoch 5 batch 89/185 loss = 0.0676\n",
      "epoch 5 batch 90/185 loss = 0.0581\n",
      "epoch 5 batch 91/185 loss = 0.0602\n",
      "epoch 5 batch 92/185 loss = 0.0554\n",
      "epoch 5 batch 93/185 loss = 0.0529\n",
      "epoch 5 batch 94/185 loss = 0.0568\n",
      "epoch 5 batch 95/185 loss = 0.0579\n",
      "epoch 5 batch 96/185 loss = 0.0631\n",
      "epoch 5 batch 97/185 loss = 0.0683\n",
      "epoch 5 batch 98/185 loss = 0.0621\n",
      "epoch 5 batch 99/185 loss = 0.0582\n",
      "epoch 5 batch 100/185 loss = 0.0585\n",
      "epoch 5 batch 101/185 loss = 0.0604\n",
      "epoch 5 batch 102/185 loss = 0.0576\n",
      "epoch 5 batch 103/185 loss = 0.0530\n",
      "epoch 5 batch 104/185 loss = 0.0646\n",
      "epoch 5 batch 105/185 loss = 0.0571\n",
      "epoch 5 batch 106/185 loss = 0.0563\n",
      "epoch 5 batch 107/185 loss = 0.0611\n",
      "epoch 5 batch 108/185 loss = 0.0567\n",
      "epoch 5 batch 109/185 loss = 0.0506\n",
      "epoch 5 batch 110/185 loss = 0.0596\n",
      "epoch 5 batch 111/185 loss = 0.0531\n",
      "epoch 5 batch 112/185 loss = 0.0557\n",
      "epoch 5 batch 113/185 loss = 0.0537\n",
      "epoch 5 batch 114/185 loss = 0.0570\n",
      "epoch 5 batch 115/185 loss = 0.0596\n",
      "epoch 5 batch 116/185 loss = 0.0515\n",
      "epoch 5 batch 117/185 loss = 0.0540\n",
      "epoch 5 batch 118/185 loss = 0.0493\n",
      "epoch 5 batch 119/185 loss = 0.0492\n",
      "epoch 5 batch 120/185 loss = 0.0570\n",
      "epoch 5 batch 121/185 loss = 0.0562\n",
      "epoch 5 batch 122/185 loss = 0.0570\n",
      "epoch 5 batch 123/185 loss = 0.0505\n",
      "epoch 5 batch 124/185 loss = 0.0563\n",
      "epoch 5 batch 125/185 loss = 0.0536\n",
      "epoch 5 batch 126/185 loss = 0.0487\n",
      "epoch 5 batch 127/185 loss = 0.0575\n",
      "epoch 5 batch 128/185 loss = 0.0562\n",
      "epoch 5 batch 129/185 loss = 0.0515\n",
      "epoch 5 batch 130/185 loss = 0.0563\n",
      "epoch 5 batch 131/185 loss = 0.0533\n",
      "epoch 5 batch 132/185 loss = 0.0492\n",
      "epoch 5 batch 133/185 loss = 0.0554\n",
      "epoch 5 batch 134/185 loss = 0.0521\n",
      "epoch 5 batch 135/185 loss = 0.0509\n",
      "epoch 5 batch 136/185 loss = 0.0472\n",
      "epoch 5 batch 137/185 loss = 0.0572\n",
      "epoch 5 batch 138/185 loss = 0.0509\n",
      "epoch 5 batch 139/185 loss = 0.0520\n",
      "epoch 5 batch 140/185 loss = 0.0500\n",
      "epoch 5 batch 141/185 loss = 0.0536\n",
      "epoch 5 batch 142/185 loss = 0.0476\n",
      "epoch 5 batch 143/185 loss = 0.0485\n",
      "epoch 5 batch 144/185 loss = 0.0504\n",
      "epoch 5 batch 145/185 loss = 0.0505\n",
      "epoch 5 batch 146/185 loss = 0.0506\n",
      "epoch 5 batch 147/185 loss = 0.0501\n",
      "epoch 5 batch 148/185 loss = 0.0509\n",
      "epoch 5 batch 149/185 loss = 0.0485\n",
      "epoch 5 batch 150/185 loss = 0.0566\n",
      "epoch 5 batch 151/185 loss = 0.0410\n",
      "epoch 5 batch 152/185 loss = 0.0499\n",
      "epoch 5 batch 153/185 loss = 0.0489\n",
      "epoch 5 batch 154/185 loss = 0.0456\n",
      "epoch 5 batch 155/185 loss = 0.0465\n",
      "epoch 5 batch 156/185 loss = 0.0463\n",
      "epoch 5 batch 157/185 loss = 0.0495\n",
      "epoch 5 batch 158/185 loss = 0.0475\n",
      "epoch 5 batch 159/185 loss = 0.0457\n",
      "epoch 5 batch 160/185 loss = 0.0468\n",
      "epoch 5 batch 161/185 loss = 0.0475\n",
      "epoch 5 batch 162/185 loss = 0.0476\n",
      "epoch 5 batch 163/185 loss = 0.0546\n",
      "epoch 5 batch 164/185 loss = 0.0447\n",
      "epoch 5 batch 165/185 loss = 0.0438\n",
      "epoch 5 batch 166/185 loss = 0.0498\n",
      "epoch 5 batch 167/185 loss = 0.0419\n",
      "epoch 5 batch 168/185 loss = 0.0558\n",
      "epoch 5 batch 169/185 loss = 0.0465\n",
      "epoch 5 batch 170/185 loss = 0.0483\n",
      "epoch 5 batch 171/185 loss = 0.0504\n",
      "epoch 5 batch 172/185 loss = 0.0410\n",
      "epoch 5 batch 173/185 loss = 0.0461\n",
      "epoch 5 batch 174/185 loss = 0.0451\n",
      "epoch 5 batch 175/185 loss = 0.0418\n",
      "epoch 5 batch 176/185 loss = 0.0501\n",
      "epoch 5 batch 177/185 loss = 0.0435\n",
      "epoch 5 batch 178/185 loss = 0.0484\n",
      "epoch 5 batch 179/185 loss = 0.0459\n",
      "epoch 5 batch 180/185 loss = 0.0428\n",
      "epoch 5 batch 181/185 loss = 0.0468\n",
      "epoch 5 batch 182/185 loss = 0.0447\n",
      "epoch 5 batch 183/185 loss = 0.0420\n",
      "epoch 5 batch 184/185 loss = 0.0380\n",
      "epoch 5 batch 185/185 loss = 0.0418\n",
      "epoch 5 train loss = 0.0607 valid loss = 0.0484\n",
      "epoch 6 batch 1/185 loss = 0.0442\n",
      "epoch 6 batch 2/185 loss = 0.0427\n",
      "epoch 6 batch 3/185 loss = 0.0466\n",
      "epoch 6 batch 4/185 loss = 0.0453\n",
      "epoch 6 batch 5/185 loss = 0.0455\n",
      "epoch 6 batch 6/185 loss = 0.0422\n",
      "epoch 6 batch 7/185 loss = 0.0408\n",
      "epoch 6 batch 8/185 loss = 0.0435\n",
      "epoch 6 batch 9/185 loss = 0.0399\n",
      "epoch 6 batch 10/185 loss = 0.0444\n",
      "epoch 6 batch 11/185 loss = 0.0413\n",
      "epoch 6 batch 12/185 loss = 0.0438\n",
      "epoch 6 batch 13/185 loss = 0.0413\n",
      "epoch 6 batch 14/185 loss = 0.0387\n",
      "epoch 6 batch 15/185 loss = 0.0437\n",
      "epoch 6 batch 16/185 loss = 0.0410\n",
      "epoch 6 batch 17/185 loss = 0.0462\n",
      "epoch 6 batch 18/185 loss = 0.0456\n",
      "epoch 6 batch 19/185 loss = 0.0443\n",
      "epoch 6 batch 20/185 loss = 0.0499\n",
      "epoch 6 batch 21/185 loss = 0.0442\n",
      "epoch 6 batch 22/185 loss = 0.0389\n",
      "epoch 6 batch 23/185 loss = 0.0387\n",
      "epoch 6 batch 24/185 loss = 0.0409\n",
      "epoch 6 batch 25/185 loss = 0.0405\n",
      "epoch 6 batch 26/185 loss = 0.0344\n",
      "epoch 6 batch 27/185 loss = 0.0436\n",
      "epoch 6 batch 28/185 loss = 0.0367\n",
      "epoch 6 batch 29/185 loss = 0.0471\n",
      "epoch 6 batch 30/185 loss = 0.0422\n",
      "epoch 6 batch 31/185 loss = 0.0399\n",
      "epoch 6 batch 32/185 loss = 0.0362\n",
      "epoch 6 batch 33/185 loss = 0.0409\n",
      "epoch 6 batch 34/185 loss = 0.0397\n",
      "epoch 6 batch 35/185 loss = 0.0406\n",
      "epoch 6 batch 36/185 loss = 0.0377\n",
      "epoch 6 batch 37/185 loss = 0.0356\n",
      "epoch 6 batch 38/185 loss = 0.0391\n",
      "epoch 6 batch 39/185 loss = 0.0354\n",
      "epoch 6 batch 40/185 loss = 0.0356\n",
      "epoch 6 batch 41/185 loss = 0.0392\n",
      "epoch 6 batch 42/185 loss = 0.0394\n",
      "epoch 6 batch 43/185 loss = 0.0432\n",
      "epoch 6 batch 44/185 loss = 0.0322\n",
      "epoch 6 batch 45/185 loss = 0.0466\n",
      "epoch 6 batch 46/185 loss = 0.0357\n",
      "epoch 6 batch 47/185 loss = 0.0372\n",
      "epoch 6 batch 48/185 loss = 0.0399\n",
      "epoch 6 batch 49/185 loss = 0.0329\n",
      "epoch 6 batch 50/185 loss = 0.0349\n",
      "epoch 6 batch 51/185 loss = 0.0409\n",
      "epoch 6 batch 52/185 loss = 0.0404\n",
      "epoch 6 batch 53/185 loss = 0.0360\n",
      "epoch 6 batch 54/185 loss = 0.0357\n",
      "epoch 6 batch 55/185 loss = 0.0387\n",
      "epoch 6 batch 56/185 loss = 0.0396\n",
      "epoch 6 batch 57/185 loss = 0.0395\n",
      "epoch 6 batch 58/185 loss = 0.0334\n",
      "epoch 6 batch 59/185 loss = 0.0361\n",
      "epoch 6 batch 60/185 loss = 0.0334\n",
      "epoch 6 batch 61/185 loss = 0.0360\n",
      "epoch 6 batch 62/185 loss = 0.0386\n",
      "epoch 6 batch 63/185 loss = 0.0319\n",
      "epoch 6 batch 64/185 loss = 0.0310\n",
      "epoch 6 batch 65/185 loss = 0.0330\n",
      "epoch 6 batch 66/185 loss = 0.0309\n",
      "epoch 6 batch 67/185 loss = 0.0360\n",
      "epoch 6 batch 68/185 loss = 0.0316\n",
      "epoch 6 batch 69/185 loss = 0.0397\n",
      "epoch 6 batch 70/185 loss = 0.0327\n",
      "epoch 6 batch 71/185 loss = 0.0321\n",
      "epoch 6 batch 72/185 loss = 0.0303\n",
      "epoch 6 batch 73/185 loss = 0.0356\n",
      "epoch 6 batch 74/185 loss = 0.0292\n",
      "epoch 6 batch 75/185 loss = 0.0365\n",
      "epoch 6 batch 76/185 loss = 0.0303\n",
      "epoch 6 batch 77/185 loss = 0.0344\n",
      "epoch 6 batch 78/185 loss = 0.0352\n",
      "epoch 6 batch 79/185 loss = 0.0340\n",
      "epoch 6 batch 80/185 loss = 0.0323\n",
      "epoch 6 batch 81/185 loss = 0.0289\n",
      "epoch 6 batch 82/185 loss = 0.0303\n",
      "epoch 6 batch 83/185 loss = 0.0303\n",
      "epoch 6 batch 84/185 loss = 0.0335\n",
      "epoch 6 batch 85/185 loss = 0.0341\n",
      "epoch 6 batch 86/185 loss = 0.0335\n",
      "epoch 6 batch 87/185 loss = 0.0350\n",
      "epoch 6 batch 88/185 loss = 0.0305\n",
      "epoch 6 batch 89/185 loss = 0.0310\n",
      "epoch 6 batch 90/185 loss = 0.0300\n",
      "epoch 6 batch 91/185 loss = 0.0339\n",
      "epoch 6 batch 92/185 loss = 0.0325\n",
      "epoch 6 batch 93/185 loss = 0.0358\n",
      "epoch 6 batch 94/185 loss = 0.0291\n",
      "epoch 6 batch 95/185 loss = 0.0336\n",
      "epoch 6 batch 96/185 loss = 0.0280\n",
      "epoch 6 batch 97/185 loss = 0.0347\n",
      "epoch 6 batch 98/185 loss = 0.0289\n",
      "epoch 6 batch 99/185 loss = 0.0279\n",
      "epoch 6 batch 100/185 loss = 0.0282\n",
      "epoch 6 batch 101/185 loss = 0.0333\n",
      "epoch 6 batch 102/185 loss = 0.0275\n",
      "epoch 6 batch 103/185 loss = 0.0362\n",
      "epoch 6 batch 104/185 loss = 0.0275\n",
      "epoch 6 batch 105/185 loss = 0.0302\n",
      "epoch 6 batch 106/185 loss = 0.0319\n",
      "epoch 6 batch 107/185 loss = 0.0263\n",
      "epoch 6 batch 108/185 loss = 0.0257\n",
      "epoch 6 batch 109/185 loss = 0.0273\n",
      "epoch 6 batch 110/185 loss = 0.0289\n",
      "epoch 6 batch 111/185 loss = 0.0279\n",
      "epoch 6 batch 112/185 loss = 0.0319\n",
      "epoch 6 batch 113/185 loss = 0.0302\n",
      "epoch 6 batch 114/185 loss = 0.0296\n",
      "epoch 6 batch 115/185 loss = 0.0271\n",
      "epoch 6 batch 116/185 loss = 0.0297\n",
      "epoch 6 batch 117/185 loss = 0.0262\n",
      "epoch 6 batch 118/185 loss = 0.0269\n",
      "epoch 6 batch 119/185 loss = 0.0308\n",
      "epoch 6 batch 120/185 loss = 0.0301\n",
      "epoch 6 batch 121/185 loss = 0.0281\n",
      "epoch 6 batch 122/185 loss = 0.0285\n",
      "epoch 6 batch 123/185 loss = 0.0263\n",
      "epoch 6 batch 124/185 loss = 0.0265\n",
      "epoch 6 batch 125/185 loss = 0.0292\n",
      "epoch 6 batch 126/185 loss = 0.0275\n",
      "epoch 6 batch 127/185 loss = 0.0274\n",
      "epoch 6 batch 128/185 loss = 0.0331\n",
      "epoch 6 batch 129/185 loss = 0.0338\n",
      "epoch 6 batch 130/185 loss = 0.0283\n",
      "epoch 6 batch 131/185 loss = 0.0369\n",
      "epoch 6 batch 132/185 loss = 0.0306\n",
      "epoch 6 batch 133/185 loss = 0.0280\n",
      "epoch 6 batch 134/185 loss = 0.0291\n",
      "epoch 6 batch 135/185 loss = 0.0332\n",
      "epoch 6 batch 136/185 loss = 0.0264\n",
      "epoch 6 batch 137/185 loss = 0.0249\n",
      "epoch 6 batch 138/185 loss = 0.0290\n",
      "epoch 6 batch 139/185 loss = 0.0303\n",
      "epoch 6 batch 140/185 loss = 0.0283\n",
      "epoch 6 batch 141/185 loss = 0.0264\n",
      "epoch 6 batch 142/185 loss = 0.0313\n",
      "epoch 6 batch 143/185 loss = 0.0277\n",
      "epoch 6 batch 144/185 loss = 0.0275\n",
      "epoch 6 batch 145/185 loss = 0.0287\n",
      "epoch 6 batch 146/185 loss = 0.0261\n",
      "epoch 6 batch 147/185 loss = 0.0227\n",
      "epoch 6 batch 148/185 loss = 0.0259\n",
      "epoch 6 batch 149/185 loss = 0.0320\n",
      "epoch 6 batch 150/185 loss = 0.0265\n",
      "epoch 6 batch 151/185 loss = 0.0270\n",
      "epoch 6 batch 152/185 loss = 0.0333\n",
      "epoch 6 batch 153/185 loss = 0.0253\n",
      "epoch 6 batch 154/185 loss = 0.0244\n",
      "epoch 6 batch 155/185 loss = 0.0245\n",
      "epoch 6 batch 156/185 loss = 0.0290\n",
      "epoch 6 batch 157/185 loss = 0.0277\n",
      "epoch 6 batch 158/185 loss = 0.0255\n",
      "epoch 6 batch 159/185 loss = 0.0254\n",
      "epoch 6 batch 160/185 loss = 0.0275\n",
      "epoch 6 batch 161/185 loss = 0.0325\n",
      "epoch 6 batch 162/185 loss = 0.0246\n",
      "epoch 6 batch 163/185 loss = 0.0279\n",
      "epoch 6 batch 164/185 loss = 0.0257\n",
      "epoch 6 batch 165/185 loss = 0.0286\n",
      "epoch 6 batch 166/185 loss = 0.0309\n",
      "epoch 6 batch 167/185 loss = 0.0246\n",
      "epoch 6 batch 168/185 loss = 0.0309\n",
      "epoch 6 batch 169/185 loss = 0.0238\n",
      "epoch 6 batch 170/185 loss = 0.0265\n",
      "epoch 6 batch 171/185 loss = 0.0269\n",
      "epoch 6 batch 172/185 loss = 0.0288\n",
      "epoch 6 batch 173/185 loss = 0.0314\n",
      "epoch 6 batch 174/185 loss = 0.0224\n",
      "epoch 6 batch 175/185 loss = 0.0313\n",
      "epoch 6 batch 176/185 loss = 0.0257\n",
      "epoch 6 batch 177/185 loss = 0.0258\n",
      "epoch 6 batch 178/185 loss = 0.0243\n",
      "epoch 6 batch 179/185 loss = 0.0251\n",
      "epoch 6 batch 180/185 loss = 0.0276\n",
      "epoch 6 batch 181/185 loss = 0.0279\n",
      "epoch 6 batch 182/185 loss = 0.0299\n",
      "epoch 6 batch 183/185 loss = 0.0244\n",
      "epoch 6 batch 184/185 loss = 0.0216\n",
      "epoch 6 batch 185/185 loss = 0.0245\n",
      "epoch 6 train loss = 0.0330 valid loss = 0.0254\n",
      "epoch 7 batch 1/185 loss = 0.0291\n",
      "epoch 7 batch 2/185 loss = 0.0213\n",
      "epoch 7 batch 3/185 loss = 0.0207\n",
      "epoch 7 batch 4/185 loss = 0.0226\n",
      "epoch 7 batch 5/185 loss = 0.0320\n",
      "epoch 7 batch 6/185 loss = 0.0217\n",
      "epoch 7 batch 7/185 loss = 0.0219\n",
      "epoch 7 batch 8/185 loss = 0.0229\n",
      "epoch 7 batch 9/185 loss = 0.0237\n",
      "epoch 7 batch 10/185 loss = 0.0268\n",
      "epoch 7 batch 11/185 loss = 0.0211\n",
      "epoch 7 batch 12/185 loss = 0.0238\n",
      "epoch 7 batch 13/185 loss = 0.0268\n",
      "epoch 7 batch 14/185 loss = 0.0244\n",
      "epoch 7 batch 15/185 loss = 0.0279\n",
      "epoch 7 batch 16/185 loss = 0.0249\n",
      "epoch 7 batch 17/185 loss = 0.0230\n",
      "epoch 7 batch 18/185 loss = 0.0211\n",
      "epoch 7 batch 19/185 loss = 0.0193\n",
      "epoch 7 batch 20/185 loss = 0.0271\n",
      "epoch 7 batch 21/185 loss = 0.0243\n",
      "epoch 7 batch 22/185 loss = 0.0241\n",
      "epoch 7 batch 23/185 loss = 0.0194\n",
      "epoch 7 batch 24/185 loss = 0.0176\n",
      "epoch 7 batch 25/185 loss = 0.0235\n",
      "epoch 7 batch 26/185 loss = 0.0214\n",
      "epoch 7 batch 27/185 loss = 0.0262\n",
      "epoch 7 batch 28/185 loss = 0.0189\n",
      "epoch 7 batch 29/185 loss = 0.0220\n",
      "epoch 7 batch 30/185 loss = 0.0280\n",
      "epoch 7 batch 31/185 loss = 0.0230\n",
      "epoch 7 batch 32/185 loss = 0.0244\n",
      "epoch 7 batch 33/185 loss = 0.0230\n",
      "epoch 7 batch 34/185 loss = 0.0237\n",
      "epoch 7 batch 35/185 loss = 0.0271\n",
      "epoch 7 batch 36/185 loss = 0.0244\n",
      "epoch 7 batch 37/185 loss = 0.0224\n",
      "epoch 7 batch 38/185 loss = 0.0203\n",
      "epoch 7 batch 39/185 loss = 0.0230\n",
      "epoch 7 batch 40/185 loss = 0.0230\n",
      "epoch 7 batch 41/185 loss = 0.0204\n",
      "epoch 7 batch 42/185 loss = 0.0234\n",
      "epoch 7 batch 43/185 loss = 0.0219\n",
      "epoch 7 batch 44/185 loss = 0.0192\n",
      "epoch 7 batch 45/185 loss = 0.0167\n",
      "epoch 7 batch 46/185 loss = 0.0285\n",
      "epoch 7 batch 47/185 loss = 0.0176\n",
      "epoch 7 batch 48/185 loss = 0.0242\n",
      "epoch 7 batch 49/185 loss = 0.0173\n",
      "epoch 7 batch 50/185 loss = 0.0218\n",
      "epoch 7 batch 51/185 loss = 0.0177\n",
      "epoch 7 batch 52/185 loss = 0.0203\n",
      "epoch 7 batch 53/185 loss = 0.0241\n",
      "epoch 7 batch 54/185 loss = 0.0213\n",
      "epoch 7 batch 55/185 loss = 0.0220\n",
      "epoch 7 batch 56/185 loss = 0.0182\n",
      "epoch 7 batch 57/185 loss = 0.0184\n",
      "epoch 7 batch 58/185 loss = 0.0256\n",
      "epoch 7 batch 59/185 loss = 0.0171\n",
      "epoch 7 batch 60/185 loss = 0.0193\n",
      "epoch 7 batch 61/185 loss = 0.0229\n",
      "epoch 7 batch 62/185 loss = 0.0206\n",
      "epoch 7 batch 63/185 loss = 0.0179\n",
      "epoch 7 batch 64/185 loss = 0.0203\n",
      "epoch 7 batch 65/185 loss = 0.0172\n",
      "epoch 7 batch 66/185 loss = 0.0251\n",
      "epoch 7 batch 67/185 loss = 0.0187\n",
      "epoch 7 batch 68/185 loss = 0.0221\n",
      "epoch 7 batch 69/185 loss = 0.0225\n",
      "epoch 7 batch 70/185 loss = 0.0204\n",
      "epoch 7 batch 71/185 loss = 0.0247\n",
      "epoch 7 batch 72/185 loss = 0.0222\n",
      "epoch 7 batch 73/185 loss = 0.0188\n",
      "epoch 7 batch 74/185 loss = 0.0205\n",
      "epoch 7 batch 75/185 loss = 0.0189\n",
      "epoch 7 batch 76/185 loss = 0.0194\n",
      "epoch 7 batch 77/185 loss = 0.0226\n",
      "epoch 7 batch 78/185 loss = 0.0200\n",
      "epoch 7 batch 79/185 loss = 0.0204\n",
      "epoch 7 batch 80/185 loss = 0.0193\n",
      "epoch 7 batch 81/185 loss = 0.0175\n",
      "epoch 7 batch 82/185 loss = 0.0185\n",
      "epoch 7 batch 83/185 loss = 0.0221\n",
      "epoch 7 batch 84/185 loss = 0.0204\n",
      "epoch 7 batch 85/185 loss = 0.0194\n",
      "epoch 7 batch 86/185 loss = 0.0204\n",
      "epoch 7 batch 87/185 loss = 0.0201\n",
      "epoch 7 batch 88/185 loss = 0.0221\n",
      "epoch 7 batch 89/185 loss = 0.0220\n",
      "epoch 7 batch 90/185 loss = 0.0182\n",
      "epoch 7 batch 91/185 loss = 0.0179\n",
      "epoch 7 batch 92/185 loss = 0.0192\n",
      "epoch 7 batch 93/185 loss = 0.0188\n",
      "epoch 7 batch 94/185 loss = 0.0241\n",
      "epoch 7 batch 95/185 loss = 0.0235\n",
      "epoch 7 batch 96/185 loss = 0.0192\n",
      "epoch 7 batch 97/185 loss = 0.0205\n",
      "epoch 7 batch 98/185 loss = 0.0200\n",
      "epoch 7 batch 99/185 loss = 0.0191\n",
      "epoch 7 batch 100/185 loss = 0.0170\n",
      "epoch 7 batch 101/185 loss = 0.0191\n",
      "epoch 7 batch 102/185 loss = 0.0207\n",
      "epoch 7 batch 103/185 loss = 0.0172\n",
      "epoch 7 batch 104/185 loss = 0.0210\n",
      "epoch 7 batch 105/185 loss = 0.0193\n",
      "epoch 7 batch 106/185 loss = 0.0179\n",
      "epoch 7 batch 107/185 loss = 0.0212\n",
      "epoch 7 batch 108/185 loss = 0.0180\n",
      "epoch 7 batch 109/185 loss = 0.0199\n",
      "epoch 7 batch 110/185 loss = 0.0188\n",
      "epoch 7 batch 111/185 loss = 0.0166\n",
      "epoch 7 batch 112/185 loss = 0.0162\n",
      "epoch 7 batch 113/185 loss = 0.0201\n",
      "epoch 7 batch 114/185 loss = 0.0210\n",
      "epoch 7 batch 115/185 loss = 0.0171\n",
      "epoch 7 batch 116/185 loss = 0.0195\n",
      "epoch 7 batch 117/185 loss = 0.0178\n",
      "epoch 7 batch 118/185 loss = 0.0237\n",
      "epoch 7 batch 119/185 loss = 0.0186\n",
      "epoch 7 batch 120/185 loss = 0.0166\n",
      "epoch 7 batch 121/185 loss = 0.0155\n",
      "epoch 7 batch 122/185 loss = 0.0177\n",
      "epoch 7 batch 123/185 loss = 0.0192\n",
      "epoch 7 batch 124/185 loss = 0.0167\n",
      "epoch 7 batch 125/185 loss = 0.0167\n",
      "epoch 7 batch 126/185 loss = 0.0203\n",
      "epoch 7 batch 127/185 loss = 0.0195\n",
      "epoch 7 batch 128/185 loss = 0.0150\n",
      "epoch 7 batch 129/185 loss = 0.0187\n",
      "epoch 7 batch 130/185 loss = 0.0190\n",
      "epoch 7 batch 131/185 loss = 0.0181\n",
      "epoch 7 batch 132/185 loss = 0.0156\n",
      "epoch 7 batch 133/185 loss = 0.0193\n",
      "epoch 7 batch 134/185 loss = 0.0137\n",
      "epoch 7 batch 135/185 loss = 0.0160\n",
      "epoch 7 batch 136/185 loss = 0.0226\n",
      "epoch 7 batch 137/185 loss = 0.0198\n",
      "epoch 7 batch 138/185 loss = 0.0202\n",
      "epoch 7 batch 139/185 loss = 0.0163\n",
      "epoch 7 batch 140/185 loss = 0.0146\n",
      "epoch 7 batch 141/185 loss = 0.0157\n",
      "epoch 7 batch 142/185 loss = 0.0173\n",
      "epoch 7 batch 143/185 loss = 0.0176\n",
      "epoch 7 batch 144/185 loss = 0.0168\n",
      "epoch 7 batch 145/185 loss = 0.0164\n",
      "epoch 7 batch 146/185 loss = 0.0162\n",
      "epoch 7 batch 147/185 loss = 0.0188\n",
      "epoch 7 batch 148/185 loss = 0.0162\n",
      "epoch 7 batch 149/185 loss = 0.0182\n",
      "epoch 7 batch 150/185 loss = 0.0213\n",
      "epoch 7 batch 151/185 loss = 0.0169\n",
      "epoch 7 batch 152/185 loss = 0.0151\n",
      "epoch 7 batch 153/185 loss = 0.0177\n",
      "epoch 7 batch 154/185 loss = 0.0219\n",
      "epoch 7 batch 155/185 loss = 0.0199\n",
      "epoch 7 batch 156/185 loss = 0.0146\n",
      "epoch 7 batch 157/185 loss = 0.0182\n",
      "epoch 7 batch 158/185 loss = 0.0161\n",
      "epoch 7 batch 159/185 loss = 0.0153\n",
      "epoch 7 batch 160/185 loss = 0.0149\n",
      "epoch 7 batch 161/185 loss = 0.0162\n",
      "epoch 7 batch 162/185 loss = 0.0168\n",
      "epoch 7 batch 163/185 loss = 0.0195\n",
      "epoch 7 batch 164/185 loss = 0.0154\n",
      "epoch 7 batch 165/185 loss = 0.0177\n",
      "epoch 7 batch 166/185 loss = 0.0182\n",
      "epoch 7 batch 167/185 loss = 0.0159\n",
      "epoch 7 batch 168/185 loss = 0.0167\n",
      "epoch 7 batch 169/185 loss = 0.0197\n",
      "epoch 7 batch 170/185 loss = 0.0168\n",
      "epoch 7 batch 171/185 loss = 0.0123\n",
      "epoch 7 batch 172/185 loss = 0.0187\n",
      "epoch 7 batch 173/185 loss = 0.0169\n",
      "epoch 7 batch 174/185 loss = 0.0208\n",
      "epoch 7 batch 175/185 loss = 0.0204\n",
      "epoch 7 batch 176/185 loss = 0.0171\n",
      "epoch 7 batch 177/185 loss = 0.0137\n",
      "epoch 7 batch 178/185 loss = 0.0163\n",
      "epoch 7 batch 179/185 loss = 0.0192\n",
      "epoch 7 batch 180/185 loss = 0.0154\n",
      "epoch 7 batch 181/185 loss = 0.0152\n",
      "epoch 7 batch 182/185 loss = 0.0133\n",
      "epoch 7 batch 183/185 loss = 0.0164\n",
      "epoch 7 batch 184/185 loss = 0.0193\n",
      "epoch 7 batch 185/185 loss = 0.0207\n",
      "epoch 7 train loss = 0.0199 valid loss = 0.0179\n",
      "epoch 8 batch 1/185 loss = 0.0142\n",
      "epoch 8 batch 2/185 loss = 0.0156\n",
      "epoch 8 batch 3/185 loss = 0.0169\n",
      "epoch 8 batch 4/185 loss = 0.0169\n",
      "epoch 8 batch 5/185 loss = 0.0167\n",
      "epoch 8 batch 6/185 loss = 0.0147\n",
      "epoch 8 batch 7/185 loss = 0.0129\n",
      "epoch 8 batch 8/185 loss = 0.0175\n",
      "epoch 8 batch 9/185 loss = 0.0153\n",
      "epoch 8 batch 10/185 loss = 0.0175\n",
      "epoch 8 batch 11/185 loss = 0.0143\n",
      "epoch 8 batch 12/185 loss = 0.0156\n",
      "epoch 8 batch 13/185 loss = 0.0177\n",
      "epoch 8 batch 14/185 loss = 0.0162\n",
      "epoch 8 batch 15/185 loss = 0.0151\n",
      "epoch 8 batch 16/185 loss = 0.0172\n",
      "epoch 8 batch 17/185 loss = 0.0140\n",
      "epoch 8 batch 18/185 loss = 0.0209\n",
      "epoch 8 batch 19/185 loss = 0.0146\n",
      "epoch 8 batch 20/185 loss = 0.0183\n",
      "epoch 8 batch 21/185 loss = 0.0156\n",
      "epoch 8 batch 22/185 loss = 0.0162\n",
      "epoch 8 batch 23/185 loss = 0.0126\n",
      "epoch 8 batch 24/185 loss = 0.0148\n",
      "epoch 8 batch 25/185 loss = 0.0160\n",
      "epoch 8 batch 26/185 loss = 0.0164\n",
      "epoch 8 batch 27/185 loss = 0.0138\n",
      "epoch 8 batch 28/185 loss = 0.0162\n",
      "epoch 8 batch 29/185 loss = 0.0134\n",
      "epoch 8 batch 30/185 loss = 0.0177\n",
      "epoch 8 batch 31/185 loss = 0.0139\n",
      "epoch 8 batch 32/185 loss = 0.0156\n",
      "epoch 8 batch 33/185 loss = 0.0151\n",
      "epoch 8 batch 34/185 loss = 0.0132\n",
      "epoch 8 batch 35/185 loss = 0.0145\n",
      "epoch 8 batch 36/185 loss = 0.0168\n",
      "epoch 8 batch 37/185 loss = 0.0130\n",
      "epoch 8 batch 38/185 loss = 0.0133\n",
      "epoch 8 batch 39/185 loss = 0.0149\n",
      "epoch 8 batch 40/185 loss = 0.0162\n",
      "epoch 8 batch 41/185 loss = 0.0149\n",
      "epoch 8 batch 42/185 loss = 0.0123\n",
      "epoch 8 batch 43/185 loss = 0.0136\n",
      "epoch 8 batch 44/185 loss = 0.0152\n",
      "epoch 8 batch 45/185 loss = 0.0153\n",
      "epoch 8 batch 46/185 loss = 0.0144\n",
      "epoch 8 batch 47/185 loss = 0.0162\n",
      "epoch 8 batch 48/185 loss = 0.0174\n",
      "epoch 8 batch 49/185 loss = 0.0113\n",
      "epoch 8 batch 50/185 loss = 0.0161\n",
      "epoch 8 batch 51/185 loss = 0.0138\n",
      "epoch 8 batch 52/185 loss = 0.0126\n",
      "epoch 8 batch 53/185 loss = 0.0158\n",
      "epoch 8 batch 54/185 loss = 0.0158\n",
      "epoch 8 batch 55/185 loss = 0.0175\n",
      "epoch 8 batch 56/185 loss = 0.0149\n",
      "epoch 8 batch 57/185 loss = 0.0133\n",
      "epoch 8 batch 58/185 loss = 0.0169\n",
      "epoch 8 batch 59/185 loss = 0.0139\n",
      "epoch 8 batch 60/185 loss = 0.0139\n",
      "epoch 8 batch 61/185 loss = 0.0142\n",
      "epoch 8 batch 62/185 loss = 0.0123\n",
      "epoch 8 batch 63/185 loss = 0.0128\n",
      "epoch 8 batch 64/185 loss = 0.0161\n",
      "epoch 8 batch 65/185 loss = 0.0166\n",
      "epoch 8 batch 66/185 loss = 0.0147\n",
      "epoch 8 batch 67/185 loss = 0.0153\n",
      "epoch 8 batch 68/185 loss = 0.0123\n",
      "epoch 8 batch 69/185 loss = 0.0135\n",
      "epoch 8 batch 70/185 loss = 0.0138\n",
      "epoch 8 batch 71/185 loss = 0.0151\n",
      "epoch 8 batch 72/185 loss = 0.0124\n",
      "epoch 8 batch 73/185 loss = 0.0149\n",
      "epoch 8 batch 74/185 loss = 0.0153\n",
      "epoch 8 batch 75/185 loss = 0.0143\n",
      "epoch 8 batch 76/185 loss = 0.0170\n",
      "epoch 8 batch 77/185 loss = 0.0129\n",
      "epoch 8 batch 78/185 loss = 0.0160\n",
      "epoch 8 batch 79/185 loss = 0.0180\n",
      "epoch 8 batch 80/185 loss = 0.0134\n",
      "epoch 8 batch 81/185 loss = 0.0129\n",
      "epoch 8 batch 82/185 loss = 0.0198\n",
      "epoch 8 batch 83/185 loss = 0.0161\n",
      "epoch 8 batch 84/185 loss = 0.0130\n",
      "epoch 8 batch 85/185 loss = 0.0157\n",
      "epoch 8 batch 86/185 loss = 0.0128\n",
      "epoch 8 batch 87/185 loss = 0.0137\n",
      "epoch 8 batch 88/185 loss = 0.0123\n",
      "epoch 8 batch 89/185 loss = 0.0144\n",
      "epoch 8 batch 90/185 loss = 0.0142\n",
      "epoch 8 batch 91/185 loss = 0.0111\n",
      "epoch 8 batch 92/185 loss = 0.0167\n",
      "epoch 8 batch 93/185 loss = 0.0127\n",
      "epoch 8 batch 94/185 loss = 0.0169\n",
      "epoch 8 batch 95/185 loss = 0.0120\n",
      "epoch 8 batch 96/185 loss = 0.0123\n",
      "epoch 8 batch 97/185 loss = 0.0168\n",
      "epoch 8 batch 98/185 loss = 0.0143\n",
      "epoch 8 batch 99/185 loss = 0.0186\n",
      "epoch 8 batch 100/185 loss = 0.0143\n",
      "epoch 8 batch 101/185 loss = 0.0177\n",
      "epoch 8 batch 102/185 loss = 0.0123\n",
      "epoch 8 batch 103/185 loss = 0.0139\n",
      "epoch 8 batch 104/185 loss = 0.0132\n",
      "epoch 8 batch 105/185 loss = 0.0149\n",
      "epoch 8 batch 106/185 loss = 0.0112\n",
      "epoch 8 batch 107/185 loss = 0.0168\n",
      "epoch 8 batch 108/185 loss = 0.0135\n",
      "epoch 8 batch 109/185 loss = 0.0125\n",
      "epoch 8 batch 110/185 loss = 0.0137\n",
      "epoch 8 batch 111/185 loss = 0.0154\n",
      "epoch 8 batch 112/185 loss = 0.0141\n",
      "epoch 8 batch 113/185 loss = 0.0164\n",
      "epoch 8 batch 114/185 loss = 0.0122\n",
      "epoch 8 batch 115/185 loss = 0.0145\n",
      "epoch 8 batch 116/185 loss = 0.0127\n",
      "epoch 8 batch 117/185 loss = 0.0126\n",
      "epoch 8 batch 118/185 loss = 0.0142\n",
      "epoch 8 batch 119/185 loss = 0.0139\n",
      "epoch 8 batch 120/185 loss = 0.0120\n",
      "epoch 8 batch 121/185 loss = 0.0150\n",
      "epoch 8 batch 122/185 loss = 0.0118\n",
      "epoch 8 batch 123/185 loss = 0.0143\n",
      "epoch 8 batch 124/185 loss = 0.0155\n",
      "epoch 8 batch 125/185 loss = 0.0159\n",
      "epoch 8 batch 126/185 loss = 0.0120\n",
      "epoch 8 batch 127/185 loss = 0.0143\n",
      "epoch 8 batch 128/185 loss = 0.0134\n",
      "epoch 8 batch 129/185 loss = 0.0134\n",
      "epoch 8 batch 130/185 loss = 0.0136\n",
      "epoch 8 batch 131/185 loss = 0.0140\n",
      "epoch 8 batch 132/185 loss = 0.0138\n",
      "epoch 8 batch 133/185 loss = 0.0123\n",
      "epoch 8 batch 134/185 loss = 0.0133\n",
      "epoch 8 batch 135/185 loss = 0.0136\n",
      "epoch 8 batch 136/185 loss = 0.0126\n",
      "epoch 8 batch 137/185 loss = 0.0125\n",
      "epoch 8 batch 138/185 loss = 0.0116\n",
      "epoch 8 batch 139/185 loss = 0.0124\n",
      "epoch 8 batch 140/185 loss = 0.0151\n",
      "epoch 8 batch 141/185 loss = 0.0118\n",
      "epoch 8 batch 142/185 loss = 0.0129\n",
      "epoch 8 batch 143/185 loss = 0.0138\n",
      "epoch 8 batch 144/185 loss = 0.0160\n",
      "epoch 8 batch 145/185 loss = 0.0125\n",
      "epoch 8 batch 146/185 loss = 0.0146\n",
      "epoch 8 batch 147/185 loss = 0.0166\n",
      "epoch 8 batch 148/185 loss = 0.0158\n",
      "epoch 8 batch 149/185 loss = 0.0109\n",
      "epoch 8 batch 150/185 loss = 0.0125\n",
      "epoch 8 batch 151/185 loss = 0.0168\n",
      "epoch 8 batch 152/185 loss = 0.0123\n",
      "epoch 8 batch 153/185 loss = 0.0115\n",
      "epoch 8 batch 154/185 loss = 0.0158\n",
      "epoch 8 batch 155/185 loss = 0.0106\n",
      "epoch 8 batch 156/185 loss = 0.0117\n",
      "epoch 8 batch 157/185 loss = 0.0153\n",
      "epoch 8 batch 158/185 loss = 0.0140\n",
      "epoch 8 batch 159/185 loss = 0.0128\n",
      "epoch 8 batch 160/185 loss = 0.0144\n",
      "epoch 8 batch 161/185 loss = 0.0148\n",
      "epoch 8 batch 162/185 loss = 0.0146\n",
      "epoch 8 batch 163/185 loss = 0.0151\n",
      "epoch 8 batch 164/185 loss = 0.0142\n",
      "epoch 8 batch 165/185 loss = 0.0119\n",
      "epoch 8 batch 166/185 loss = 0.0147\n",
      "epoch 8 batch 167/185 loss = 0.0173\n",
      "epoch 8 batch 168/185 loss = 0.0118\n",
      "epoch 8 batch 169/185 loss = 0.0121\n",
      "epoch 8 batch 170/185 loss = 0.0127\n",
      "epoch 8 batch 171/185 loss = 0.0119\n",
      "epoch 8 batch 172/185 loss = 0.0149\n",
      "epoch 8 batch 173/185 loss = 0.0129\n",
      "epoch 8 batch 174/185 loss = 0.0124\n",
      "epoch 8 batch 175/185 loss = 0.0128\n",
      "epoch 8 batch 176/185 loss = 0.0106\n",
      "epoch 8 batch 177/185 loss = 0.0117\n",
      "epoch 8 batch 178/185 loss = 0.0158\n",
      "epoch 8 batch 179/185 loss = 0.0151\n",
      "epoch 8 batch 180/185 loss = 0.0155\n",
      "epoch 8 batch 181/185 loss = 0.0140\n",
      "epoch 8 batch 182/185 loss = 0.0103\n",
      "epoch 8 batch 183/185 loss = 0.0124\n",
      "epoch 8 batch 184/185 loss = 0.0135\n",
      "epoch 8 batch 185/185 loss = 0.0167\n",
      "epoch 8 train loss = 0.0144 valid loss = 0.0141\n",
      "epoch 9 batch 1/185 loss = 0.0124\n",
      "epoch 9 batch 2/185 loss = 0.0112\n",
      "epoch 9 batch 3/185 loss = 0.0115\n",
      "epoch 9 batch 4/185 loss = 0.0161\n",
      "epoch 9 batch 5/185 loss = 0.0153\n",
      "epoch 9 batch 6/185 loss = 0.0133\n",
      "epoch 9 batch 7/185 loss = 0.0138\n",
      "epoch 9 batch 8/185 loss = 0.0133\n",
      "epoch 9 batch 9/185 loss = 0.0126\n",
      "epoch 9 batch 10/185 loss = 0.0104\n",
      "epoch 9 batch 11/185 loss = 0.0116\n",
      "epoch 9 batch 12/185 loss = 0.0148\n",
      "epoch 9 batch 13/185 loss = 0.0112\n",
      "epoch 9 batch 14/185 loss = 0.0116\n",
      "epoch 9 batch 15/185 loss = 0.0111\n",
      "epoch 9 batch 16/185 loss = 0.0141\n",
      "epoch 9 batch 17/185 loss = 0.0131\n",
      "epoch 9 batch 18/185 loss = 0.0113\n",
      "epoch 9 batch 19/185 loss = 0.0159\n",
      "epoch 9 batch 20/185 loss = 0.0117\n",
      "epoch 9 batch 21/185 loss = 0.0107\n",
      "epoch 9 batch 22/185 loss = 0.0108\n",
      "epoch 9 batch 23/185 loss = 0.0151\n",
      "epoch 9 batch 24/185 loss = 0.0113\n",
      "epoch 9 batch 25/185 loss = 0.0126\n",
      "epoch 9 batch 26/185 loss = 0.0116\n",
      "epoch 9 batch 27/185 loss = 0.0133\n",
      "epoch 9 batch 28/185 loss = 0.0102\n",
      "epoch 9 batch 29/185 loss = 0.0108\n",
      "epoch 9 batch 30/185 loss = 0.0144\n",
      "epoch 9 batch 31/185 loss = 0.0113\n",
      "epoch 9 batch 32/185 loss = 0.0110\n",
      "epoch 9 batch 33/185 loss = 0.0111\n",
      "epoch 9 batch 34/185 loss = 0.0144\n",
      "epoch 9 batch 35/185 loss = 0.0127\n",
      "epoch 9 batch 36/185 loss = 0.0115\n",
      "epoch 9 batch 37/185 loss = 0.0126\n",
      "epoch 9 batch 38/185 loss = 0.0098\n",
      "epoch 9 batch 39/185 loss = 0.0121\n",
      "epoch 9 batch 40/185 loss = 0.0118\n",
      "epoch 9 batch 41/185 loss = 0.0147\n",
      "epoch 9 batch 42/185 loss = 0.0131\n",
      "epoch 9 batch 43/185 loss = 0.0141\n",
      "epoch 9 batch 44/185 loss = 0.0123\n",
      "epoch 9 batch 45/185 loss = 0.0153\n",
      "epoch 9 batch 46/185 loss = 0.0129\n",
      "epoch 9 batch 47/185 loss = 0.0127\n",
      "epoch 9 batch 48/185 loss = 0.0106\n",
      "epoch 9 batch 49/185 loss = 0.0138\n",
      "epoch 9 batch 50/185 loss = 0.0118\n",
      "epoch 9 batch 51/185 loss = 0.0111\n",
      "epoch 9 batch 52/185 loss = 0.0157\n",
      "epoch 9 batch 53/185 loss = 0.0130\n",
      "epoch 9 batch 54/185 loss = 0.0135\n",
      "epoch 9 batch 55/185 loss = 0.0097\n",
      "epoch 9 batch 56/185 loss = 0.0123\n",
      "epoch 9 batch 57/185 loss = 0.0157\n",
      "epoch 9 batch 58/185 loss = 0.0124\n",
      "epoch 9 batch 59/185 loss = 0.0129\n",
      "epoch 9 batch 60/185 loss = 0.0135\n",
      "epoch 9 batch 61/185 loss = 0.0125\n",
      "epoch 9 batch 62/185 loss = 0.0106\n",
      "epoch 9 batch 63/185 loss = 0.0121\n",
      "epoch 9 batch 64/185 loss = 0.0120\n",
      "epoch 9 batch 65/185 loss = 0.0122\n",
      "epoch 9 batch 66/185 loss = 0.0126\n",
      "epoch 9 batch 67/185 loss = 0.0112\n",
      "epoch 9 batch 68/185 loss = 0.0120\n",
      "epoch 9 batch 69/185 loss = 0.0129\n",
      "epoch 9 batch 70/185 loss = 0.0114\n",
      "epoch 9 batch 71/185 loss = 0.0119\n",
      "epoch 9 batch 72/185 loss = 0.0165\n",
      "epoch 9 batch 73/185 loss = 0.0107\n",
      "epoch 9 batch 74/185 loss = 0.0130\n",
      "epoch 9 batch 75/185 loss = 0.0118\n",
      "epoch 9 batch 76/185 loss = 0.0123\n",
      "epoch 9 batch 77/185 loss = 0.0105\n",
      "epoch 9 batch 78/185 loss = 0.0117\n",
      "epoch 9 batch 79/185 loss = 0.0133\n",
      "epoch 9 batch 80/185 loss = 0.0146\n",
      "epoch 9 batch 81/185 loss = 0.0120\n",
      "epoch 9 batch 82/185 loss = 0.0120\n",
      "epoch 9 batch 83/185 loss = 0.0110\n",
      "epoch 9 batch 84/185 loss = 0.0162\n",
      "epoch 9 batch 85/185 loss = 0.0106\n",
      "epoch 9 batch 86/185 loss = 0.0125\n",
      "epoch 9 batch 87/185 loss = 0.0131\n",
      "epoch 9 batch 88/185 loss = 0.0124\n",
      "epoch 9 batch 89/185 loss = 0.0118\n",
      "epoch 9 batch 90/185 loss = 0.0130\n",
      "epoch 9 batch 91/185 loss = 0.0119\n",
      "epoch 9 batch 92/185 loss = 0.0105\n",
      "epoch 9 batch 93/185 loss = 0.0100\n",
      "epoch 9 batch 94/185 loss = 0.0104\n",
      "epoch 9 batch 95/185 loss = 0.0106\n",
      "epoch 9 batch 96/185 loss = 0.0135\n",
      "epoch 9 batch 97/185 loss = 0.0136\n",
      "epoch 9 batch 98/185 loss = 0.0116\n",
      "epoch 9 batch 99/185 loss = 0.0128\n",
      "epoch 9 batch 100/185 loss = 0.0112\n",
      "epoch 9 batch 101/185 loss = 0.0116\n",
      "epoch 9 batch 102/185 loss = 0.0137\n",
      "epoch 9 batch 103/185 loss = 0.0125\n",
      "epoch 9 batch 104/185 loss = 0.0113\n",
      "epoch 9 batch 105/185 loss = 0.0144\n",
      "epoch 9 batch 106/185 loss = 0.0093\n",
      "epoch 9 batch 107/185 loss = 0.0133\n",
      "epoch 9 batch 108/185 loss = 0.0099\n",
      "epoch 9 batch 109/185 loss = 0.0094\n",
      "epoch 9 batch 110/185 loss = 0.0115\n",
      "epoch 9 batch 111/185 loss = 0.0118\n",
      "epoch 9 batch 112/185 loss = 0.0124\n",
      "epoch 9 batch 113/185 loss = 0.0129\n",
      "epoch 9 batch 114/185 loss = 0.0117\n",
      "epoch 9 batch 115/185 loss = 0.0152\n",
      "epoch 9 batch 116/185 loss = 0.0140\n",
      "epoch 9 batch 117/185 loss = 0.0103\n",
      "epoch 9 batch 118/185 loss = 0.0125\n",
      "epoch 9 batch 119/185 loss = 0.0137\n",
      "epoch 9 batch 120/185 loss = 0.0101\n",
      "epoch 9 batch 121/185 loss = 0.0113\n",
      "epoch 9 batch 122/185 loss = 0.0115\n",
      "epoch 9 batch 123/185 loss = 0.0130\n",
      "epoch 9 batch 124/185 loss = 0.0122\n",
      "epoch 9 batch 125/185 loss = 0.0098\n",
      "epoch 9 batch 126/185 loss = 0.0130\n",
      "epoch 9 batch 127/185 loss = 0.0111\n",
      "epoch 9 batch 128/185 loss = 0.0113\n",
      "epoch 9 batch 129/185 loss = 0.0105\n",
      "epoch 9 batch 130/185 loss = 0.0113\n",
      "epoch 9 batch 131/185 loss = 0.0117\n",
      "epoch 9 batch 132/185 loss = 0.0104\n",
      "epoch 9 batch 133/185 loss = 0.0120\n",
      "epoch 9 batch 134/185 loss = 0.0107\n",
      "epoch 9 batch 135/185 loss = 0.0112\n",
      "epoch 9 batch 136/185 loss = 0.0118\n",
      "epoch 9 batch 137/185 loss = 0.0131\n",
      "epoch 9 batch 138/185 loss = 0.0127\n",
      "epoch 9 batch 139/185 loss = 0.0134\n",
      "epoch 9 batch 140/185 loss = 0.0112\n",
      "epoch 9 batch 141/185 loss = 0.0135\n",
      "epoch 9 batch 142/185 loss = 0.0106\n",
      "epoch 9 batch 143/185 loss = 0.0119\n",
      "epoch 9 batch 144/185 loss = 0.0107\n",
      "epoch 9 batch 145/185 loss = 0.0113\n",
      "epoch 9 batch 146/185 loss = 0.0154\n",
      "epoch 9 batch 147/185 loss = 0.0115\n",
      "epoch 9 batch 148/185 loss = 0.0125\n",
      "epoch 9 batch 149/185 loss = 0.0106\n",
      "epoch 9 batch 150/185 loss = 0.0112\n",
      "epoch 9 batch 151/185 loss = 0.0098\n",
      "epoch 9 batch 152/185 loss = 0.0129\n",
      "epoch 9 batch 153/185 loss = 0.0155\n",
      "epoch 9 batch 154/185 loss = 0.0127\n",
      "epoch 9 batch 155/185 loss = 0.0101\n",
      "epoch 9 batch 156/185 loss = 0.0129\n",
      "epoch 9 batch 157/185 loss = 0.0144\n",
      "epoch 9 batch 158/185 loss = 0.0132\n",
      "epoch 9 batch 159/185 loss = 0.0104\n",
      "epoch 9 batch 160/185 loss = 0.0110\n",
      "epoch 9 batch 161/185 loss = 0.0146\n",
      "epoch 9 batch 162/185 loss = 0.0129\n",
      "epoch 9 batch 163/185 loss = 0.0103\n",
      "epoch 9 batch 164/185 loss = 0.0111\n",
      "epoch 9 batch 165/185 loss = 0.0106\n",
      "epoch 9 batch 166/185 loss = 0.0113\n",
      "epoch 9 batch 167/185 loss = 0.0117\n",
      "epoch 9 batch 168/185 loss = 0.0107\n",
      "epoch 9 batch 169/185 loss = 0.0104\n",
      "epoch 9 batch 170/185 loss = 0.0144\n",
      "epoch 9 batch 171/185 loss = 0.0124\n",
      "epoch 9 batch 172/185 loss = 0.0126\n",
      "epoch 9 batch 173/185 loss = 0.0127\n",
      "epoch 9 batch 174/185 loss = 0.0127\n",
      "epoch 9 batch 175/185 loss = 0.0164\n",
      "epoch 9 batch 176/185 loss = 0.0120\n",
      "epoch 9 batch 177/185 loss = 0.0096\n",
      "epoch 9 batch 178/185 loss = 0.0136\n",
      "epoch 9 batch 179/185 loss = 0.0114\n",
      "epoch 9 batch 180/185 loss = 0.0129\n",
      "epoch 9 batch 181/185 loss = 0.0113\n",
      "epoch 9 batch 182/185 loss = 0.0130\n",
      "epoch 9 batch 183/185 loss = 0.0129\n",
      "epoch 9 batch 184/185 loss = 0.0128\n",
      "epoch 9 batch 185/185 loss = 0.0133\n",
      "epoch 9 train loss = 0.0123 valid loss = 0.0133\n",
      "epoch 10 batch 1/185 loss = 0.0123\n",
      "epoch 10 batch 2/185 loss = 0.0120\n",
      "epoch 10 batch 3/185 loss = 0.0117\n",
      "epoch 10 batch 4/185 loss = 0.0137\n",
      "epoch 10 batch 5/185 loss = 0.0107\n",
      "epoch 10 batch 6/185 loss = 0.0096\n",
      "epoch 10 batch 7/185 loss = 0.0126\n",
      "epoch 10 batch 8/185 loss = 0.0100\n",
      "epoch 10 batch 9/185 loss = 0.0136\n",
      "epoch 10 batch 10/185 loss = 0.0128\n",
      "epoch 10 batch 11/185 loss = 0.0142\n",
      "epoch 10 batch 12/185 loss = 0.0124\n",
      "epoch 10 batch 13/185 loss = 0.0139\n",
      "epoch 10 batch 14/185 loss = 0.0111\n",
      "epoch 10 batch 15/185 loss = 0.0112\n",
      "epoch 10 batch 16/185 loss = 0.0123\n",
      "epoch 10 batch 17/185 loss = 0.0116\n",
      "epoch 10 batch 18/185 loss = 0.0121\n",
      "epoch 10 batch 19/185 loss = 0.0123\n",
      "epoch 10 batch 20/185 loss = 0.0108\n",
      "epoch 10 batch 21/185 loss = 0.0126\n",
      "epoch 10 batch 22/185 loss = 0.0114\n",
      "epoch 10 batch 23/185 loss = 0.0139\n",
      "epoch 10 batch 24/185 loss = 0.0131\n",
      "epoch 10 batch 25/185 loss = 0.0122\n",
      "epoch 10 batch 26/185 loss = 0.0118\n",
      "epoch 10 batch 27/185 loss = 0.0102\n",
      "epoch 10 batch 28/185 loss = 0.0143\n",
      "epoch 10 batch 29/185 loss = 0.0117\n",
      "epoch 10 batch 30/185 loss = 0.0132\n",
      "epoch 10 batch 31/185 loss = 0.0121\n",
      "epoch 10 batch 32/185 loss = 0.0114\n",
      "epoch 10 batch 33/185 loss = 0.0094\n",
      "epoch 10 batch 34/185 loss = 0.0111\n",
      "epoch 10 batch 35/185 loss = 0.0133\n",
      "epoch 10 batch 36/185 loss = 0.0105\n",
      "epoch 10 batch 37/185 loss = 0.0128\n",
      "epoch 10 batch 38/185 loss = 0.0102\n",
      "epoch 10 batch 39/185 loss = 0.0100\n",
      "epoch 10 batch 40/185 loss = 0.0104\n",
      "epoch 10 batch 41/185 loss = 0.0117\n",
      "epoch 10 batch 42/185 loss = 0.0114\n",
      "epoch 10 batch 43/185 loss = 0.0115\n",
      "epoch 10 batch 44/185 loss = 0.0106\n",
      "epoch 10 batch 45/185 loss = 0.0118\n",
      "epoch 10 batch 46/185 loss = 0.0120\n",
      "epoch 10 batch 47/185 loss = 0.0098\n",
      "epoch 10 batch 48/185 loss = 0.0100\n",
      "epoch 10 batch 49/185 loss = 0.0128\n",
      "epoch 10 batch 50/185 loss = 0.0136\n",
      "epoch 10 batch 51/185 loss = 0.0131\n",
      "epoch 10 batch 52/185 loss = 0.0105\n",
      "epoch 10 batch 53/185 loss = 0.0111\n",
      "epoch 10 batch 54/185 loss = 0.0158\n",
      "epoch 10 batch 55/185 loss = 0.0097\n",
      "epoch 10 batch 56/185 loss = 0.0123\n",
      "epoch 10 batch 57/185 loss = 0.0129\n",
      "epoch 10 batch 58/185 loss = 0.0110\n",
      "epoch 10 batch 59/185 loss = 0.0111\n",
      "epoch 10 batch 60/185 loss = 0.0099\n",
      "epoch 10 batch 61/185 loss = 0.0146\n",
      "epoch 10 batch 62/185 loss = 0.0120\n",
      "epoch 10 batch 63/185 loss = 0.0110\n",
      "epoch 10 batch 64/185 loss = 0.0124\n",
      "epoch 10 batch 65/185 loss = 0.0121\n",
      "epoch 10 batch 66/185 loss = 0.0113\n",
      "epoch 10 batch 67/185 loss = 0.0111\n",
      "epoch 10 batch 68/185 loss = 0.0120\n",
      "epoch 10 batch 69/185 loss = 0.0103\n",
      "epoch 10 batch 70/185 loss = 0.0118\n",
      "epoch 10 batch 71/185 loss = 0.0096\n",
      "epoch 10 batch 72/185 loss = 0.0112\n",
      "epoch 10 batch 73/185 loss = 0.0115\n",
      "epoch 10 batch 74/185 loss = 0.0108\n",
      "epoch 10 batch 75/185 loss = 0.0114\n",
      "epoch 10 batch 76/185 loss = 0.0119\n",
      "epoch 10 batch 77/185 loss = 0.0107\n",
      "epoch 10 batch 78/185 loss = 0.0116\n",
      "epoch 10 batch 79/185 loss = 0.0137\n",
      "epoch 10 batch 80/185 loss = 0.0113\n",
      "epoch 10 batch 81/185 loss = 0.0111\n",
      "epoch 10 batch 82/185 loss = 0.0098\n",
      "epoch 10 batch 83/185 loss = 0.0123\n",
      "epoch 10 batch 84/185 loss = 0.0144\n",
      "epoch 10 batch 85/185 loss = 0.0100\n",
      "epoch 10 batch 86/185 loss = 0.0113\n",
      "epoch 10 batch 87/185 loss = 0.0165\n",
      "epoch 10 batch 88/185 loss = 0.0122\n",
      "epoch 10 batch 89/185 loss = 0.0114\n",
      "epoch 10 batch 90/185 loss = 0.0111\n",
      "epoch 10 batch 91/185 loss = 0.0099\n",
      "epoch 10 batch 92/185 loss = 0.0137\n",
      "epoch 10 batch 93/185 loss = 0.0131\n",
      "epoch 10 batch 94/185 loss = 0.0095\n",
      "epoch 10 batch 95/185 loss = 0.0136\n",
      "epoch 10 batch 96/185 loss = 0.0117\n",
      "epoch 10 batch 97/185 loss = 0.0127\n",
      "epoch 10 batch 98/185 loss = 0.0083\n",
      "epoch 10 batch 99/185 loss = 0.0112\n",
      "epoch 10 batch 100/185 loss = 0.0097\n",
      "epoch 10 batch 101/185 loss = 0.0111\n",
      "epoch 10 batch 102/185 loss = 0.0137\n",
      "epoch 10 batch 103/185 loss = 0.0119\n",
      "epoch 10 batch 104/185 loss = 0.0119\n",
      "epoch 10 batch 105/185 loss = 0.0112\n",
      "epoch 10 batch 106/185 loss = 0.0109\n",
      "epoch 10 batch 107/185 loss = 0.0129\n",
      "epoch 10 batch 108/185 loss = 0.0111\n",
      "epoch 10 batch 109/185 loss = 0.0107\n",
      "epoch 10 batch 110/185 loss = 0.0104\n",
      "epoch 10 batch 111/185 loss = 0.0139\n",
      "epoch 10 batch 112/185 loss = 0.0116\n",
      "epoch 10 batch 113/185 loss = 0.0104\n",
      "epoch 10 batch 114/185 loss = 0.0137\n",
      "epoch 10 batch 115/185 loss = 0.0112\n",
      "epoch 10 batch 116/185 loss = 0.0103\n",
      "epoch 10 batch 117/185 loss = 0.0139\n",
      "epoch 10 batch 118/185 loss = 0.0115\n",
      "epoch 10 batch 119/185 loss = 0.0132\n",
      "epoch 10 batch 120/185 loss = 0.0143\n",
      "epoch 10 batch 121/185 loss = 0.0140\n",
      "epoch 10 batch 122/185 loss = 0.0120\n",
      "epoch 10 batch 123/185 loss = 0.0117\n",
      "epoch 10 batch 124/185 loss = 0.0138\n",
      "epoch 10 batch 125/185 loss = 0.0120\n",
      "epoch 10 batch 126/185 loss = 0.0100\n",
      "epoch 10 batch 127/185 loss = 0.0131\n",
      "epoch 10 batch 128/185 loss = 0.0106\n",
      "epoch 10 batch 129/185 loss = 0.0128\n",
      "epoch 10 batch 130/185 loss = 0.0119\n",
      "epoch 10 batch 131/185 loss = 0.0118\n",
      "epoch 10 batch 132/185 loss = 0.0141\n",
      "epoch 10 batch 133/185 loss = 0.0115\n",
      "epoch 10 batch 134/185 loss = 0.0118\n",
      "epoch 10 batch 135/185 loss = 0.0090\n",
      "epoch 10 batch 136/185 loss = 0.0135\n",
      "epoch 10 batch 137/185 loss = 0.0108\n",
      "epoch 10 batch 138/185 loss = 0.0139\n",
      "epoch 10 batch 139/185 loss = 0.0096\n",
      "epoch 10 batch 140/185 loss = 0.0123\n",
      "epoch 10 batch 141/185 loss = 0.0113\n",
      "epoch 10 batch 142/185 loss = 0.0101\n",
      "epoch 10 batch 143/185 loss = 0.0107\n",
      "epoch 10 batch 144/185 loss = 0.0093\n",
      "epoch 10 batch 145/185 loss = 0.0084\n",
      "epoch 10 batch 146/185 loss = 0.0082\n",
      "epoch 10 batch 147/185 loss = 0.0114\n",
      "epoch 10 batch 148/185 loss = 0.0091\n",
      "epoch 10 batch 149/185 loss = 0.0114\n",
      "epoch 10 batch 150/185 loss = 0.0115\n",
      "epoch 10 batch 151/185 loss = 0.0132\n",
      "epoch 10 batch 152/185 loss = 0.0153\n",
      "epoch 10 batch 153/185 loss = 0.0119\n",
      "epoch 10 batch 154/185 loss = 0.0146\n",
      "epoch 10 batch 155/185 loss = 0.0119\n",
      "epoch 10 batch 156/185 loss = 0.0076\n",
      "epoch 10 batch 157/185 loss = 0.0104\n",
      "epoch 10 batch 158/185 loss = 0.0083\n",
      "epoch 10 batch 159/185 loss = 0.0109\n",
      "epoch 10 batch 160/185 loss = 0.0128\n",
      "epoch 10 batch 161/185 loss = 0.0132\n",
      "epoch 10 batch 162/185 loss = 0.0097\n",
      "epoch 10 batch 163/185 loss = 0.0106\n",
      "epoch 10 batch 164/185 loss = 0.0106\n",
      "epoch 10 batch 165/185 loss = 0.0108\n",
      "epoch 10 batch 166/185 loss = 0.0110\n",
      "epoch 10 batch 167/185 loss = 0.0118\n",
      "epoch 10 batch 168/185 loss = 0.0119\n",
      "epoch 10 batch 169/185 loss = 0.0095\n",
      "epoch 10 batch 170/185 loss = 0.0120\n",
      "epoch 10 batch 171/185 loss = 0.0121\n",
      "epoch 10 batch 172/185 loss = 0.0106\n",
      "epoch 10 batch 173/185 loss = 0.0096\n",
      "epoch 10 batch 174/185 loss = 0.0088\n",
      "epoch 10 batch 175/185 loss = 0.0120\n",
      "epoch 10 batch 176/185 loss = 0.0117\n",
      "epoch 10 batch 177/185 loss = 0.0096\n",
      "epoch 10 batch 178/185 loss = 0.0131\n",
      "epoch 10 batch 179/185 loss = 0.0103\n",
      "epoch 10 batch 180/185 loss = 0.0098\n",
      "epoch 10 batch 181/185 loss = 0.0111\n",
      "epoch 10 batch 182/185 loss = 0.0104\n",
      "epoch 10 batch 183/185 loss = 0.0099\n",
      "epoch 10 batch 184/185 loss = 0.0120\n",
      "epoch 10 batch 185/185 loss = 0.0088\n",
      "epoch 10 train loss = 0.0116 valid loss = 0.0122\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    current_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    train_image_path = glob('../input/cityscapes-image-pairs/cityscapes_data/train/*')\n",
    "    valid_image_path = glob('../input/cityscapes-image-pairs/cityscapes_data/val/*')\n",
    "\n",
    "    image_transforms = transforms.Compose([\n",
    "#         transforms.ToPILImage(mode='RGB'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(256),\n",
    "#         transforms.PILToTensor(),\n",
    "#         transforms.ColorJitter(),\n",
    "#         transforms.GaussianBlur((5,), (5, 15)),\n",
    "#         transforms.Normalize((0.5,), (0.5,)),\n",
    "    ])\n",
    "    target_transforms = transforms.Compose([\n",
    "#         transforms.ToPILImage(mode='RGB'),\n",
    "#         transforms.Grayscale(),\n",
    "#         transforms.PILToTensor(),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.Normalize((0.5,), (0.5,)),\n",
    "    ])\n",
    "    valid_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.Normalize((0.5,), (0.5,)),\n",
    "#         transforms.ToPILImage(mode='RGB'),\n",
    "        transforms.Resize(256),\n",
    "#         transforms.PILToTensor(),\n",
    "#         transforms.Normalize((0.5,), (0.5,)),\n",
    "    ])\n",
    "\n",
    "    train_cityscapes = Cityscapes(train_image_path, image_transforms, target_transforms)\n",
    "    valid_cityscapes = Cityscapes(valid_image_path, valid_transforms, target_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_cityscapes, batch_size=16, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_cityscapes, batch_size=16, shuffle=True, drop_last=True)\n",
    "\n",
    "    unet = UNet().to(current_device)\n",
    "    ce_loss = nn.MSELoss().to(current_device)\n",
    "    optimizer_adam = optim.Adam(unet.parameters(), lr=0.001)\n",
    "    continue_train = False\n",
    "    if continue_train:\n",
    "        trained_model_params = torch.load('../input/cityscapes-unet/unet-last.pth')\n",
    "        unet.load_state_dict(trained_model_params['model_state_dict'])\n",
    "        optimizer_adam.load_state_dict(trained_model_params['optimizer_state_dict'])\n",
    "    train(train_loader, valid_loader, unet, optimizer_adam, ce_loss, epoch=10, device=current_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1318.142364,
   "end_time": "2022-06-16T07:10:10.051296",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-16T06:48:11.908932",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
