{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca9757a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-28T14:37:58.486268Z",
     "iopub.status.busy": "2022-05-28T14:37:58.485614Z",
     "iopub.status.idle": "2022-05-28T14:37:58.496420Z",
     "shell.execute_reply": "2022-05-28T14:37:58.495812Z"
    },
    "papermill": {
     "duration": 0.027394,
     "end_time": "2022-05-28T14:37:58.498188",
     "exception": false,
     "start_time": "2022-05-28T14:37:58.470794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#os.remove('/kaggle/working/datasets/cityscapes.pth')\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#       print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1744c9d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T14:37:58.519466Z",
     "iopub.status.busy": "2022-05-28T14:37:58.519280Z",
     "iopub.status.idle": "2022-05-28T14:38:00.200000Z",
     "shell.execute_reply": "2022-05-28T14:38:00.199260Z"
    },
    "papermill": {
     "duration": 1.693525,
     "end_time": "2022-05-28T14:38:00.202043",
     "exception": false,
     "start_time": "2022-05-28T14:37:58.508518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as functional\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2ffb95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T14:38:00.223289Z",
     "iopub.status.busy": "2022-05-28T14:38:00.223089Z",
     "iopub.status.idle": "2022-05-28T14:38:00.589391Z",
     "shell.execute_reply": "2022-05-28T14:38:00.588633Z"
    },
    "papermill": {
     "duration": 0.37969,
     "end_time": "2022-05-28T14:38:00.591717",
     "exception": false,
     "start_time": "2022-05-28T14:38:00.212027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = glob('/kaggle/input/cityscapes-image-pairs/cityscapes_data/train/*')\n",
    "valid_path = glob('/kaggle/input/cityscapes-image-pairs/cityscapes_data/val/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22177c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T14:38:00.613037Z",
     "iopub.status.busy": "2022-05-28T14:38:00.612809Z",
     "iopub.status.idle": "2022-05-28T14:38:00.620054Z",
     "shell.execute_reply": "2022-05-28T14:38:00.619366Z"
    },
    "papermill": {
     "duration": 0.020014,
     "end_time": "2022-05-28T14:38:00.621867",
     "exception": false,
     "start_time": "2022-05-28T14:38:00.601853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cityscapes(Dataset):\n",
    "    def __init__(self, data_path, transform=None, target_transform=None):\n",
    "        super(Cityscapes, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        #self.datasets = np.array(data)\n",
    "        #self.images, self.targets = np.array_split(self.datasets, 2, axis=2)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image_pair = plt.imread(self.data_path[item])\n",
    "        image, target = image_pair[:, :int(image_pair.shape[1] / 2)], image_pair[:, int(image_pair.shape[1] / 2):]\n",
    "        #image = self.images[item]\n",
    "        #target = self.targets[item]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016430d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T14:38:00.643298Z",
     "iopub.status.busy": "2022-05-28T14:38:00.643053Z",
     "iopub.status.idle": "2022-05-28T14:38:00.647333Z",
     "shell.execute_reply": "2022-05-28T14:38:00.646639Z"
    },
    "papermill": {
     "duration": 0.017059,
     "end_time": "2022-05-28T14:38:00.649016",
     "exception": false,
     "start_time": "2022-05-28T14:38:00.631957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_directory(path):\n",
    "    image_list = []\n",
    "    for filename in os.listdir(path):\n",
    "        image = plt.imread(path + '/' + filename)\n",
    "        image_list.append(image)\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72957a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T14:38:00.670448Z",
     "iopub.status.busy": "2022-05-28T14:38:00.670218Z",
     "iopub.status.idle": "2022-05-28T14:38:00.676930Z",
     "shell.execute_reply": "2022-05-28T14:38:00.676246Z"
    },
    "papermill": {
     "duration": 0.019409,
     "end_time": "2022-05-28T14:38:00.678713",
     "exception": false,
     "start_time": "2022-05-28T14:38:00.659304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "#     if path is not None and os.path.exists(path):\n",
    "#         return torch.load(path)['train_loader']\n",
    "#     else:\n",
    "        transform_list = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        target_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        #cityscapes = read_directory('/kaggle/input/cityscapes-image-pairs/cityscapes_data/train')\n",
    "        train_dataset = Cityscapes(train_path, transform=transform_list, target_transform=target_transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, drop_last=True)\n",
    "#         path = path if path is not None else '/kaggle/working/datasets/' + str(uuid.UUID()) + '.pth'\n",
    "#         if not os.path.exists(path):\n",
    "#             index = path.rindex('/')\n",
    "#             dirs = path[:index]\n",
    "#             if not os.path.exists(dirs):\n",
    "#                 os.mkdir(dirs)\n",
    "#             with open(path, 'w'):\n",
    "#                 pass\n",
    "#         datas = {\n",
    "#             'initial_dataset': cityscapes,\n",
    "#             'train_loader': train_loader\n",
    "#         }\n",
    "#         torch.save(datas, path)\n",
    "        return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4dd3fd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T14:38:00.700486Z",
     "iopub.status.busy": "2022-05-28T14:38:00.700050Z",
     "iopub.status.idle": "2022-05-28T14:38:00.740263Z",
     "shell.execute_reply": "2022-05-28T14:38:00.739333Z"
    },
    "papermill": {
     "duration": 0.055759,
     "end_time": "2022-05-28T14:38:00.744306",
     "exception": false,
     "start_time": "2022-05-28T14:38:00.688547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net模型的pytorch实现。\n",
    "    论文地址：https://arxiv.org/abs/1505.04597\n",
    "    模型的总体结构: 编码器 -> 一个ConvBlock -> 解码器 -> 一个Conv 1 * 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # 编码器部分\n",
    "        self.eb1 = EncoderBlock(3, 64, 64, kernel_size=2)\n",
    "        self.eb2 = EncoderBlock(64, 128, 128, kernel_size=2)\n",
    "        self.eb3 = EncoderBlock(128, 256, 256, kernel_size=2)\n",
    "        self.eb4 = EncoderBlock(256, 512, 512, kernel_size=2)\n",
    "        # 编码器与解码器之间有一个ConvBlock\n",
    "        self.cb = ConvBlock(512, 1024, 1024)\n",
    "        # 解码器部分\n",
    "        self.db1 = DecoderBlock(1024, 512, 512)\n",
    "        self.db2 = DecoderBlock(512, 512, 256)\n",
    "        self.db3 = DecoderBlock(256, 128, 128)\n",
    "        self.db4 = DecoderBlock(128, 64, 64)\n",
    "        # 一个Conv 1 * 1\n",
    "        self.conv1x1 = nn.Conv2d(64, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ex1, skip_x1 = self.eb1(x)\n",
    "        ex2, skip_x2 = self.eb2(ex1)\n",
    "        ex3, skip_x3 = self.eb3(ex2)\n",
    "        ex4, skip_x4 = self.eb4(ex3)\n",
    "        cbx = self.cb(ex4)\n",
    "        dx1 = self.db1(cbx, skip_x4)\n",
    "        dx2 = self.db2(dx1, skip_x3)\n",
    "        dx3 = self.db3(dx2, skip_x2)\n",
    "        dx4 = self.db4(dx3, skip_x1)\n",
    "        crop = transforms.CenterCrop(size=(x.shape[-1], x.shape[-2]))\n",
    "        normalize = transforms.Normalize((0.5,), (0.5,))\n",
    "        return self.conv1x1(normalize(crop(dx4)))\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"一个Conv2d卷积后跟一个Relu激活函数，卷积核大小为3 * 3\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        conv_relu_list = [nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=2),\n",
    "                          nn.BatchNorm2d(mid_channels),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=2),\n",
    "                          nn.BatchNorm2d(out_channels),\n",
    "                          nn.ReLU(inplace=True)]\n",
    "        self.conv_relu = nn.Sequential(*conv_relu_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_relu(x)\n",
    "\n",
    "\n",
    "class DownSampling(nn.Module):\n",
    "    \"\"\"下采样，使用max pool方法执行，核大小为 2 * 2，用在编码器的ConvBlock后面\n",
    "\n",
    "    :param kernel_size: 下采样层（即最大池化层）的核大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(DownSampling, self).__init__()\n",
    "        self.down_sample = nn.MaxPool2d(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_sample(x)\n",
    "\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "    \"\"\"上采样，用在解码器的ConvBlock前面，使用转置卷积，同时通道数减半，\n",
    "\n",
    "    C_out = out_channels\n",
    "    H_out = (H_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "    W_out = (W_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "\n",
    "    :param in_channels: 转置卷积的输入通道数\n",
    "    :param out_channels: 转置卷积的输出通道数\n",
    "    :param kernel_size: 转置卷积的卷积核大小，默认为2\n",
    "    :param stride: 转置卷积的步幅，默认为2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, stride=2, dilation=1, padding=0, output_padding=1):\n",
    "        super(UpSampling, self).__init__()\n",
    "        # self.up_sample = nn.Upsample(scale_factor=scale_factor, mode='bilinear')\n",
    "        # stride=2, kernel_size=2相当于宽高翻倍\n",
    "        self.up_sample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                            dilation=dilation, padding=padding, output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up_sample(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"编码器中的一个层次块\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    :param kernel_size: 下采样层（即最大池化层）的核大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, kernel_size):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv_block = ConvBlock(in_channels, mid_channels, out_channels)\n",
    "        self.down_sample = DownSampling(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_block(x)\n",
    "        return self.down_sample(x1), x1\n",
    "\n",
    "\n",
    "class ConcatLayer(nn.Module):\n",
    "    \"\"\"跳跃连接，在通道维上连接\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConcatLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        # 将从编码器传过来的特征图裁剪到与输入相同尺寸\n",
    "        x1 = functional.center_crop(skip_x, [x.shape[-2], x.shape[-1]])\n",
    "        if x1.shape != x.shape:\n",
    "            raise Exception('要连接的两个特征图尺寸不一致，skip_x.shape={}，x.shape={}'.format(skip_x.shape, x.shape))\n",
    "        # 通道维连接\n",
    "        return torch.cat([x, x1], dim=1)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中的层次块，每个层次块都是UpSampling -> Concat -> ConvBlock\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.up_sample = UpSampling(in_channels, out_channels)\n",
    "        self.conv_block = ConvBlock(in_channels, mid_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        x1 = self.up_sample(x)\n",
    "        concat = ConcatLayer()\n",
    "        x2 = concat(x1, skip_x)\n",
    "        return self.conv_block(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcb5ea15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T14:38:00.781131Z",
     "iopub.status.busy": "2022-05-28T14:38:00.780618Z",
     "iopub.status.idle": "2022-05-28T14:38:00.800442Z",
     "shell.execute_reply": "2022-05-28T14:38:00.799736Z"
    },
    "papermill": {
     "duration": 0.038445,
     "end_time": "2022-05-28T14:38:00.802214",
     "exception": false,
     "start_time": "2022-05-28T14:38:00.763769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(model, dataloader, optimizer, loss, epoch, current_epoch=0, device=None):\n",
    "    epoch_index_list = []\n",
    "    loss_change_list = []\n",
    "    for i in range(current_epoch, epoch):\n",
    "        total_loss = 0.0\n",
    "        for index, (image, label) in enumerate(dataloader):\n",
    "            segment_mask = model(image.to(device))\n",
    "            loss_value = loss(segment_mask, label.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            total_loss = total_loss + loss_value.item()\n",
    "            if index == len(dataloader) - 1:\n",
    "                train_params = {\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': i\n",
    "                }\n",
    "                path = '/kaggle/working/model_params/u_net_seg_50.pth'\n",
    "                if not os.path.exists(path):\n",
    "                    index = path.rindex('/')\n",
    "                    dirs = path[:index]\n",
    "                    if not os.path.exists(dirs):\n",
    "                        os.mkdir(dirs)\n",
    "                    with open(path, 'w'):\n",
    "                        pass\n",
    "                torch.save(train_params, path)\n",
    "                print('epoch {} batch {}/{} average loss = {:.4f} last loss = {:.4f}'.format(i + 1, index + 1,\n",
    "                                                                                             len(dataloader),\n",
    "                                                                                             total_loss / len(dataloader),\n",
    "                                                                                             loss_value))\n",
    "            else:\n",
    "                print('epoch {} batch {}/{} loss = {:.4f}'.format(i + 1, index + 1, len(dataloader), loss_value))\n",
    "        epoch_index_list.append(i)\n",
    "        loss_change_list.append(total_loss / len(dataloader))\n",
    "        loss_change = {\n",
    "            'epoch_index_list': epoch_index_list,\n",
    "            'loss_change_list': loss_change_list\n",
    "        }\n",
    "        torch.save(loss_change, '/kaggle/working/model_params/loss_change.pth')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e746da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T14:38:00.824644Z",
     "iopub.status.busy": "2022-05-28T14:38:00.823986Z",
     "iopub.status.idle": "2022-05-28T14:38:00.827892Z",
     "shell.execute_reply": "2022-05-28T14:38:00.827196Z"
    },
    "papermill": {
     "duration": 0.016623,
     "end_time": "2022-05-28T14:38:00.829534",
     "exception": false,
     "start_time": "2022-05-28T14:38:00.812911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'epoch': 20,\n",
    "    'lr': 3e-4,\n",
    "    'betas': (0.5, 0.999)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0b97813",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-28T14:38:00.850241Z",
     "iopub.status.busy": "2022-05-28T14:38:00.849942Z",
     "iopub.status.idle": "2022-05-28T15:06:32.853498Z",
     "shell.execute_reply": "2022-05-28T15:06:32.852406Z"
    },
    "papermill": {
     "duration": 1712.01676,
     "end_time": "2022-05-28T15:06:32.856045",
     "exception": false,
     "start_time": "2022-05-28T14:38:00.839285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch 19 batch 1/123 loss = 0.0052\n",
      "epoch 19 batch 2/123 loss = 0.0046\n",
      "epoch 19 batch 3/123 loss = 0.0047\n",
      "epoch 19 batch 4/123 loss = 0.0048\n",
      "epoch 19 batch 5/123 loss = 0.0047\n",
      "epoch 19 batch 6/123 loss = 0.0050\n",
      "epoch 19 batch 7/123 loss = 0.0049\n",
      "epoch 19 batch 8/123 loss = 0.0052\n",
      "epoch 19 batch 9/123 loss = 0.0049\n",
      "epoch 19 batch 10/123 loss = 0.0044\n",
      "epoch 19 batch 11/123 loss = 0.0054\n",
      "epoch 19 batch 12/123 loss = 0.0043\n",
      "epoch 19 batch 13/123 loss = 0.0043\n",
      "epoch 19 batch 14/123 loss = 0.0046\n",
      "epoch 19 batch 15/123 loss = 0.0052\n",
      "epoch 19 batch 16/123 loss = 0.0049\n",
      "epoch 19 batch 17/123 loss = 0.0057\n",
      "epoch 19 batch 18/123 loss = 0.0060\n",
      "epoch 19 batch 19/123 loss = 0.0061\n",
      "epoch 19 batch 20/123 loss = 0.0066\n",
      "epoch 19 batch 21/123 loss = 0.0066\n",
      "epoch 19 batch 22/123 loss = 0.0069\n",
      "epoch 19 batch 23/123 loss = 0.0063\n",
      "epoch 19 batch 24/123 loss = 0.0058\n",
      "epoch 19 batch 25/123 loss = 0.0058\n",
      "epoch 19 batch 26/123 loss = 0.0053\n",
      "epoch 19 batch 27/123 loss = 0.0057\n",
      "epoch 19 batch 28/123 loss = 0.0054\n",
      "epoch 19 batch 29/123 loss = 0.0063\n",
      "epoch 19 batch 30/123 loss = 0.0053\n",
      "epoch 19 batch 31/123 loss = 0.0056\n",
      "epoch 19 batch 32/123 loss = 0.0057\n",
      "epoch 19 batch 33/123 loss = 0.0057\n",
      "epoch 19 batch 34/123 loss = 0.0058\n",
      "epoch 19 batch 35/123 loss = 0.0049\n",
      "epoch 19 batch 36/123 loss = 0.0055\n",
      "epoch 19 batch 37/123 loss = 0.0049\n",
      "epoch 19 batch 38/123 loss = 0.0057\n",
      "epoch 19 batch 39/123 loss = 0.0056\n",
      "epoch 19 batch 40/123 loss = 0.0055\n",
      "epoch 19 batch 41/123 loss = 0.0048\n",
      "epoch 19 batch 42/123 loss = 0.0054\n",
      "epoch 19 batch 43/123 loss = 0.0047\n",
      "epoch 19 batch 44/123 loss = 0.0048\n",
      "epoch 19 batch 45/123 loss = 0.0051\n",
      "epoch 19 batch 46/123 loss = 0.0048\n",
      "epoch 19 batch 47/123 loss = 0.0049\n",
      "epoch 19 batch 48/123 loss = 0.0051\n",
      "epoch 19 batch 49/123 loss = 0.0047\n",
      "epoch 19 batch 50/123 loss = 0.0052\n",
      "epoch 19 batch 51/123 loss = 0.0049\n",
      "epoch 19 batch 52/123 loss = 0.0048\n",
      "epoch 19 batch 53/123 loss = 0.0051\n",
      "epoch 19 batch 54/123 loss = 0.0054\n",
      "epoch 19 batch 55/123 loss = 0.0050\n",
      "epoch 19 batch 56/123 loss = 0.0052\n",
      "epoch 19 batch 57/123 loss = 0.0054\n",
      "epoch 19 batch 58/123 loss = 0.0050\n",
      "epoch 19 batch 59/123 loss = 0.0054\n",
      "epoch 19 batch 60/123 loss = 0.0046\n",
      "epoch 19 batch 61/123 loss = 0.0049\n",
      "epoch 19 batch 62/123 loss = 0.0048\n",
      "epoch 19 batch 63/123 loss = 0.0050\n",
      "epoch 19 batch 64/123 loss = 0.0051\n",
      "epoch 19 batch 65/123 loss = 0.0046\n",
      "epoch 19 batch 66/123 loss = 0.0050\n",
      "epoch 19 batch 67/123 loss = 0.0053\n",
      "epoch 19 batch 68/123 loss = 0.0052\n",
      "epoch 19 batch 69/123 loss = 0.0050\n",
      "epoch 19 batch 70/123 loss = 0.0047\n",
      "epoch 19 batch 71/123 loss = 0.0046\n",
      "epoch 19 batch 72/123 loss = 0.0050\n",
      "epoch 19 batch 73/123 loss = 0.0048\n",
      "epoch 19 batch 74/123 loss = 0.0056\n",
      "epoch 19 batch 75/123 loss = 0.0053\n",
      "epoch 19 batch 76/123 loss = 0.0055\n",
      "epoch 19 batch 77/123 loss = 0.0052\n",
      "epoch 19 batch 78/123 loss = 0.0049\n",
      "epoch 19 batch 79/123 loss = 0.0053\n",
      "epoch 19 batch 80/123 loss = 0.0048\n",
      "epoch 19 batch 81/123 loss = 0.0052\n",
      "epoch 19 batch 82/123 loss = 0.0050\n",
      "epoch 19 batch 83/123 loss = 0.0049\n",
      "epoch 19 batch 84/123 loss = 0.0048\n",
      "epoch 19 batch 85/123 loss = 0.0045\n",
      "epoch 19 batch 86/123 loss = 0.0048\n",
      "epoch 19 batch 87/123 loss = 0.0050\n",
      "epoch 19 batch 88/123 loss = 0.0048\n",
      "epoch 19 batch 89/123 loss = 0.0047\n",
      "epoch 19 batch 90/123 loss = 0.0054\n",
      "epoch 19 batch 91/123 loss = 0.0052\n",
      "epoch 19 batch 92/123 loss = 0.0051\n",
      "epoch 19 batch 93/123 loss = 0.0046\n",
      "epoch 19 batch 94/123 loss = 0.0052\n",
      "epoch 19 batch 95/123 loss = 0.0051\n",
      "epoch 19 batch 96/123 loss = 0.0050\n",
      "epoch 19 batch 97/123 loss = 0.0050\n",
      "epoch 19 batch 98/123 loss = 0.0050\n",
      "epoch 19 batch 99/123 loss = 0.0051\n",
      "epoch 19 batch 100/123 loss = 0.0055\n",
      "epoch 19 batch 101/123 loss = 0.0049\n",
      "epoch 19 batch 102/123 loss = 0.0063\n",
      "epoch 19 batch 103/123 loss = 0.0065\n",
      "epoch 19 batch 104/123 loss = 0.0053\n",
      "epoch 19 batch 105/123 loss = 0.0072\n",
      "epoch 19 batch 106/123 loss = 0.0066\n",
      "epoch 19 batch 107/123 loss = 0.0058\n",
      "epoch 19 batch 108/123 loss = 0.0066\n",
      "epoch 19 batch 109/123 loss = 0.0070\n",
      "epoch 19 batch 110/123 loss = 0.0067\n",
      "epoch 19 batch 111/123 loss = 0.0080\n",
      "epoch 19 batch 112/123 loss = 0.0075\n",
      "epoch 19 batch 113/123 loss = 0.0069\n",
      "epoch 19 batch 114/123 loss = 0.0071\n",
      "epoch 19 batch 115/123 loss = 0.0081\n",
      "epoch 19 batch 116/123 loss = 0.0077\n",
      "epoch 19 batch 117/123 loss = 0.0064\n",
      "epoch 19 batch 118/123 loss = 0.0074\n",
      "epoch 19 batch 119/123 loss = 0.0064\n",
      "epoch 19 batch 120/123 loss = 0.0074\n",
      "epoch 19 batch 121/123 loss = 0.0059\n",
      "epoch 19 batch 122/123 loss = 0.0073\n",
      "epoch 19 batch 29/123 average loss = 0.0055 last loss = 0.0064\n",
      "epoch 20 batch 1/123 loss = 0.0066\n",
      "epoch 20 batch 2/123 loss = 0.0064\n",
      "epoch 20 batch 3/123 loss = 0.0066\n",
      "epoch 20 batch 4/123 loss = 0.0077\n",
      "epoch 20 batch 5/123 loss = 0.0077\n",
      "epoch 20 batch 6/123 loss = 0.0080\n",
      "epoch 20 batch 7/123 loss = 0.0067\n",
      "epoch 20 batch 8/123 loss = 0.0081\n",
      "epoch 20 batch 9/123 loss = 0.0081\n",
      "epoch 20 batch 10/123 loss = 0.0068\n",
      "epoch 20 batch 11/123 loss = 0.0077\n",
      "epoch 20 batch 12/123 loss = 0.0089\n",
      "epoch 20 batch 13/123 loss = 0.0131\n",
      "epoch 20 batch 14/123 loss = 0.0103\n",
      "epoch 20 batch 15/123 loss = 0.0104\n",
      "epoch 20 batch 16/123 loss = 0.0108\n",
      "epoch 20 batch 17/123 loss = 0.0115\n",
      "epoch 20 batch 18/123 loss = 0.0125\n",
      "epoch 20 batch 19/123 loss = 0.0099\n",
      "epoch 20 batch 20/123 loss = 0.0103\n",
      "epoch 20 batch 21/123 loss = 0.0102\n",
      "epoch 20 batch 22/123 loss = 0.0090\n",
      "epoch 20 batch 23/123 loss = 0.0122\n",
      "epoch 20 batch 24/123 loss = 0.0156\n",
      "epoch 20 batch 25/123 loss = 0.0133\n",
      "epoch 20 batch 26/123 loss = 0.0126\n",
      "epoch 20 batch 27/123 loss = 0.0120\n",
      "epoch 20 batch 28/123 loss = 0.0102\n",
      "epoch 20 batch 29/123 loss = 0.0113\n",
      "epoch 20 batch 30/123 loss = 0.0116\n",
      "epoch 20 batch 31/123 loss = 0.0090\n",
      "epoch 20 batch 32/123 loss = 0.0096\n",
      "epoch 20 batch 33/123 loss = 0.0102\n",
      "epoch 20 batch 34/123 loss = 0.0109\n",
      "epoch 20 batch 35/123 loss = 0.0120\n",
      "epoch 20 batch 36/123 loss = 0.0112\n",
      "epoch 20 batch 37/123 loss = 0.0109\n",
      "epoch 20 batch 38/123 loss = 0.0153\n",
      "epoch 20 batch 39/123 loss = 0.0126\n",
      "epoch 20 batch 40/123 loss = 0.0142\n",
      "epoch 20 batch 41/123 loss = 0.0161\n",
      "epoch 20 batch 42/123 loss = 0.0150\n",
      "epoch 20 batch 43/123 loss = 0.0138\n",
      "epoch 20 batch 44/123 loss = 0.0123\n",
      "epoch 20 batch 45/123 loss = 0.0122\n",
      "epoch 20 batch 46/123 loss = 0.0105\n",
      "epoch 20 batch 47/123 loss = 0.0109\n",
      "epoch 20 batch 48/123 loss = 0.0132\n",
      "epoch 20 batch 49/123 loss = 0.0092\n",
      "epoch 20 batch 50/123 loss = 0.0103\n",
      "epoch 20 batch 51/123 loss = 0.0092\n",
      "epoch 20 batch 52/123 loss = 0.0097\n",
      "epoch 20 batch 53/123 loss = 0.0099\n",
      "epoch 20 batch 54/123 loss = 0.0103\n",
      "epoch 20 batch 55/123 loss = 0.0092\n",
      "epoch 20 batch 56/123 loss = 0.0110\n",
      "epoch 20 batch 57/123 loss = 0.0123\n",
      "epoch 20 batch 58/123 loss = 0.0143\n",
      "epoch 20 batch 59/123 loss = 0.0110\n",
      "epoch 20 batch 60/123 loss = 0.0114\n",
      "epoch 20 batch 61/123 loss = 0.0095\n",
      "epoch 20 batch 62/123 loss = 0.0103\n",
      "epoch 20 batch 63/123 loss = 0.0111\n",
      "epoch 20 batch 64/123 loss = 0.0132\n",
      "epoch 20 batch 65/123 loss = 0.0096\n",
      "epoch 20 batch 66/123 loss = 0.0074\n",
      "epoch 20 batch 67/123 loss = 0.0098\n",
      "epoch 20 batch 68/123 loss = 0.0083\n",
      "epoch 20 batch 69/123 loss = 0.0078\n",
      "epoch 20 batch 70/123 loss = 0.0083\n",
      "epoch 20 batch 71/123 loss = 0.0088\n",
      "epoch 20 batch 72/123 loss = 0.0087\n",
      "epoch 20 batch 73/123 loss = 0.0091\n",
      "epoch 20 batch 74/123 loss = 0.0068\n",
      "epoch 20 batch 75/123 loss = 0.0071\n",
      "epoch 20 batch 76/123 loss = 0.0079\n",
      "epoch 20 batch 77/123 loss = 0.0074\n",
      "epoch 20 batch 78/123 loss = 0.0073\n",
      "epoch 20 batch 79/123 loss = 0.0082\n",
      "epoch 20 batch 80/123 loss = 0.0068\n",
      "epoch 20 batch 81/123 loss = 0.0087\n",
      "epoch 20 batch 82/123 loss = 0.0081\n",
      "epoch 20 batch 83/123 loss = 0.0076\n",
      "epoch 20 batch 84/123 loss = 0.0073\n",
      "epoch 20 batch 85/123 loss = 0.0083\n",
      "epoch 20 batch 86/123 loss = 0.0075\n",
      "epoch 20 batch 87/123 loss = 0.0073\n",
      "epoch 20 batch 88/123 loss = 0.0075\n",
      "epoch 20 batch 89/123 loss = 0.0074\n",
      "epoch 20 batch 90/123 loss = 0.0068\n",
      "epoch 20 batch 91/123 loss = 0.0070\n",
      "epoch 20 batch 92/123 loss = 0.0075\n",
      "epoch 20 batch 93/123 loss = 0.0083\n",
      "epoch 20 batch 94/123 loss = 0.0070\n",
      "epoch 20 batch 95/123 loss = 0.0076\n",
      "epoch 20 batch 96/123 loss = 0.0073\n",
      "epoch 20 batch 97/123 loss = 0.0069\n",
      "epoch 20 batch 98/123 loss = 0.0074\n",
      "epoch 20 batch 99/123 loss = 0.0067\n",
      "epoch 20 batch 100/123 loss = 0.0068\n",
      "epoch 20 batch 101/123 loss = 0.0095\n",
      "epoch 20 batch 102/123 loss = 0.0073\n",
      "epoch 20 batch 103/123 loss = 0.0073\n",
      "epoch 20 batch 104/123 loss = 0.0076\n",
      "epoch 20 batch 105/123 loss = 0.0063\n",
      "epoch 20 batch 106/123 loss = 0.0075\n",
      "epoch 20 batch 107/123 loss = 0.0064\n",
      "epoch 20 batch 108/123 loss = 0.0062\n",
      "epoch 20 batch 109/123 loss = 0.0074\n",
      "epoch 20 batch 110/123 loss = 0.0073\n",
      "epoch 20 batch 111/123 loss = 0.0070\n",
      "epoch 20 batch 112/123 loss = 0.0080\n",
      "epoch 20 batch 113/123 loss = 0.0067\n",
      "epoch 20 batch 114/123 loss = 0.0075\n",
      "epoch 20 batch 115/123 loss = 0.0085\n",
      "epoch 20 batch 116/123 loss = 0.0097\n",
      "epoch 20 batch 117/123 loss = 0.0097\n",
      "epoch 20 batch 118/123 loss = 0.0077\n",
      "epoch 20 batch 119/123 loss = 0.0071\n",
      "epoch 20 batch 120/123 loss = 0.0071\n",
      "epoch 20 batch 121/123 loss = 0.0079\n",
      "epoch 20 batch 122/123 loss = 0.0076\n",
      "epoch 20 batch 123/123 average loss = 0.0093 last loss = 0.0069\n",
      "epoch 21 batch 1/123 loss = 0.0070\n",
      "epoch 21 batch 2/123 loss = 0.0066\n",
      "epoch 21 batch 3/123 loss = 0.0060\n",
      "epoch 21 batch 4/123 loss = 0.0069\n",
      "epoch 21 batch 5/123 loss = 0.0068\n",
      "epoch 21 batch 6/123 loss = 0.0065\n",
      "epoch 21 batch 7/123 loss = 0.0057\n",
      "epoch 21 batch 8/123 loss = 0.0065\n",
      "epoch 21 batch 9/123 loss = 0.0065\n",
      "epoch 21 batch 10/123 loss = 0.0065\n",
      "epoch 21 batch 11/123 loss = 0.0059\n",
      "epoch 21 batch 12/123 loss = 0.0061\n",
      "epoch 21 batch 13/123 loss = 0.0057\n",
      "epoch 21 batch 14/123 loss = 0.0054\n",
      "epoch 21 batch 15/123 loss = 0.0061\n",
      "epoch 21 batch 16/123 loss = 0.0058\n",
      "epoch 21 batch 17/123 loss = 0.0061\n",
      "epoch 21 batch 18/123 loss = 0.0063\n",
      "epoch 21 batch 19/123 loss = 0.0059\n",
      "epoch 21 batch 20/123 loss = 0.0062\n",
      "epoch 21 batch 21/123 loss = 0.0062\n",
      "epoch 21 batch 22/123 loss = 0.0063\n",
      "epoch 21 batch 23/123 loss = 0.0053\n",
      "epoch 21 batch 24/123 loss = 0.0055\n",
      "epoch 21 batch 25/123 loss = 0.0056\n",
      "epoch 21 batch 26/123 loss = 0.0054\n",
      "epoch 21 batch 27/123 loss = 0.0067\n",
      "epoch 21 batch 28/123 loss = 0.0056\n",
      "epoch 21 batch 29/123 loss = 0.0057\n",
      "epoch 21 batch 30/123 loss = 0.0055\n",
      "epoch 21 batch 31/123 loss = 0.0053\n",
      "epoch 21 batch 32/123 loss = 0.0054\n",
      "epoch 21 batch 33/123 loss = 0.0051\n",
      "epoch 21 batch 34/123 loss = 0.0048\n",
      "epoch 21 batch 35/123 loss = 0.0053\n",
      "epoch 21 batch 36/123 loss = 0.0057\n",
      "epoch 21 batch 37/123 loss = 0.0055\n",
      "epoch 21 batch 38/123 loss = 0.0064\n",
      "epoch 21 batch 39/123 loss = 0.0060\n",
      "epoch 21 batch 40/123 loss = 0.0058\n",
      "epoch 21 batch 41/123 loss = 0.0057\n",
      "epoch 21 batch 42/123 loss = 0.0053\n",
      "epoch 21 batch 43/123 loss = 0.0050\n",
      "epoch 21 batch 44/123 loss = 0.0059\n",
      "epoch 21 batch 45/123 loss = 0.0064\n",
      "epoch 21 batch 46/123 loss = 0.0061\n",
      "epoch 21 batch 47/123 loss = 0.0057\n",
      "epoch 21 batch 48/123 loss = 0.0059\n",
      "epoch 21 batch 49/123 loss = 0.0053\n",
      "epoch 21 batch 50/123 loss = 0.0059\n",
      "epoch 21 batch 51/123 loss = 0.0055\n",
      "epoch 21 batch 52/123 loss = 0.0055\n",
      "epoch 21 batch 53/123 loss = 0.0056\n",
      "epoch 21 batch 54/123 loss = 0.0058\n",
      "epoch 21 batch 55/123 loss = 0.0052\n",
      "epoch 21 batch 56/123 loss = 0.0052\n",
      "epoch 21 batch 57/123 loss = 0.0054\n",
      "epoch 21 batch 58/123 loss = 0.0051\n",
      "epoch 21 batch 59/123 loss = 0.0049\n",
      "epoch 21 batch 60/123 loss = 0.0052\n",
      "epoch 21 batch 61/123 loss = 0.0051\n",
      "epoch 21 batch 62/123 loss = 0.0054\n",
      "epoch 21 batch 63/123 loss = 0.0057\n",
      "epoch 21 batch 64/123 loss = 0.0058\n",
      "epoch 21 batch 65/123 loss = 0.0059\n",
      "epoch 21 batch 66/123 loss = 0.0065\n",
      "epoch 21 batch 67/123 loss = 0.0055\n",
      "epoch 21 batch 68/123 loss = 0.0057\n",
      "epoch 21 batch 69/123 loss = 0.0059\n",
      "epoch 21 batch 70/123 loss = 0.0053\n",
      "epoch 21 batch 71/123 loss = 0.0055\n",
      "epoch 21 batch 72/123 loss = 0.0062\n",
      "epoch 21 batch 73/123 loss = 0.0057\n",
      "epoch 21 batch 74/123 loss = 0.0057\n",
      "epoch 21 batch 75/123 loss = 0.0058\n",
      "epoch 21 batch 76/123 loss = 0.0060\n",
      "epoch 21 batch 77/123 loss = 0.0057\n",
      "epoch 21 batch 78/123 loss = 0.0061\n",
      "epoch 21 batch 79/123 loss = 0.0052\n",
      "epoch 21 batch 80/123 loss = 0.0054\n",
      "epoch 21 batch 81/123 loss = 0.0059\n",
      "epoch 21 batch 82/123 loss = 0.0059\n",
      "epoch 21 batch 83/123 loss = 0.0060\n",
      "epoch 21 batch 84/123 loss = 0.0058\n",
      "epoch 21 batch 85/123 loss = 0.0055\n",
      "epoch 21 batch 86/123 loss = 0.0068\n",
      "epoch 21 batch 87/123 loss = 0.0054\n",
      "epoch 21 batch 88/123 loss = 0.0067\n",
      "epoch 21 batch 89/123 loss = 0.0065\n",
      "epoch 21 batch 90/123 loss = 0.0058\n",
      "epoch 21 batch 91/123 loss = 0.0064\n",
      "epoch 21 batch 92/123 loss = 0.0063\n",
      "epoch 21 batch 93/123 loss = 0.0067\n",
      "epoch 21 batch 94/123 loss = 0.0060\n",
      "epoch 21 batch 95/123 loss = 0.0061\n",
      "epoch 21 batch 96/123 loss = 0.0056\n",
      "epoch 21 batch 97/123 loss = 0.0050\n",
      "epoch 21 batch 98/123 loss = 0.0056\n",
      "epoch 21 batch 99/123 loss = 0.0059\n",
      "epoch 21 batch 100/123 loss = 0.0053\n",
      "epoch 21 batch 101/123 loss = 0.0059\n",
      "epoch 21 batch 102/123 loss = 0.0053\n",
      "epoch 21 batch 103/123 loss = 0.0055\n",
      "epoch 21 batch 104/123 loss = 0.0048\n",
      "epoch 21 batch 105/123 loss = 0.0060\n",
      "epoch 21 batch 106/123 loss = 0.0059\n",
      "epoch 21 batch 107/123 loss = 0.0063\n",
      "epoch 21 batch 108/123 loss = 0.0061\n",
      "epoch 21 batch 109/123 loss = 0.0059\n",
      "epoch 21 batch 110/123 loss = 0.0056\n",
      "epoch 21 batch 111/123 loss = 0.0066\n",
      "epoch 21 batch 112/123 loss = 0.0052\n",
      "epoch 21 batch 113/123 loss = 0.0056\n",
      "epoch 21 batch 114/123 loss = 0.0053\n",
      "epoch 21 batch 115/123 loss = 0.0059\n",
      "epoch 21 batch 116/123 loss = 0.0061\n",
      "epoch 21 batch 117/123 loss = 0.0054\n",
      "epoch 21 batch 118/123 loss = 0.0052\n",
      "epoch 21 batch 119/123 loss = 0.0059\n",
      "epoch 21 batch 120/123 loss = 0.0047\n",
      "epoch 21 batch 121/123 loss = 0.0053\n",
      "epoch 21 batch 122/123 loss = 0.0046\n",
      "epoch 21 batch 123/123 average loss = 0.0058 last loss = 0.0052\n",
      "epoch 22 batch 1/123 loss = 0.0053\n",
      "epoch 22 batch 2/123 loss = 0.0049\n",
      "epoch 22 batch 3/123 loss = 0.0057\n",
      "epoch 22 batch 4/123 loss = 0.0055\n",
      "epoch 22 batch 5/123 loss = 0.0050\n",
      "epoch 22 batch 6/123 loss = 0.0047\n",
      "epoch 22 batch 7/123 loss = 0.0047\n",
      "epoch 22 batch 8/123 loss = 0.0046\n",
      "epoch 22 batch 9/123 loss = 0.0055\n",
      "epoch 22 batch 10/123 loss = 0.0047\n",
      "epoch 22 batch 11/123 loss = 0.0049\n",
      "epoch 22 batch 12/123 loss = 0.0050\n",
      "epoch 22 batch 13/123 loss = 0.0048\n",
      "epoch 22 batch 14/123 loss = 0.0053\n",
      "epoch 22 batch 15/123 loss = 0.0057\n",
      "epoch 22 batch 16/123 loss = 0.0051\n",
      "epoch 22 batch 17/123 loss = 0.0049\n",
      "epoch 22 batch 18/123 loss = 0.0050\n",
      "epoch 22 batch 19/123 loss = 0.0046\n",
      "epoch 22 batch 20/123 loss = 0.0043\n",
      "epoch 22 batch 21/123 loss = 0.0049\n",
      "epoch 22 batch 22/123 loss = 0.0052\n",
      "epoch 22 batch 23/123 loss = 0.0047\n",
      "epoch 22 batch 24/123 loss = 0.0049\n",
      "epoch 22 batch 25/123 loss = 0.0052\n",
      "epoch 22 batch 26/123 loss = 0.0050\n",
      "epoch 22 batch 27/123 loss = 0.0051\n",
      "epoch 22 batch 28/123 loss = 0.0056\n",
      "epoch 22 batch 29/123 loss = 0.0046\n",
      "epoch 22 batch 30/123 loss = 0.0051\n",
      "epoch 22 batch 31/123 loss = 0.0045\n",
      "epoch 22 batch 32/123 loss = 0.0045\n",
      "epoch 22 batch 33/123 loss = 0.0050\n",
      "epoch 22 batch 34/123 loss = 0.0046\n",
      "epoch 22 batch 35/123 loss = 0.0050\n",
      "epoch 22 batch 36/123 loss = 0.0052\n",
      "epoch 22 batch 37/123 loss = 0.0051\n",
      "epoch 22 batch 38/123 loss = 0.0049\n",
      "epoch 22 batch 39/123 loss = 0.0048\n",
      "epoch 22 batch 40/123 loss = 0.0050\n",
      "epoch 22 batch 41/123 loss = 0.0052\n",
      "epoch 22 batch 42/123 loss = 0.0050\n",
      "epoch 22 batch 43/123 loss = 0.0048\n",
      "epoch 22 batch 44/123 loss = 0.0049\n",
      "epoch 22 batch 45/123 loss = 0.0044\n",
      "epoch 22 batch 46/123 loss = 0.0046\n",
      "epoch 22 batch 47/123 loss = 0.0048\n",
      "epoch 22 batch 48/123 loss = 0.0048\n",
      "epoch 22 batch 49/123 loss = 0.0049\n",
      "epoch 22 batch 50/123 loss = 0.0048\n",
      "epoch 22 batch 51/123 loss = 0.0042\n",
      "epoch 22 batch 52/123 loss = 0.0048\n",
      "epoch 22 batch 53/123 loss = 0.0056\n",
      "epoch 22 batch 54/123 loss = 0.0050\n",
      "epoch 22 batch 55/123 loss = 0.0051\n",
      "epoch 22 batch 56/123 loss = 0.0049\n",
      "epoch 22 batch 57/123 loss = 0.0049\n",
      "epoch 22 batch 58/123 loss = 0.0050\n",
      "epoch 22 batch 59/123 loss = 0.0050\n",
      "epoch 22 batch 60/123 loss = 0.0048\n",
      "epoch 22 batch 61/123 loss = 0.0048\n",
      "epoch 22 batch 62/123 loss = 0.0048\n",
      "epoch 22 batch 63/123 loss = 0.0052\n",
      "epoch 22 batch 64/123 loss = 0.0051\n",
      "epoch 22 batch 65/123 loss = 0.0048\n",
      "epoch 22 batch 66/123 loss = 0.0044\n",
      "epoch 22 batch 67/123 loss = 0.0042\n",
      "epoch 22 batch 68/123 loss = 0.0052\n",
      "epoch 22 batch 69/123 loss = 0.0053\n",
      "epoch 22 batch 70/123 loss = 0.0051\n",
      "epoch 22 batch 71/123 loss = 0.0046\n",
      "epoch 22 batch 72/123 loss = 0.0049\n",
      "epoch 22 batch 73/123 loss = 0.0049\n",
      "epoch 22 batch 74/123 loss = 0.0046\n",
      "epoch 22 batch 75/123 loss = 0.0050\n",
      "epoch 22 batch 76/123 loss = 0.0045\n",
      "epoch 22 batch 77/123 loss = 0.0047\n",
      "epoch 22 batch 78/123 loss = 0.0049\n",
      "epoch 22 batch 79/123 loss = 0.0045\n",
      "epoch 22 batch 80/123 loss = 0.0045\n",
      "epoch 22 batch 81/123 loss = 0.0049\n",
      "epoch 22 batch 82/123 loss = 0.0050\n",
      "epoch 22 batch 83/123 loss = 0.0048\n",
      "epoch 22 batch 84/123 loss = 0.0051\n",
      "epoch 22 batch 85/123 loss = 0.0046\n",
      "epoch 22 batch 86/123 loss = 0.0048\n",
      "epoch 22 batch 87/123 loss = 0.0047\n",
      "epoch 22 batch 88/123 loss = 0.0045\n",
      "epoch 22 batch 89/123 loss = 0.0051\n",
      "epoch 22 batch 90/123 loss = 0.0049\n",
      "epoch 22 batch 91/123 loss = 0.0046\n",
      "epoch 22 batch 92/123 loss = 0.0045\n",
      "epoch 22 batch 93/123 loss = 0.0049\n",
      "epoch 22 batch 94/123 loss = 0.0052\n",
      "epoch 22 batch 95/123 loss = 0.0063\n",
      "epoch 22 batch 96/123 loss = 0.0064\n",
      "epoch 22 batch 97/123 loss = 0.0071\n",
      "epoch 22 batch 98/123 loss = 0.0064\n",
      "epoch 22 batch 99/123 loss = 0.0050\n",
      "epoch 22 batch 100/123 loss = 0.0055\n",
      "epoch 22 batch 101/123 loss = 0.0053\n",
      "epoch 22 batch 102/123 loss = 0.0051\n",
      "epoch 22 batch 103/123 loss = 0.0055\n",
      "epoch 22 batch 104/123 loss = 0.0048\n",
      "epoch 22 batch 105/123 loss = 0.0049\n",
      "epoch 22 batch 106/123 loss = 0.0054\n",
      "epoch 22 batch 107/123 loss = 0.0051\n",
      "epoch 22 batch 108/123 loss = 0.0048\n",
      "epoch 22 batch 109/123 loss = 0.0054\n",
      "epoch 22 batch 110/123 loss = 0.0057\n",
      "epoch 22 batch 111/123 loss = 0.0048\n",
      "epoch 22 batch 112/123 loss = 0.0049\n",
      "epoch 22 batch 113/123 loss = 0.0048\n",
      "epoch 22 batch 114/123 loss = 0.0047\n",
      "epoch 22 batch 115/123 loss = 0.0049\n",
      "epoch 22 batch 116/123 loss = 0.0052\n",
      "epoch 22 batch 117/123 loss = 0.0044\n",
      "epoch 22 batch 118/123 loss = 0.0051\n",
      "epoch 22 batch 119/123 loss = 0.0051\n",
      "epoch 22 batch 120/123 loss = 0.0050\n",
      "epoch 22 batch 121/123 loss = 0.0053\n",
      "epoch 22 batch 122/123 loss = 0.0053\n",
      "epoch 22 batch 123/123 average loss = 0.0050 last loss = 0.0050\n",
      "epoch 23 batch 1/123 loss = 0.0044\n",
      "epoch 23 batch 2/123 loss = 0.0052\n",
      "epoch 23 batch 3/123 loss = 0.0050\n",
      "epoch 23 batch 4/123 loss = 0.0054\n",
      "epoch 23 batch 5/123 loss = 0.0047\n",
      "epoch 23 batch 6/123 loss = 0.0049\n",
      "epoch 23 batch 7/123 loss = 0.0049\n",
      "epoch 23 batch 8/123 loss = 0.0048\n",
      "epoch 23 batch 9/123 loss = 0.0052\n",
      "epoch 23 batch 10/123 loss = 0.0050\n",
      "epoch 23 batch 11/123 loss = 0.0044\n",
      "epoch 23 batch 12/123 loss = 0.0045\n",
      "epoch 23 batch 13/123 loss = 0.0044\n",
      "epoch 23 batch 14/123 loss = 0.0045\n",
      "epoch 23 batch 15/123 loss = 0.0047\n",
      "epoch 23 batch 16/123 loss = 0.0043\n",
      "epoch 23 batch 17/123 loss = 0.0051\n",
      "epoch 23 batch 18/123 loss = 0.0048\n",
      "epoch 23 batch 19/123 loss = 0.0055\n",
      "epoch 23 batch 20/123 loss = 0.0055\n",
      "epoch 23 batch 21/123 loss = 0.0053\n",
      "epoch 23 batch 22/123 loss = 0.0050\n",
      "epoch 23 batch 23/123 loss = 0.0048\n",
      "epoch 23 batch 24/123 loss = 0.0047\n",
      "epoch 23 batch 25/123 loss = 0.0052\n",
      "epoch 23 batch 26/123 loss = 0.0055\n",
      "epoch 23 batch 27/123 loss = 0.0042\n",
      "epoch 23 batch 28/123 loss = 0.0046\n",
      "epoch 23 batch 29/123 loss = 0.0046\n",
      "epoch 23 batch 30/123 loss = 0.0050\n",
      "epoch 23 batch 31/123 loss = 0.0044\n",
      "epoch 23 batch 32/123 loss = 0.0048\n",
      "epoch 23 batch 33/123 loss = 0.0045\n",
      "epoch 23 batch 34/123 loss = 0.0048\n",
      "epoch 23 batch 35/123 loss = 0.0053\n",
      "epoch 23 batch 36/123 loss = 0.0043\n",
      "epoch 23 batch 37/123 loss = 0.0042\n",
      "epoch 23 batch 38/123 loss = 0.0047\n",
      "epoch 23 batch 39/123 loss = 0.0051\n",
      "epoch 23 batch 40/123 loss = 0.0048\n",
      "epoch 23 batch 41/123 loss = 0.0050\n",
      "epoch 23 batch 42/123 loss = 0.0047\n",
      "epoch 23 batch 43/123 loss = 0.0048\n",
      "epoch 23 batch 44/123 loss = 0.0053\n",
      "epoch 23 batch 45/123 loss = 0.0045\n",
      "epoch 23 batch 46/123 loss = 0.0048\n",
      "epoch 23 batch 47/123 loss = 0.0044\n",
      "epoch 23 batch 48/123 loss = 0.0044\n",
      "epoch 23 batch 49/123 loss = 0.0051\n",
      "epoch 23 batch 50/123 loss = 0.0044\n",
      "epoch 23 batch 51/123 loss = 0.0053\n",
      "epoch 23 batch 52/123 loss = 0.0044\n",
      "epoch 23 batch 53/123 loss = 0.0048\n",
      "epoch 23 batch 54/123 loss = 0.0051\n",
      "epoch 23 batch 55/123 loss = 0.0059\n",
      "epoch 23 batch 56/123 loss = 0.0050\n",
      "epoch 23 batch 57/123 loss = 0.0046\n",
      "epoch 23 batch 58/123 loss = 0.0052\n",
      "epoch 23 batch 59/123 loss = 0.0050\n",
      "epoch 23 batch 60/123 loss = 0.0053\n",
      "epoch 23 batch 61/123 loss = 0.0048\n",
      "epoch 23 batch 62/123 loss = 0.0045\n",
      "epoch 23 batch 63/123 loss = 0.0043\n",
      "epoch 23 batch 64/123 loss = 0.0050\n",
      "epoch 23 batch 65/123 loss = 0.0043\n",
      "epoch 23 batch 66/123 loss = 0.0041\n",
      "epoch 23 batch 67/123 loss = 0.0047\n",
      "epoch 23 batch 68/123 loss = 0.0049\n",
      "epoch 23 batch 69/123 loss = 0.0047\n",
      "epoch 23 batch 70/123 loss = 0.0046\n",
      "epoch 23 batch 71/123 loss = 0.0047\n",
      "epoch 23 batch 72/123 loss = 0.0046\n",
      "epoch 23 batch 73/123 loss = 0.0049\n",
      "epoch 23 batch 74/123 loss = 0.0044\n",
      "epoch 23 batch 75/123 loss = 0.0047\n",
      "epoch 23 batch 76/123 loss = 0.0050\n",
      "epoch 23 batch 77/123 loss = 0.0050\n",
      "epoch 23 batch 78/123 loss = 0.0049\n",
      "epoch 23 batch 79/123 loss = 0.0047\n",
      "epoch 23 batch 80/123 loss = 0.0043\n",
      "epoch 23 batch 81/123 loss = 0.0049\n",
      "epoch 23 batch 82/123 loss = 0.0044\n",
      "epoch 23 batch 83/123 loss = 0.0049\n",
      "epoch 23 batch 84/123 loss = 0.0049\n",
      "epoch 23 batch 85/123 loss = 0.0041\n",
      "epoch 23 batch 86/123 loss = 0.0049\n",
      "epoch 23 batch 87/123 loss = 0.0050\n",
      "epoch 23 batch 88/123 loss = 0.0046\n",
      "epoch 23 batch 89/123 loss = 0.0045\n",
      "epoch 23 batch 90/123 loss = 0.0047\n",
      "epoch 23 batch 91/123 loss = 0.0046\n",
      "epoch 23 batch 92/123 loss = 0.0043\n",
      "epoch 23 batch 93/123 loss = 0.0042\n",
      "epoch 23 batch 94/123 loss = 0.0041\n",
      "epoch 23 batch 95/123 loss = 0.0046\n",
      "epoch 23 batch 96/123 loss = 0.0044\n",
      "epoch 23 batch 97/123 loss = 0.0043\n",
      "epoch 23 batch 98/123 loss = 0.0045\n",
      "epoch 23 batch 99/123 loss = 0.0046\n",
      "epoch 23 batch 100/123 loss = 0.0044\n",
      "epoch 23 batch 101/123 loss = 0.0046\n",
      "epoch 23 batch 102/123 loss = 0.0046\n",
      "epoch 23 batch 103/123 loss = 0.0046\n",
      "epoch 23 batch 104/123 loss = 0.0044\n",
      "epoch 23 batch 105/123 loss = 0.0049\n",
      "epoch 23 batch 106/123 loss = 0.0046\n",
      "epoch 23 batch 107/123 loss = 0.0042\n",
      "epoch 23 batch 108/123 loss = 0.0050\n",
      "epoch 23 batch 109/123 loss = 0.0046\n",
      "epoch 23 batch 110/123 loss = 0.0053\n",
      "epoch 23 batch 111/123 loss = 0.0052\n",
      "epoch 23 batch 112/123 loss = 0.0043\n",
      "epoch 23 batch 113/123 loss = 0.0049\n",
      "epoch 23 batch 114/123 loss = 0.0046\n",
      "epoch 23 batch 115/123 loss = 0.0048\n",
      "epoch 23 batch 116/123 loss = 0.0046\n",
      "epoch 23 batch 117/123 loss = 0.0049\n",
      "epoch 23 batch 118/123 loss = 0.0046\n",
      "epoch 23 batch 119/123 loss = 0.0048\n",
      "epoch 23 batch 120/123 loss = 0.0052\n",
      "epoch 23 batch 121/123 loss = 0.0045\n",
      "epoch 23 batch 122/123 loss = 0.0044\n",
      "epoch 23 batch 123/123 average loss = 0.0047 last loss = 0.0042\n",
      "epoch 24 batch 1/123 loss = 0.0042\n",
      "epoch 24 batch 2/123 loss = 0.0053\n",
      "epoch 24 batch 3/123 loss = 0.0047\n",
      "epoch 24 batch 4/123 loss = 0.0044\n",
      "epoch 24 batch 5/123 loss = 0.0042\n",
      "epoch 24 batch 6/123 loss = 0.0051\n",
      "epoch 24 batch 7/123 loss = 0.0043\n",
      "epoch 24 batch 8/123 loss = 0.0046\n",
      "epoch 24 batch 9/123 loss = 0.0048\n",
      "epoch 24 batch 10/123 loss = 0.0044\n",
      "epoch 24 batch 11/123 loss = 0.0045\n",
      "epoch 24 batch 12/123 loss = 0.0047\n",
      "epoch 24 batch 13/123 loss = 0.0049\n",
      "epoch 24 batch 14/123 loss = 0.0053\n",
      "epoch 24 batch 15/123 loss = 0.0047\n",
      "epoch 24 batch 16/123 loss = 0.0042\n",
      "epoch 24 batch 17/123 loss = 0.0045\n",
      "epoch 24 batch 18/123 loss = 0.0041\n",
      "epoch 24 batch 19/123 loss = 0.0052\n",
      "epoch 24 batch 20/123 loss = 0.0045\n",
      "epoch 24 batch 21/123 loss = 0.0044\n",
      "epoch 24 batch 22/123 loss = 0.0041\n",
      "epoch 24 batch 23/123 loss = 0.0044\n",
      "epoch 24 batch 24/123 loss = 0.0046\n",
      "epoch 24 batch 25/123 loss = 0.0044\n",
      "epoch 24 batch 26/123 loss = 0.0048\n",
      "epoch 24 batch 27/123 loss = 0.0042\n",
      "epoch 24 batch 28/123 loss = 0.0042\n",
      "epoch 24 batch 29/123 loss = 0.0042\n",
      "epoch 24 batch 30/123 loss = 0.0045\n",
      "epoch 24 batch 31/123 loss = 0.0043\n",
      "epoch 24 batch 32/123 loss = 0.0048\n",
      "epoch 24 batch 33/123 loss = 0.0043\n",
      "epoch 24 batch 34/123 loss = 0.0048\n",
      "epoch 24 batch 35/123 loss = 0.0046\n",
      "epoch 24 batch 36/123 loss = 0.0048\n",
      "epoch 24 batch 37/123 loss = 0.0048\n",
      "epoch 24 batch 38/123 loss = 0.0043\n",
      "epoch 24 batch 39/123 loss = 0.0051\n",
      "epoch 24 batch 40/123 loss = 0.0045\n",
      "epoch 24 batch 41/123 loss = 0.0049\n",
      "epoch 24 batch 42/123 loss = 0.0039\n",
      "epoch 24 batch 43/123 loss = 0.0038\n",
      "epoch 24 batch 44/123 loss = 0.0044\n",
      "epoch 24 batch 45/123 loss = 0.0044\n",
      "epoch 24 batch 46/123 loss = 0.0044\n",
      "epoch 24 batch 47/123 loss = 0.0046\n",
      "epoch 24 batch 48/123 loss = 0.0042\n",
      "epoch 24 batch 49/123 loss = 0.0050\n",
      "epoch 24 batch 50/123 loss = 0.0043\n",
      "epoch 24 batch 51/123 loss = 0.0042\n",
      "epoch 24 batch 52/123 loss = 0.0044\n",
      "epoch 24 batch 53/123 loss = 0.0044\n",
      "epoch 24 batch 54/123 loss = 0.0048\n",
      "epoch 24 batch 55/123 loss = 0.0041\n",
      "epoch 24 batch 56/123 loss = 0.0042\n",
      "epoch 24 batch 57/123 loss = 0.0044\n",
      "epoch 24 batch 58/123 loss = 0.0050\n",
      "epoch 24 batch 59/123 loss = 0.0044\n",
      "epoch 24 batch 60/123 loss = 0.0049\n",
      "epoch 24 batch 61/123 loss = 0.0044\n",
      "epoch 24 batch 62/123 loss = 0.0053\n",
      "epoch 24 batch 63/123 loss = 0.0045\n",
      "epoch 24 batch 64/123 loss = 0.0045\n",
      "epoch 24 batch 65/123 loss = 0.0047\n",
      "epoch 24 batch 66/123 loss = 0.0043\n",
      "epoch 24 batch 67/123 loss = 0.0044\n",
      "epoch 24 batch 68/123 loss = 0.0047\n",
      "epoch 24 batch 69/123 loss = 0.0042\n",
      "epoch 24 batch 70/123 loss = 0.0043\n",
      "epoch 24 batch 71/123 loss = 0.0047\n",
      "epoch 24 batch 72/123 loss = 0.0042\n",
      "epoch 24 batch 73/123 loss = 0.0045\n",
      "epoch 24 batch 74/123 loss = 0.0046\n",
      "epoch 24 batch 75/123 loss = 0.0044\n",
      "epoch 24 batch 76/123 loss = 0.0042\n",
      "epoch 24 batch 77/123 loss = 0.0039\n",
      "epoch 24 batch 78/123 loss = 0.0041\n",
      "epoch 24 batch 79/123 loss = 0.0051\n",
      "epoch 24 batch 80/123 loss = 0.0045\n",
      "epoch 24 batch 81/123 loss = 0.0048\n",
      "epoch 24 batch 82/123 loss = 0.0046\n",
      "epoch 24 batch 83/123 loss = 0.0047\n",
      "epoch 24 batch 84/123 loss = 0.0045\n",
      "epoch 24 batch 85/123 loss = 0.0046\n",
      "epoch 24 batch 86/123 loss = 0.0048\n",
      "epoch 24 batch 87/123 loss = 0.0046\n",
      "epoch 24 batch 88/123 loss = 0.0050\n",
      "epoch 24 batch 89/123 loss = 0.0047\n",
      "epoch 24 batch 90/123 loss = 0.0051\n",
      "epoch 24 batch 91/123 loss = 0.0044\n",
      "epoch 24 batch 92/123 loss = 0.0045\n",
      "epoch 24 batch 93/123 loss = 0.0042\n",
      "epoch 24 batch 94/123 loss = 0.0048\n",
      "epoch 24 batch 95/123 loss = 0.0042\n",
      "epoch 24 batch 96/123 loss = 0.0044\n",
      "epoch 24 batch 97/123 loss = 0.0043\n",
      "epoch 24 batch 98/123 loss = 0.0046\n",
      "epoch 24 batch 99/123 loss = 0.0043\n",
      "epoch 24 batch 100/123 loss = 0.0045\n",
      "epoch 24 batch 101/123 loss = 0.0045\n",
      "epoch 24 batch 102/123 loss = 0.0043\n",
      "epoch 24 batch 103/123 loss = 0.0044\n",
      "epoch 24 batch 104/123 loss = 0.0044\n",
      "epoch 24 batch 105/123 loss = 0.0047\n",
      "epoch 24 batch 106/123 loss = 0.0043\n",
      "epoch 24 batch 107/123 loss = 0.0045\n",
      "epoch 24 batch 108/123 loss = 0.0043\n",
      "epoch 24 batch 109/123 loss = 0.0039\n",
      "epoch 24 batch 110/123 loss = 0.0042\n",
      "epoch 24 batch 111/123 loss = 0.0046\n",
      "epoch 24 batch 112/123 loss = 0.0047\n",
      "epoch 24 batch 113/123 loss = 0.0042\n",
      "epoch 24 batch 114/123 loss = 0.0040\n",
      "epoch 24 batch 115/123 loss = 0.0040\n",
      "epoch 24 batch 116/123 loss = 0.0044\n",
      "epoch 24 batch 117/123 loss = 0.0045\n",
      "epoch 24 batch 118/123 loss = 0.0041\n",
      "epoch 24 batch 119/123 loss = 0.0044\n",
      "epoch 24 batch 120/123 loss = 0.0046\n",
      "epoch 24 batch 121/123 loss = 0.0041\n",
      "epoch 24 batch 122/123 loss = 0.0046\n",
      "epoch 24 batch 123/123 average loss = 0.0045 last loss = 0.0041\n",
      "epoch 25 batch 1/123 loss = 0.0043\n",
      "epoch 25 batch 2/123 loss = 0.0041\n",
      "epoch 25 batch 3/123 loss = 0.0052\n",
      "epoch 25 batch 4/123 loss = 0.0044\n",
      "epoch 25 batch 5/123 loss = 0.0044\n",
      "epoch 25 batch 6/123 loss = 0.0040\n",
      "epoch 25 batch 7/123 loss = 0.0045\n",
      "epoch 25 batch 8/123 loss = 0.0043\n",
      "epoch 25 batch 9/123 loss = 0.0040\n",
      "epoch 25 batch 10/123 loss = 0.0043\n",
      "epoch 25 batch 11/123 loss = 0.0040\n",
      "epoch 25 batch 12/123 loss = 0.0043\n",
      "epoch 25 batch 13/123 loss = 0.0041\n",
      "epoch 25 batch 14/123 loss = 0.0039\n",
      "epoch 25 batch 15/123 loss = 0.0045\n",
      "epoch 25 batch 16/123 loss = 0.0041\n",
      "epoch 25 batch 17/123 loss = 0.0042\n",
      "epoch 25 batch 18/123 loss = 0.0043\n",
      "epoch 25 batch 19/123 loss = 0.0040\n",
      "epoch 25 batch 20/123 loss = 0.0039\n",
      "epoch 25 batch 21/123 loss = 0.0040\n",
      "epoch 25 batch 22/123 loss = 0.0038\n",
      "epoch 25 batch 23/123 loss = 0.0042\n",
      "epoch 25 batch 24/123 loss = 0.0038\n",
      "epoch 25 batch 25/123 loss = 0.0042\n",
      "epoch 25 batch 26/123 loss = 0.0041\n",
      "epoch 25 batch 27/123 loss = 0.0040\n",
      "epoch 25 batch 28/123 loss = 0.0046\n",
      "epoch 25 batch 29/123 loss = 0.0043\n",
      "epoch 25 batch 30/123 loss = 0.0043\n",
      "epoch 25 batch 31/123 loss = 0.0047\n",
      "epoch 25 batch 32/123 loss = 0.0042\n",
      "epoch 25 batch 33/123 loss = 0.0039\n",
      "epoch 25 batch 34/123 loss = 0.0041\n",
      "epoch 25 batch 35/123 loss = 0.0048\n",
      "epoch 25 batch 36/123 loss = 0.0043\n",
      "epoch 25 batch 37/123 loss = 0.0043\n",
      "epoch 25 batch 38/123 loss = 0.0046\n",
      "epoch 25 batch 39/123 loss = 0.0045\n",
      "epoch 25 batch 40/123 loss = 0.0041\n",
      "epoch 25 batch 41/123 loss = 0.0041\n",
      "epoch 25 batch 42/123 loss = 0.0040\n",
      "epoch 25 batch 43/123 loss = 0.0044\n",
      "epoch 25 batch 44/123 loss = 0.0039\n",
      "epoch 25 batch 45/123 loss = 0.0042\n",
      "epoch 25 batch 46/123 loss = 0.0045\n",
      "epoch 25 batch 47/123 loss = 0.0044\n",
      "epoch 25 batch 48/123 loss = 0.0041\n",
      "epoch 25 batch 49/123 loss = 0.0043\n",
      "epoch 25 batch 50/123 loss = 0.0039\n",
      "epoch 25 batch 51/123 loss = 0.0041\n",
      "epoch 25 batch 52/123 loss = 0.0042\n",
      "epoch 25 batch 53/123 loss = 0.0041\n",
      "epoch 25 batch 54/123 loss = 0.0039\n",
      "epoch 25 batch 55/123 loss = 0.0044\n",
      "epoch 25 batch 56/123 loss = 0.0042\n",
      "epoch 25 batch 57/123 loss = 0.0041\n",
      "epoch 25 batch 58/123 loss = 0.0038\n",
      "epoch 25 batch 59/123 loss = 0.0043\n",
      "epoch 25 batch 60/123 loss = 0.0041\n",
      "epoch 25 batch 61/123 loss = 0.0048\n",
      "epoch 25 batch 62/123 loss = 0.0044\n",
      "epoch 25 batch 63/123 loss = 0.0044\n",
      "epoch 25 batch 64/123 loss = 0.0045\n",
      "epoch 25 batch 65/123 loss = 0.0043\n",
      "epoch 25 batch 66/123 loss = 0.0042\n",
      "epoch 25 batch 67/123 loss = 0.0045\n",
      "epoch 25 batch 68/123 loss = 0.0048\n",
      "epoch 25 batch 69/123 loss = 0.0042\n",
      "epoch 25 batch 70/123 loss = 0.0045\n",
      "epoch 25 batch 71/123 loss = 0.0046\n",
      "epoch 25 batch 72/123 loss = 0.0043\n",
      "epoch 25 batch 73/123 loss = 0.0043\n",
      "epoch 25 batch 74/123 loss = 0.0043\n",
      "epoch 25 batch 75/123 loss = 0.0040\n",
      "epoch 25 batch 76/123 loss = 0.0040\n",
      "epoch 25 batch 77/123 loss = 0.0038\n",
      "epoch 25 batch 78/123 loss = 0.0043\n",
      "epoch 25 batch 79/123 loss = 0.0042\n",
      "epoch 25 batch 80/123 loss = 0.0040\n",
      "epoch 25 batch 81/123 loss = 0.0040\n",
      "epoch 25 batch 82/123 loss = 0.0045\n",
      "epoch 25 batch 83/123 loss = 0.0039\n",
      "epoch 25 batch 84/123 loss = 0.0043\n",
      "epoch 25 batch 85/123 loss = 0.0043\n",
      "epoch 25 batch 86/123 loss = 0.0041\n",
      "epoch 25 batch 87/123 loss = 0.0039\n",
      "epoch 25 batch 88/123 loss = 0.0045\n",
      "epoch 25 batch 89/123 loss = 0.0045\n",
      "epoch 25 batch 90/123 loss = 0.0044\n",
      "epoch 25 batch 91/123 loss = 0.0046\n",
      "epoch 25 batch 92/123 loss = 0.0042\n",
      "epoch 25 batch 93/123 loss = 0.0041\n",
      "epoch 25 batch 94/123 loss = 0.0039\n",
      "epoch 25 batch 95/123 loss = 0.0041\n",
      "epoch 25 batch 96/123 loss = 0.0041\n",
      "epoch 25 batch 97/123 loss = 0.0045\n",
      "epoch 25 batch 98/123 loss = 0.0043\n",
      "epoch 25 batch 99/123 loss = 0.0043\n",
      "epoch 25 batch 100/123 loss = 0.0043\n",
      "epoch 25 batch 101/123 loss = 0.0047\n",
      "epoch 25 batch 102/123 loss = 0.0043\n",
      "epoch 25 batch 103/123 loss = 0.0039\n",
      "epoch 25 batch 104/123 loss = 0.0037\n",
      "epoch 25 batch 105/123 loss = 0.0039\n",
      "epoch 25 batch 106/123 loss = 0.0045\n",
      "epoch 25 batch 107/123 loss = 0.0046\n",
      "epoch 25 batch 108/123 loss = 0.0046\n",
      "epoch 25 batch 109/123 loss = 0.0042\n",
      "epoch 25 batch 110/123 loss = 0.0043\n",
      "epoch 25 batch 111/123 loss = 0.0049\n",
      "epoch 25 batch 112/123 loss = 0.0047\n",
      "epoch 25 batch 113/123 loss = 0.0043\n",
      "epoch 25 batch 114/123 loss = 0.0043\n",
      "epoch 25 batch 115/123 loss = 0.0046\n",
      "epoch 25 batch 116/123 loss = 0.0046\n",
      "epoch 25 batch 117/123 loss = 0.0044\n",
      "epoch 25 batch 118/123 loss = 0.0048\n",
      "epoch 25 batch 119/123 loss = 0.0045\n",
      "epoch 25 batch 120/123 loss = 0.0043\n",
      "epoch 25 batch 121/123 loss = 0.0047\n",
      "epoch 25 batch 122/123 loss = 0.0043\n",
      "epoch 25 batch 123/123 average loss = 0.0043 last loss = 0.0041\n",
      "epoch 26 batch 1/123 loss = 0.0041\n",
      "epoch 26 batch 2/123 loss = 0.0042\n",
      "epoch 26 batch 3/123 loss = 0.0042\n",
      "epoch 26 batch 4/123 loss = 0.0041\n",
      "epoch 26 batch 5/123 loss = 0.0045\n",
      "epoch 26 batch 6/123 loss = 0.0042\n",
      "epoch 26 batch 7/123 loss = 0.0040\n",
      "epoch 26 batch 8/123 loss = 0.0039\n",
      "epoch 26 batch 9/123 loss = 0.0041\n",
      "epoch 26 batch 10/123 loss = 0.0045\n",
      "epoch 26 batch 11/123 loss = 0.0042\n",
      "epoch 26 batch 12/123 loss = 0.0043\n",
      "epoch 26 batch 13/123 loss = 0.0039\n",
      "epoch 26 batch 14/123 loss = 0.0045\n",
      "epoch 26 batch 15/123 loss = 0.0044\n",
      "epoch 26 batch 16/123 loss = 0.0043\n",
      "epoch 26 batch 17/123 loss = 0.0041\n",
      "epoch 26 batch 18/123 loss = 0.0042\n",
      "epoch 26 batch 19/123 loss = 0.0041\n",
      "epoch 26 batch 20/123 loss = 0.0041\n",
      "epoch 26 batch 21/123 loss = 0.0043\n",
      "epoch 26 batch 22/123 loss = 0.0045\n",
      "epoch 26 batch 23/123 loss = 0.0042\n",
      "epoch 26 batch 24/123 loss = 0.0039\n",
      "epoch 26 batch 25/123 loss = 0.0042\n",
      "epoch 26 batch 26/123 loss = 0.0040\n",
      "epoch 26 batch 27/123 loss = 0.0037\n",
      "epoch 26 batch 28/123 loss = 0.0041\n",
      "epoch 26 batch 29/123 loss = 0.0040\n",
      "epoch 26 batch 30/123 loss = 0.0043\n",
      "epoch 26 batch 31/123 loss = 0.0040\n",
      "epoch 26 batch 32/123 loss = 0.0041\n",
      "epoch 26 batch 33/123 loss = 0.0042\n",
      "epoch 26 batch 34/123 loss = 0.0040\n",
      "epoch 26 batch 35/123 loss = 0.0042\n",
      "epoch 26 batch 36/123 loss = 0.0041\n",
      "epoch 26 batch 37/123 loss = 0.0038\n",
      "epoch 26 batch 38/123 loss = 0.0040\n",
      "epoch 26 batch 39/123 loss = 0.0047\n",
      "epoch 26 batch 40/123 loss = 0.0043\n",
      "epoch 26 batch 41/123 loss = 0.0040\n",
      "epoch 26 batch 42/123 loss = 0.0039\n",
      "epoch 26 batch 43/123 loss = 0.0043\n",
      "epoch 26 batch 44/123 loss = 0.0041\n",
      "epoch 26 batch 45/123 loss = 0.0044\n",
      "epoch 26 batch 46/123 loss = 0.0045\n",
      "epoch 26 batch 47/123 loss = 0.0043\n",
      "epoch 26 batch 48/123 loss = 0.0037\n",
      "epoch 26 batch 49/123 loss = 0.0045\n",
      "epoch 26 batch 50/123 loss = 0.0041\n",
      "epoch 26 batch 51/123 loss = 0.0044\n",
      "epoch 26 batch 52/123 loss = 0.0041\n",
      "epoch 26 batch 53/123 loss = 0.0045\n",
      "epoch 26 batch 54/123 loss = 0.0042\n",
      "epoch 26 batch 55/123 loss = 0.0039\n",
      "epoch 26 batch 56/123 loss = 0.0039\n",
      "epoch 26 batch 57/123 loss = 0.0038\n",
      "epoch 26 batch 58/123 loss = 0.0045\n",
      "epoch 26 batch 59/123 loss = 0.0041\n",
      "epoch 26 batch 60/123 loss = 0.0043\n",
      "epoch 26 batch 61/123 loss = 0.0042\n",
      "epoch 26 batch 62/123 loss = 0.0040\n",
      "epoch 26 batch 63/123 loss = 0.0040\n",
      "epoch 26 batch 64/123 loss = 0.0038\n",
      "epoch 26 batch 65/123 loss = 0.0040\n",
      "epoch 26 batch 66/123 loss = 0.0039\n",
      "epoch 26 batch 67/123 loss = 0.0039\n",
      "epoch 26 batch 68/123 loss = 0.0042\n",
      "epoch 26 batch 69/123 loss = 0.0038\n",
      "epoch 26 batch 70/123 loss = 0.0043\n",
      "epoch 26 batch 71/123 loss = 0.0042\n",
      "epoch 26 batch 72/123 loss = 0.0041\n",
      "epoch 26 batch 73/123 loss = 0.0043\n",
      "epoch 26 batch 74/123 loss = 0.0042\n",
      "epoch 26 batch 75/123 loss = 0.0042\n",
      "epoch 26 batch 76/123 loss = 0.0039\n",
      "epoch 26 batch 77/123 loss = 0.0042\n",
      "epoch 26 batch 78/123 loss = 0.0041\n",
      "epoch 26 batch 79/123 loss = 0.0041\n",
      "epoch 26 batch 80/123 loss = 0.0046\n",
      "epoch 26 batch 81/123 loss = 0.0046\n",
      "epoch 26 batch 82/123 loss = 0.0041\n",
      "epoch 26 batch 83/123 loss = 0.0039\n",
      "epoch 26 batch 84/123 loss = 0.0040\n",
      "epoch 26 batch 85/123 loss = 0.0044\n",
      "epoch 26 batch 86/123 loss = 0.0044\n",
      "epoch 26 batch 87/123 loss = 0.0039\n",
      "epoch 26 batch 88/123 loss = 0.0040\n",
      "epoch 26 batch 89/123 loss = 0.0042\n",
      "epoch 26 batch 90/123 loss = 0.0045\n",
      "epoch 26 batch 91/123 loss = 0.0054\n",
      "epoch 26 batch 92/123 loss = 0.0040\n",
      "epoch 26 batch 93/123 loss = 0.0048\n",
      "epoch 26 batch 94/123 loss = 0.0051\n",
      "epoch 26 batch 95/123 loss = 0.0042\n",
      "epoch 26 batch 96/123 loss = 0.0039\n",
      "epoch 26 batch 97/123 loss = 0.0046\n",
      "epoch 26 batch 98/123 loss = 0.0044\n",
      "epoch 26 batch 99/123 loss = 0.0041\n",
      "epoch 26 batch 100/123 loss = 0.0044\n",
      "epoch 26 batch 101/123 loss = 0.0047\n",
      "epoch 26 batch 102/123 loss = 0.0040\n",
      "epoch 26 batch 103/123 loss = 0.0037\n",
      "epoch 26 batch 104/123 loss = 0.0043\n",
      "epoch 26 batch 105/123 loss = 0.0045\n",
      "epoch 26 batch 106/123 loss = 0.0041\n",
      "epoch 26 batch 107/123 loss = 0.0039\n",
      "epoch 26 batch 108/123 loss = 0.0043\n",
      "epoch 26 batch 109/123 loss = 0.0046\n",
      "epoch 26 batch 110/123 loss = 0.0043\n",
      "epoch 26 batch 111/123 loss = 0.0044\n",
      "epoch 26 batch 112/123 loss = 0.0039\n",
      "epoch 26 batch 113/123 loss = 0.0045\n",
      "epoch 26 batch 114/123 loss = 0.0040\n",
      "epoch 26 batch 115/123 loss = 0.0036\n",
      "epoch 26 batch 116/123 loss = 0.0046\n",
      "epoch 26 batch 117/123 loss = 0.0044\n",
      "epoch 26 batch 118/123 loss = 0.0041\n",
      "epoch 26 batch 119/123 loss = 0.0042\n",
      "epoch 26 batch 120/123 loss = 0.0045\n",
      "epoch 26 batch 121/123 loss = 0.0041\n",
      "epoch 26 batch 122/123 loss = 0.0046\n",
      "epoch 26 batch 123/123 average loss = 0.0042 last loss = 0.0043\n",
      "epoch 27 batch 1/123 loss = 0.0044\n",
      "epoch 27 batch 2/123 loss = 0.0043\n",
      "epoch 27 batch 3/123 loss = 0.0040\n",
      "epoch 27 batch 4/123 loss = 0.0038\n",
      "epoch 27 batch 5/123 loss = 0.0043\n",
      "epoch 27 batch 6/123 loss = 0.0042\n",
      "epoch 27 batch 7/123 loss = 0.0042\n",
      "epoch 27 batch 8/123 loss = 0.0040\n",
      "epoch 27 batch 9/123 loss = 0.0042\n",
      "epoch 27 batch 10/123 loss = 0.0038\n",
      "epoch 27 batch 11/123 loss = 0.0035\n",
      "epoch 27 batch 12/123 loss = 0.0040\n",
      "epoch 27 batch 13/123 loss = 0.0040\n",
      "epoch 27 batch 14/123 loss = 0.0044\n",
      "epoch 27 batch 15/123 loss = 0.0041\n",
      "epoch 27 batch 16/123 loss = 0.0040\n",
      "epoch 27 batch 17/123 loss = 0.0042\n",
      "epoch 27 batch 18/123 loss = 0.0039\n",
      "epoch 27 batch 19/123 loss = 0.0043\n",
      "epoch 27 batch 20/123 loss = 0.0042\n",
      "epoch 27 batch 21/123 loss = 0.0044\n",
      "epoch 27 batch 22/123 loss = 0.0038\n",
      "epoch 27 batch 23/123 loss = 0.0040\n",
      "epoch 27 batch 24/123 loss = 0.0042\n",
      "epoch 27 batch 25/123 loss = 0.0039\n",
      "epoch 27 batch 26/123 loss = 0.0042\n",
      "epoch 27 batch 27/123 loss = 0.0041\n",
      "epoch 27 batch 28/123 loss = 0.0043\n",
      "epoch 27 batch 29/123 loss = 0.0040\n",
      "epoch 27 batch 30/123 loss = 0.0038\n",
      "epoch 27 batch 31/123 loss = 0.0041\n",
      "epoch 27 batch 32/123 loss = 0.0043\n",
      "epoch 27 batch 33/123 loss = 0.0040\n",
      "epoch 27 batch 34/123 loss = 0.0038\n",
      "epoch 27 batch 35/123 loss = 0.0040\n",
      "epoch 27 batch 36/123 loss = 0.0041\n",
      "epoch 27 batch 37/123 loss = 0.0039\n",
      "epoch 27 batch 38/123 loss = 0.0038\n",
      "epoch 27 batch 39/123 loss = 0.0042\n",
      "epoch 27 batch 40/123 loss = 0.0040\n",
      "epoch 27 batch 41/123 loss = 0.0043\n",
      "epoch 27 batch 42/123 loss = 0.0042\n",
      "epoch 27 batch 43/123 loss = 0.0048\n",
      "epoch 27 batch 44/123 loss = 0.0049\n",
      "epoch 27 batch 45/123 loss = 0.0041\n",
      "epoch 27 batch 46/123 loss = 0.0044\n",
      "epoch 27 batch 47/123 loss = 0.0039\n",
      "epoch 27 batch 48/123 loss = 0.0041\n",
      "epoch 27 batch 49/123 loss = 0.0037\n",
      "epoch 27 batch 50/123 loss = 0.0038\n",
      "epoch 27 batch 51/123 loss = 0.0038\n",
      "epoch 27 batch 52/123 loss = 0.0049\n",
      "epoch 27 batch 53/123 loss = 0.0041\n",
      "epoch 27 batch 54/123 loss = 0.0042\n",
      "epoch 27 batch 55/123 loss = 0.0041\n",
      "epoch 27 batch 56/123 loss = 0.0039\n",
      "epoch 27 batch 57/123 loss = 0.0046\n",
      "epoch 27 batch 58/123 loss = 0.0046\n",
      "epoch 27 batch 59/123 loss = 0.0044\n",
      "epoch 27 batch 60/123 loss = 0.0049\n",
      "epoch 27 batch 61/123 loss = 0.0042\n",
      "epoch 27 batch 62/123 loss = 0.0042\n",
      "epoch 27 batch 63/123 loss = 0.0039\n",
      "epoch 27 batch 64/123 loss = 0.0052\n",
      "epoch 27 batch 65/123 loss = 0.0044\n",
      "epoch 27 batch 66/123 loss = 0.0045\n",
      "epoch 27 batch 67/123 loss = 0.0044\n",
      "epoch 27 batch 68/123 loss = 0.0041\n",
      "epoch 27 batch 69/123 loss = 0.0040\n",
      "epoch 27 batch 70/123 loss = 0.0036\n",
      "epoch 27 batch 71/123 loss = 0.0046\n",
      "epoch 27 batch 72/123 loss = 0.0041\n",
      "epoch 27 batch 73/123 loss = 0.0043\n",
      "epoch 27 batch 74/123 loss = 0.0037\n",
      "epoch 27 batch 75/123 loss = 0.0040\n",
      "epoch 27 batch 76/123 loss = 0.0044\n",
      "epoch 27 batch 77/123 loss = 0.0043\n",
      "epoch 27 batch 78/123 loss = 0.0042\n",
      "epoch 27 batch 79/123 loss = 0.0039\n",
      "epoch 27 batch 80/123 loss = 0.0038\n",
      "epoch 27 batch 81/123 loss = 0.0037\n",
      "epoch 27 batch 82/123 loss = 0.0041\n",
      "epoch 27 batch 83/123 loss = 0.0043\n",
      "epoch 27 batch 84/123 loss = 0.0040\n",
      "epoch 27 batch 85/123 loss = 0.0038\n",
      "epoch 27 batch 86/123 loss = 0.0039\n",
      "epoch 27 batch 87/123 loss = 0.0042\n",
      "epoch 27 batch 88/123 loss = 0.0039\n",
      "epoch 27 batch 89/123 loss = 0.0041\n",
      "epoch 27 batch 90/123 loss = 0.0040\n",
      "epoch 27 batch 91/123 loss = 0.0045\n",
      "epoch 27 batch 92/123 loss = 0.0043\n",
      "epoch 27 batch 93/123 loss = 0.0038\n",
      "epoch 27 batch 94/123 loss = 0.0039\n",
      "epoch 27 batch 95/123 loss = 0.0041\n",
      "epoch 27 batch 96/123 loss = 0.0042\n",
      "epoch 27 batch 97/123 loss = 0.0043\n",
      "epoch 27 batch 98/123 loss = 0.0041\n",
      "epoch 27 batch 99/123 loss = 0.0041\n",
      "epoch 27 batch 100/123 loss = 0.0044\n",
      "epoch 27 batch 101/123 loss = 0.0043\n",
      "epoch 27 batch 102/123 loss = 0.0042\n",
      "epoch 27 batch 103/123 loss = 0.0044\n",
      "epoch 27 batch 104/123 loss = 0.0043\n",
      "epoch 27 batch 105/123 loss = 0.0049\n",
      "epoch 27 batch 106/123 loss = 0.0041\n",
      "epoch 27 batch 107/123 loss = 0.0045\n",
      "epoch 27 batch 108/123 loss = 0.0040\n",
      "epoch 27 batch 109/123 loss = 0.0041\n",
      "epoch 27 batch 110/123 loss = 0.0042\n",
      "epoch 27 batch 111/123 loss = 0.0043\n",
      "epoch 27 batch 112/123 loss = 0.0039\n",
      "epoch 27 batch 113/123 loss = 0.0042\n",
      "epoch 27 batch 114/123 loss = 0.0044\n",
      "epoch 27 batch 115/123 loss = 0.0052\n",
      "epoch 27 batch 116/123 loss = 0.0045\n",
      "epoch 27 batch 117/123 loss = 0.0051\n",
      "epoch 27 batch 118/123 loss = 0.0043\n",
      "epoch 27 batch 119/123 loss = 0.0045\n",
      "epoch 27 batch 120/123 loss = 0.0046\n",
      "epoch 27 batch 121/123 loss = 0.0044\n",
      "epoch 27 batch 122/123 loss = 0.0042\n",
      "epoch 27 batch 123/123 average loss = 0.0042 last loss = 0.0044\n",
      "epoch 28 batch 1/123 loss = 0.0043\n",
      "epoch 28 batch 2/123 loss = 0.0040\n",
      "epoch 28 batch 3/123 loss = 0.0041\n",
      "epoch 28 batch 4/123 loss = 0.0042\n",
      "epoch 28 batch 5/123 loss = 0.0039\n",
      "epoch 28 batch 6/123 loss = 0.0040\n",
      "epoch 28 batch 7/123 loss = 0.0039\n",
      "epoch 28 batch 8/123 loss = 0.0036\n",
      "epoch 28 batch 9/123 loss = 0.0038\n",
      "epoch 28 batch 10/123 loss = 0.0038\n",
      "epoch 28 batch 11/123 loss = 0.0040\n",
      "epoch 28 batch 12/123 loss = 0.0036\n",
      "epoch 28 batch 13/123 loss = 0.0044\n",
      "epoch 28 batch 14/123 loss = 0.0040\n",
      "epoch 28 batch 15/123 loss = 0.0038\n",
      "epoch 28 batch 16/123 loss = 0.0041\n",
      "epoch 28 batch 17/123 loss = 0.0039\n",
      "epoch 28 batch 18/123 loss = 0.0038\n",
      "epoch 28 batch 19/123 loss = 0.0039\n",
      "epoch 28 batch 20/123 loss = 0.0036\n",
      "epoch 28 batch 21/123 loss = 0.0038\n",
      "epoch 28 batch 22/123 loss = 0.0039\n",
      "epoch 28 batch 23/123 loss = 0.0038\n",
      "epoch 28 batch 24/123 loss = 0.0039\n",
      "epoch 28 batch 25/123 loss = 0.0043\n",
      "epoch 28 batch 26/123 loss = 0.0040\n",
      "epoch 28 batch 27/123 loss = 0.0041\n",
      "epoch 28 batch 28/123 loss = 0.0038\n",
      "epoch 28 batch 29/123 loss = 0.0038\n",
      "epoch 28 batch 30/123 loss = 0.0037\n",
      "epoch 28 batch 31/123 loss = 0.0035\n",
      "epoch 28 batch 32/123 loss = 0.0039\n",
      "epoch 28 batch 33/123 loss = 0.0034\n",
      "epoch 28 batch 34/123 loss = 0.0034\n",
      "epoch 28 batch 35/123 loss = 0.0040\n",
      "epoch 28 batch 36/123 loss = 0.0043\n",
      "epoch 28 batch 37/123 loss = 0.0038\n",
      "epoch 28 batch 38/123 loss = 0.0037\n",
      "epoch 28 batch 39/123 loss = 0.0037\n",
      "epoch 28 batch 40/123 loss = 0.0044\n",
      "epoch 28 batch 41/123 loss = 0.0041\n",
      "epoch 28 batch 42/123 loss = 0.0036\n",
      "epoch 28 batch 43/123 loss = 0.0042\n",
      "epoch 28 batch 44/123 loss = 0.0038\n",
      "epoch 28 batch 45/123 loss = 0.0044\n",
      "epoch 28 batch 46/123 loss = 0.0042\n",
      "epoch 28 batch 47/123 loss = 0.0041\n",
      "epoch 28 batch 48/123 loss = 0.0040\n",
      "epoch 28 batch 49/123 loss = 0.0040\n",
      "epoch 28 batch 50/123 loss = 0.0042\n",
      "epoch 28 batch 51/123 loss = 0.0042\n",
      "epoch 28 batch 52/123 loss = 0.0042\n",
      "epoch 28 batch 53/123 loss = 0.0056\n",
      "epoch 28 batch 54/123 loss = 0.0049\n",
      "epoch 28 batch 55/123 loss = 0.0042\n",
      "epoch 28 batch 56/123 loss = 0.0039\n",
      "epoch 28 batch 57/123 loss = 0.0043\n",
      "epoch 28 batch 58/123 loss = 0.0041\n",
      "epoch 28 batch 59/123 loss = 0.0040\n",
      "epoch 28 batch 60/123 loss = 0.0045\n",
      "epoch 28 batch 61/123 loss = 0.0044\n",
      "epoch 28 batch 62/123 loss = 0.0041\n",
      "epoch 28 batch 63/123 loss = 0.0043\n",
      "epoch 28 batch 64/123 loss = 0.0038\n",
      "epoch 28 batch 65/123 loss = 0.0037\n",
      "epoch 28 batch 66/123 loss = 0.0044\n",
      "epoch 28 batch 67/123 loss = 0.0039\n",
      "epoch 28 batch 68/123 loss = 0.0043\n",
      "epoch 28 batch 69/123 loss = 0.0044\n",
      "epoch 28 batch 70/123 loss = 0.0047\n",
      "epoch 28 batch 71/123 loss = 0.0042\n",
      "epoch 28 batch 72/123 loss = 0.0048\n",
      "epoch 28 batch 73/123 loss = 0.0043\n",
      "epoch 28 batch 74/123 loss = 0.0044\n",
      "epoch 28 batch 75/123 loss = 0.0043\n",
      "epoch 28 batch 76/123 loss = 0.0043\n",
      "epoch 28 batch 77/123 loss = 0.0041\n",
      "epoch 28 batch 78/123 loss = 0.0040\n",
      "epoch 28 batch 79/123 loss = 0.0040\n",
      "epoch 28 batch 80/123 loss = 0.0043\n",
      "epoch 28 batch 81/123 loss = 0.0044\n",
      "epoch 28 batch 82/123 loss = 0.0044\n",
      "epoch 28 batch 83/123 loss = 0.0042\n",
      "epoch 28 batch 84/123 loss = 0.0043\n",
      "epoch 28 batch 85/123 loss = 0.0039\n",
      "epoch 28 batch 86/123 loss = 0.0040\n",
      "epoch 28 batch 87/123 loss = 0.0040\n",
      "epoch 28 batch 88/123 loss = 0.0048\n",
      "epoch 28 batch 89/123 loss = 0.0045\n",
      "epoch 28 batch 90/123 loss = 0.0041\n",
      "epoch 28 batch 91/123 loss = 0.0040\n",
      "epoch 28 batch 92/123 loss = 0.0043\n",
      "epoch 28 batch 93/123 loss = 0.0040\n",
      "epoch 28 batch 94/123 loss = 0.0040\n",
      "epoch 28 batch 95/123 loss = 0.0044\n",
      "epoch 28 batch 96/123 loss = 0.0048\n",
      "epoch 28 batch 97/123 loss = 0.0039\n",
      "epoch 28 batch 98/123 loss = 0.0040\n",
      "epoch 28 batch 99/123 loss = 0.0043\n",
      "epoch 28 batch 100/123 loss = 0.0046\n",
      "epoch 28 batch 101/123 loss = 0.0045\n",
      "epoch 28 batch 102/123 loss = 0.0042\n",
      "epoch 28 batch 103/123 loss = 0.0041\n",
      "epoch 28 batch 104/123 loss = 0.0047\n",
      "epoch 28 batch 105/123 loss = 0.0050\n",
      "epoch 28 batch 106/123 loss = 0.0042\n",
      "epoch 28 batch 107/123 loss = 0.0039\n",
      "epoch 28 batch 108/123 loss = 0.0045\n",
      "epoch 28 batch 109/123 loss = 0.0044\n",
      "epoch 28 batch 110/123 loss = 0.0042\n",
      "epoch 28 batch 111/123 loss = 0.0041\n",
      "epoch 28 batch 112/123 loss = 0.0049\n",
      "epoch 28 batch 113/123 loss = 0.0047\n",
      "epoch 28 batch 114/123 loss = 0.0043\n",
      "epoch 28 batch 115/123 loss = 0.0042\n",
      "epoch 28 batch 116/123 loss = 0.0049\n",
      "epoch 28 batch 117/123 loss = 0.0047\n",
      "epoch 28 batch 118/123 loss = 0.0053\n",
      "epoch 28 batch 119/123 loss = 0.0045\n",
      "epoch 28 batch 120/123 loss = 0.0041\n",
      "epoch 28 batch 121/123 loss = 0.0045\n",
      "epoch 28 batch 122/123 loss = 0.0041\n",
      "epoch 28 batch 123/123 average loss = 0.0042 last loss = 0.0044\n",
      "epoch 29 batch 1/123 loss = 0.0042\n",
      "epoch 29 batch 2/123 loss = 0.0042\n",
      "epoch 29 batch 3/123 loss = 0.0044\n",
      "epoch 29 batch 4/123 loss = 0.0045\n",
      "epoch 29 batch 5/123 loss = 0.0044\n",
      "epoch 29 batch 6/123 loss = 0.0036\n",
      "epoch 29 batch 7/123 loss = 0.0038\n",
      "epoch 29 batch 8/123 loss = 0.0042\n",
      "epoch 29 batch 9/123 loss = 0.0038\n",
      "epoch 29 batch 10/123 loss = 0.0038\n",
      "epoch 29 batch 11/123 loss = 0.0045\n",
      "epoch 29 batch 12/123 loss = 0.0040\n",
      "epoch 29 batch 13/123 loss = 0.0041\n",
      "epoch 29 batch 14/123 loss = 0.0039\n",
      "epoch 29 batch 15/123 loss = 0.0041\n",
      "epoch 29 batch 16/123 loss = 0.0040\n",
      "epoch 29 batch 17/123 loss = 0.0039\n",
      "epoch 29 batch 18/123 loss = 0.0039\n",
      "epoch 29 batch 19/123 loss = 0.0047\n",
      "epoch 29 batch 20/123 loss = 0.0042\n",
      "epoch 29 batch 21/123 loss = 0.0038\n",
      "epoch 29 batch 22/123 loss = 0.0045\n",
      "epoch 29 batch 23/123 loss = 0.0046\n",
      "epoch 29 batch 24/123 loss = 0.0048\n",
      "epoch 29 batch 25/123 loss = 0.0043\n",
      "epoch 29 batch 26/123 loss = 0.0045\n",
      "epoch 29 batch 27/123 loss = 0.0043\n",
      "epoch 29 batch 28/123 loss = 0.0043\n",
      "epoch 29 batch 29/123 loss = 0.0044\n",
      "epoch 29 batch 30/123 loss = 0.0040\n",
      "epoch 29 batch 31/123 loss = 0.0041\n",
      "epoch 29 batch 32/123 loss = 0.0040\n",
      "epoch 29 batch 33/123 loss = 0.0037\n",
      "epoch 29 batch 34/123 loss = 0.0041\n",
      "epoch 29 batch 35/123 loss = 0.0039\n",
      "epoch 29 batch 36/123 loss = 0.0039\n",
      "epoch 29 batch 37/123 loss = 0.0039\n",
      "epoch 29 batch 38/123 loss = 0.0039\n",
      "epoch 29 batch 39/123 loss = 0.0040\n",
      "epoch 29 batch 40/123 loss = 0.0038\n",
      "epoch 29 batch 41/123 loss = 0.0034\n",
      "epoch 29 batch 42/123 loss = 0.0038\n",
      "epoch 29 batch 43/123 loss = 0.0042\n",
      "epoch 29 batch 44/123 loss = 0.0042\n",
      "epoch 29 batch 45/123 loss = 0.0042\n",
      "epoch 29 batch 46/123 loss = 0.0042\n",
      "epoch 29 batch 47/123 loss = 0.0037\n",
      "epoch 29 batch 48/123 loss = 0.0040\n",
      "epoch 29 batch 49/123 loss = 0.0036\n",
      "epoch 29 batch 50/123 loss = 0.0040\n",
      "epoch 29 batch 51/123 loss = 0.0038\n",
      "epoch 29 batch 52/123 loss = 0.0038\n",
      "epoch 29 batch 53/123 loss = 0.0044\n",
      "epoch 29 batch 54/123 loss = 0.0038\n",
      "epoch 29 batch 55/123 loss = 0.0038\n",
      "epoch 29 batch 56/123 loss = 0.0045\n",
      "epoch 29 batch 57/123 loss = 0.0039\n",
      "epoch 29 batch 58/123 loss = 0.0042\n",
      "epoch 29 batch 59/123 loss = 0.0039\n",
      "epoch 29 batch 60/123 loss = 0.0040\n",
      "epoch 29 batch 61/123 loss = 0.0042\n",
      "epoch 29 batch 62/123 loss = 0.0044\n",
      "epoch 29 batch 63/123 loss = 0.0037\n",
      "epoch 29 batch 64/123 loss = 0.0043\n",
      "epoch 29 batch 65/123 loss = 0.0041\n",
      "epoch 29 batch 66/123 loss = 0.0043\n",
      "epoch 29 batch 67/123 loss = 0.0043\n",
      "epoch 29 batch 68/123 loss = 0.0045\n",
      "epoch 29 batch 69/123 loss = 0.0042\n",
      "epoch 29 batch 70/123 loss = 0.0040\n",
      "epoch 29 batch 71/123 loss = 0.0041\n",
      "epoch 29 batch 72/123 loss = 0.0043\n",
      "epoch 29 batch 73/123 loss = 0.0039\n",
      "epoch 29 batch 74/123 loss = 0.0038\n",
      "epoch 29 batch 75/123 loss = 0.0035\n",
      "epoch 29 batch 76/123 loss = 0.0040\n",
      "epoch 29 batch 77/123 loss = 0.0038\n",
      "epoch 29 batch 78/123 loss = 0.0036\n",
      "epoch 29 batch 79/123 loss = 0.0041\n",
      "epoch 29 batch 80/123 loss = 0.0041\n",
      "epoch 29 batch 81/123 loss = 0.0039\n",
      "epoch 29 batch 82/123 loss = 0.0040\n",
      "epoch 29 batch 83/123 loss = 0.0041\n",
      "epoch 29 batch 84/123 loss = 0.0037\n",
      "epoch 29 batch 85/123 loss = 0.0041\n",
      "epoch 29 batch 86/123 loss = 0.0038\n",
      "epoch 29 batch 87/123 loss = 0.0037\n",
      "epoch 29 batch 88/123 loss = 0.0039\n",
      "epoch 29 batch 89/123 loss = 0.0039\n",
      "epoch 29 batch 90/123 loss = 0.0040\n",
      "epoch 29 batch 91/123 loss = 0.0042\n",
      "epoch 29 batch 92/123 loss = 0.0038\n",
      "epoch 29 batch 93/123 loss = 0.0041\n",
      "epoch 29 batch 94/123 loss = 0.0038\n",
      "epoch 29 batch 95/123 loss = 0.0037\n",
      "epoch 29 batch 96/123 loss = 0.0041\n",
      "epoch 29 batch 97/123 loss = 0.0038\n",
      "epoch 29 batch 98/123 loss = 0.0038\n",
      "epoch 29 batch 99/123 loss = 0.0041\n",
      "epoch 29 batch 100/123 loss = 0.0035\n",
      "epoch 29 batch 101/123 loss = 0.0033\n",
      "epoch 29 batch 102/123 loss = 0.0035\n",
      "epoch 29 batch 103/123 loss = 0.0037\n",
      "epoch 29 batch 104/123 loss = 0.0039\n",
      "epoch 29 batch 105/123 loss = 0.0037\n",
      "epoch 29 batch 106/123 loss = 0.0040\n",
      "epoch 29 batch 107/123 loss = 0.0043\n",
      "epoch 29 batch 108/123 loss = 0.0043\n",
      "epoch 29 batch 109/123 loss = 0.0038\n",
      "epoch 29 batch 110/123 loss = 0.0037\n",
      "epoch 29 batch 111/123 loss = 0.0037\n",
      "epoch 29 batch 112/123 loss = 0.0039\n",
      "epoch 29 batch 113/123 loss = 0.0047\n",
      "epoch 29 batch 114/123 loss = 0.0050\n",
      "epoch 29 batch 115/123 loss = 0.0043\n",
      "epoch 29 batch 116/123 loss = 0.0043\n",
      "epoch 29 batch 117/123 loss = 0.0038\n",
      "epoch 29 batch 118/123 loss = 0.0045\n",
      "epoch 29 batch 119/123 loss = 0.0041\n",
      "epoch 29 batch 120/123 loss = 0.0041\n",
      "epoch 29 batch 121/123 loss = 0.0040\n",
      "epoch 29 batch 122/123 loss = 0.0043\n",
      "epoch 29 batch 123/123 average loss = 0.0040 last loss = 0.0038\n",
      "epoch 30 batch 1/123 loss = 0.0036\n",
      "epoch 30 batch 2/123 loss = 0.0041\n",
      "epoch 30 batch 3/123 loss = 0.0037\n",
      "epoch 30 batch 4/123 loss = 0.0038\n",
      "epoch 30 batch 5/123 loss = 0.0040\n",
      "epoch 30 batch 6/123 loss = 0.0038\n",
      "epoch 30 batch 7/123 loss = 0.0045\n",
      "epoch 30 batch 8/123 loss = 0.0044\n",
      "epoch 30 batch 9/123 loss = 0.0043\n",
      "epoch 30 batch 10/123 loss = 0.0035\n",
      "epoch 30 batch 11/123 loss = 0.0039\n",
      "epoch 30 batch 12/123 loss = 0.0036\n",
      "epoch 30 batch 13/123 loss = 0.0037\n",
      "epoch 30 batch 14/123 loss = 0.0040\n",
      "epoch 30 batch 15/123 loss = 0.0038\n",
      "epoch 30 batch 16/123 loss = 0.0036\n",
      "epoch 30 batch 17/123 loss = 0.0035\n",
      "epoch 30 batch 18/123 loss = 0.0039\n",
      "epoch 30 batch 19/123 loss = 0.0034\n",
      "epoch 30 batch 20/123 loss = 0.0042\n",
      "epoch 30 batch 21/123 loss = 0.0041\n",
      "epoch 30 batch 22/123 loss = 0.0036\n",
      "epoch 30 batch 23/123 loss = 0.0041\n",
      "epoch 30 batch 24/123 loss = 0.0037\n",
      "epoch 30 batch 25/123 loss = 0.0038\n",
      "epoch 30 batch 26/123 loss = 0.0039\n",
      "epoch 30 batch 27/123 loss = 0.0033\n",
      "epoch 30 batch 28/123 loss = 0.0037\n",
      "epoch 30 batch 29/123 loss = 0.0041\n",
      "epoch 30 batch 30/123 loss = 0.0037\n",
      "epoch 30 batch 31/123 loss = 0.0043\n",
      "epoch 30 batch 32/123 loss = 0.0042\n",
      "epoch 30 batch 33/123 loss = 0.0038\n",
      "epoch 30 batch 34/123 loss = 0.0037\n",
      "epoch 30 batch 35/123 loss = 0.0036\n",
      "epoch 30 batch 36/123 loss = 0.0042\n",
      "epoch 30 batch 37/123 loss = 0.0041\n",
      "epoch 30 batch 38/123 loss = 0.0040\n",
      "epoch 30 batch 39/123 loss = 0.0043\n",
      "epoch 30 batch 40/123 loss = 0.0041\n",
      "epoch 30 batch 41/123 loss = 0.0036\n",
      "epoch 30 batch 42/123 loss = 0.0037\n",
      "epoch 30 batch 43/123 loss = 0.0035\n",
      "epoch 30 batch 44/123 loss = 0.0042\n",
      "epoch 30 batch 45/123 loss = 0.0038\n",
      "epoch 30 batch 46/123 loss = 0.0033\n",
      "epoch 30 batch 47/123 loss = 0.0035\n",
      "epoch 30 batch 48/123 loss = 0.0040\n",
      "epoch 30 batch 49/123 loss = 0.0037\n",
      "epoch 30 batch 50/123 loss = 0.0035\n",
      "epoch 30 batch 51/123 loss = 0.0040\n",
      "epoch 30 batch 52/123 loss = 0.0040\n",
      "epoch 30 batch 53/123 loss = 0.0041\n",
      "epoch 30 batch 54/123 loss = 0.0038\n",
      "epoch 30 batch 55/123 loss = 0.0037\n",
      "epoch 30 batch 56/123 loss = 0.0042\n",
      "epoch 30 batch 57/123 loss = 0.0039\n",
      "epoch 30 batch 58/123 loss = 0.0045\n",
      "epoch 30 batch 59/123 loss = 0.0048\n",
      "epoch 30 batch 60/123 loss = 0.0043\n",
      "epoch 30 batch 61/123 loss = 0.0041\n",
      "epoch 30 batch 62/123 loss = 0.0041\n",
      "epoch 30 batch 63/123 loss = 0.0040\n",
      "epoch 30 batch 64/123 loss = 0.0036\n",
      "epoch 30 batch 65/123 loss = 0.0041\n",
      "epoch 30 batch 66/123 loss = 0.0038\n",
      "epoch 30 batch 67/123 loss = 0.0035\n",
      "epoch 30 batch 68/123 loss = 0.0038\n",
      "epoch 30 batch 69/123 loss = 0.0036\n",
      "epoch 30 batch 70/123 loss = 0.0042\n",
      "epoch 30 batch 71/123 loss = 0.0040\n",
      "epoch 30 batch 72/123 loss = 0.0037\n",
      "epoch 30 batch 73/123 loss = 0.0041\n",
      "epoch 30 batch 74/123 loss = 0.0037\n",
      "epoch 30 batch 75/123 loss = 0.0038\n",
      "epoch 30 batch 76/123 loss = 0.0041\n",
      "epoch 30 batch 77/123 loss = 0.0041\n",
      "epoch 30 batch 78/123 loss = 0.0038\n",
      "epoch 30 batch 79/123 loss = 0.0038\n",
      "epoch 30 batch 80/123 loss = 0.0037\n",
      "epoch 30 batch 81/123 loss = 0.0036\n",
      "epoch 30 batch 82/123 loss = 0.0036\n",
      "epoch 30 batch 83/123 loss = 0.0037\n",
      "epoch 30 batch 84/123 loss = 0.0036\n",
      "epoch 30 batch 85/123 loss = 0.0036\n",
      "epoch 30 batch 86/123 loss = 0.0040\n",
      "epoch 30 batch 87/123 loss = 0.0037\n",
      "epoch 30 batch 88/123 loss = 0.0039\n",
      "epoch 30 batch 89/123 loss = 0.0037\n",
      "epoch 30 batch 90/123 loss = 0.0038\n",
      "epoch 30 batch 91/123 loss = 0.0040\n",
      "epoch 30 batch 92/123 loss = 0.0041\n",
      "epoch 30 batch 93/123 loss = 0.0042\n",
      "epoch 30 batch 94/123 loss = 0.0041\n",
      "epoch 30 batch 95/123 loss = 0.0039\n",
      "epoch 30 batch 96/123 loss = 0.0044\n",
      "epoch 30 batch 97/123 loss = 0.0038\n",
      "epoch 30 batch 98/123 loss = 0.0039\n",
      "epoch 30 batch 99/123 loss = 0.0043\n",
      "epoch 30 batch 100/123 loss = 0.0042\n",
      "epoch 30 batch 101/123 loss = 0.0041\n",
      "epoch 30 batch 102/123 loss = 0.0039\n",
      "epoch 30 batch 103/123 loss = 0.0038\n",
      "epoch 30 batch 104/123 loss = 0.0037\n",
      "epoch 30 batch 105/123 loss = 0.0038\n",
      "epoch 30 batch 106/123 loss = 0.0034\n",
      "epoch 30 batch 107/123 loss = 0.0038\n",
      "epoch 30 batch 108/123 loss = 0.0044\n",
      "epoch 30 batch 109/123 loss = 0.0040\n",
      "epoch 30 batch 110/123 loss = 0.0040\n",
      "epoch 30 batch 111/123 loss = 0.0038\n",
      "epoch 30 batch 112/123 loss = 0.0039\n",
      "epoch 30 batch 113/123 loss = 0.0037\n",
      "epoch 30 batch 114/123 loss = 0.0039\n",
      "epoch 30 batch 115/123 loss = 0.0037\n",
      "epoch 30 batch 116/123 loss = 0.0036\n",
      "epoch 30 batch 117/123 loss = 0.0040\n",
      "epoch 30 batch 118/123 loss = 0.0039\n",
      "epoch 30 batch 119/123 loss = 0.0037\n",
      "epoch 30 batch 120/123 loss = 0.0037\n",
      "epoch 30 batch 121/123 loss = 0.0040\n",
      "epoch 30 batch 122/123 loss = 0.0039\n",
      "epoch 30 batch 123/123 average loss = 0.0039 last loss = 0.0038\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    current_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(current_device)\n",
    "    data_loader = load_data('/kaggle/working/datasets/cityscapes.pth')\n",
    "    unet = UNet().to(current_device)\n",
    "    mse_loss = nn.MSELoss().to(current_device)\n",
    "    optimizer_adam = optim.Adam(unet.parameters(), lr=params.get(\"lr\"), betas=params.get(\"betas\"))\n",
    "    not_retrain = False\n",
    "    if not_retrain:\n",
    "        unet = trainer(unet, data_loader, optimizer_adam, mse_loss, params.get('epoch'), device=current_device)\n",
    "    else:\n",
    "        model_params = torch.load('../input/cityscapes-38epochpth/u_net_seg_38.pth')\n",
    "#         model = UNet().to(current_device)\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=params.get('lr'), betas=params.get(\"betas\"))\n",
    "#         mse_loss = nn.MSELoss().to(current_device)\n",
    "        unet.load_state_dict(model_params['model_state_dict'])\n",
    "        optimizer_adam.load_state_dict(model_params['optimizer_state_dict'])\n",
    "        epoch = model_params['epoch'] + 1\n",
    "#         loss = model_params['loss']\n",
    "        unet.train()\n",
    "        unet = trainer(unet, data_loader, optimizer_adam, mse_loss, 30, current_epoch=epoch, device=current_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1724.113447,
   "end_time": "2022-05-28T15:06:34.749799",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-28T14:37:50.636352",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
