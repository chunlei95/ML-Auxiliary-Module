{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784c85e3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-26T09:57:40.449288Z",
     "iopub.status.busy": "2022-05-26T09:57:40.448853Z",
     "iopub.status.idle": "2022-05-26T09:57:40.459861Z",
     "shell.execute_reply": "2022-05-26T09:57:40.459153Z"
    },
    "papermill": {
     "duration": 0.027097,
     "end_time": "2022-05-26T09:57:40.461788",
     "exception": false,
     "start_time": "2022-05-26T09:57:40.434691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#os.remove('/kaggle/working/datasets/cityscapes.pth')\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#       print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830623d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T09:57:40.484200Z",
     "iopub.status.busy": "2022-05-26T09:57:40.483950Z",
     "iopub.status.idle": "2022-05-26T09:57:42.574973Z",
     "shell.execute_reply": "2022-05-26T09:57:42.574251Z"
    },
    "papermill": {
     "duration": 2.104632,
     "end_time": "2022-05-26T09:57:42.576973",
     "exception": false,
     "start_time": "2022-05-26T09:57:40.472341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as functional\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793afeaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T09:57:42.597689Z",
     "iopub.status.busy": "2022-05-26T09:57:42.597476Z",
     "iopub.status.idle": "2022-05-26T09:57:43.050688Z",
     "shell.execute_reply": "2022-05-26T09:57:43.049938Z"
    },
    "papermill": {
     "duration": 0.466005,
     "end_time": "2022-05-26T09:57:43.052933",
     "exception": false,
     "start_time": "2022-05-26T09:57:42.586928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = glob('/kaggle/input/cityscapes-image-pairs/cityscapes_data/train/*')\n",
    "valid_path = glob('/kaggle/input/cityscapes-image-pairs/cityscapes_data/val/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79417e70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T09:57:43.075663Z",
     "iopub.status.busy": "2022-05-26T09:57:43.075446Z",
     "iopub.status.idle": "2022-05-26T09:57:43.082324Z",
     "shell.execute_reply": "2022-05-26T09:57:43.081659Z"
    },
    "papermill": {
     "duration": 0.020521,
     "end_time": "2022-05-26T09:57:43.083922",
     "exception": false,
     "start_time": "2022-05-26T09:57:43.063401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cityscapes(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        super(Cityscapes, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        #self.datasets = np.array(data)\n",
    "        #self.images, self.targets = np.array_split(self.datasets, 2, axis=2)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image_pair = plt.imread(self.data_path[item])\n",
    "        image, target = image_pair[:, :int(image_pair.shape[1] / 2)], image_pair[:, int(image_pair.shape[1] / 2):]\n",
    "        #image = self.images[item]\n",
    "        #target = self.targets[item]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            target = self.transform(target)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2021d8eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T09:57:43.104874Z",
     "iopub.status.busy": "2022-05-26T09:57:43.104688Z",
     "iopub.status.idle": "2022-05-26T09:57:43.108488Z",
     "shell.execute_reply": "2022-05-26T09:57:43.107869Z"
    },
    "papermill": {
     "duration": 0.016174,
     "end_time": "2022-05-26T09:57:43.110201",
     "exception": false,
     "start_time": "2022-05-26T09:57:43.094027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_directory(path):\n",
    "    image_list = []\n",
    "    for filename in os.listdir(path):\n",
    "        image = plt.imread(path + '/' + filename)\n",
    "        image_list.append(image)\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0261c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T09:57:43.131011Z",
     "iopub.status.busy": "2022-05-26T09:57:43.130599Z",
     "iopub.status.idle": "2022-05-26T09:57:43.135516Z",
     "shell.execute_reply": "2022-05-26T09:57:43.134878Z"
    },
    "papermill": {
     "duration": 0.017297,
     "end_time": "2022-05-26T09:57:43.137150",
     "exception": false,
     "start_time": "2022-05-26T09:57:43.119853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "#     if path is not None and os.path.exists(path):\n",
    "#         return torch.load(path)['train_loader']\n",
    "#     else:\n",
    "        transform_list = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        #cityscapes = read_directory('/kaggle/input/cityscapes-image-pairs/cityscapes_data/train')\n",
    "        train_dataset = Cityscapes(train_path, transform=transform_list)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, drop_last=True)\n",
    "#         path = path if path is not None else '/kaggle/working/datasets/' + str(uuid.UUID()) + '.pth'\n",
    "#         if not os.path.exists(path):\n",
    "#             index = path.rindex('/')\n",
    "#             dirs = path[:index]\n",
    "#             if not os.path.exists(dirs):\n",
    "#                 os.mkdir(dirs)\n",
    "#             with open(path, 'w'):\n",
    "#                 pass\n",
    "#         datas = {\n",
    "#             'initial_dataset': cityscapes,\n",
    "#             'train_loader': train_loader\n",
    "#         }\n",
    "#         torch.save(datas, path)\n",
    "        return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a734573c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T09:57:43.158836Z",
     "iopub.status.busy": "2022-05-26T09:57:43.158648Z",
     "iopub.status.idle": "2022-05-26T09:57:43.185980Z",
     "shell.execute_reply": "2022-05-26T09:57:43.185175Z"
    },
    "papermill": {
     "duration": 0.040729,
     "end_time": "2022-05-26T09:57:43.188093",
     "exception": false,
     "start_time": "2022-05-26T09:57:43.147364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net模型的pytorch实现。\n",
    "    论文地址：https://arxiv.org/abs/1505.04597\n",
    "    模型的总体结构: 编码器 -> 一个ConvBlock -> 解码器 -> 一个Conv 1 * 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # 编码器部分\n",
    "        self.eb1 = EncoderBlock(3, 64, 64, kernel_size=2)\n",
    "        self.eb2 = EncoderBlock(64, 128, 128, kernel_size=2)\n",
    "        self.eb3 = EncoderBlock(128, 256, 256, kernel_size=2)\n",
    "        self.eb4 = EncoderBlock(256, 512, 512, kernel_size=2)\n",
    "        # 编码器与解码器之间有一个ConvBlock\n",
    "        self.cb = ConvBlock(512, 1024, 1024)\n",
    "        # 解码器部分\n",
    "        self.db1 = DecoderBlock(1024, 512, 512)\n",
    "        self.db2 = DecoderBlock(512, 512, 256)\n",
    "        self.db3 = DecoderBlock(256, 128, 128)\n",
    "        self.db4 = DecoderBlock(128, 64, 64)\n",
    "        # 一个Conv 1 * 1\n",
    "        self.conv1x1 = nn.Conv2d(64, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ex1, skip_x1 = self.eb1(x)\n",
    "        ex2, skip_x2 = self.eb2(ex1)\n",
    "        ex3, skip_x3 = self.eb3(ex2)\n",
    "        ex4, skip_x4 = self.eb4(ex3)\n",
    "        cbx = self.cb(ex4)\n",
    "        dx1 = self.db1(cbx, skip_x4)\n",
    "        dx2 = self.db2(dx1, skip_x3)\n",
    "        dx3 = self.db3(dx2, skip_x2)\n",
    "        dx4 = self.db4(dx3, skip_x1)\n",
    "        crop = transforms.CenterCrop(size=(x.shape[-1], x.shape[-2]))\n",
    "        normalize = transforms.Normalize((0.5,), (0.5,))\n",
    "        return self.conv1x1(normalize(crop(dx4)))\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"一个Conv2d卷积后跟一个Relu激活函数，卷积核大小为3 * 3\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        conv_relu_list = [nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=2),\n",
    "                          nn.BatchNorm2d(mid_channels),\n",
    "                          nn.ReLU(inplace=True),\n",
    "                          nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=2),\n",
    "                          nn.BatchNorm2d(out_channels),\n",
    "                          nn.ReLU(inplace=True)]\n",
    "        self.conv_relu = nn.Sequential(*conv_relu_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_relu(x)\n",
    "\n",
    "\n",
    "class DownSampling(nn.Module):\n",
    "    \"\"\"下采样，使用max pool方法执行，核大小为 2 * 2，用在编码器的ConvBlock后面\n",
    "\n",
    "    :param kernel_size: 下采样层（即最大池化层）的核大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(DownSampling, self).__init__()\n",
    "        self.down_sample = nn.MaxPool2d(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_sample(x)\n",
    "\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "    \"\"\"上采样，用在解码器的ConvBlock前面，使用转置卷积，同时通道数减半，\n",
    "\n",
    "    C_out = out_channels\n",
    "    H_out = (H_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "    W_out = (W_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "\n",
    "    :param in_channels: 转置卷积的输入通道数\n",
    "    :param out_channels: 转置卷积的输出通道数\n",
    "    :param kernel_size: 转置卷积的卷积核大小，默认为2\n",
    "    :param stride: 转置卷积的步幅，默认为2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, stride=2, dilation=1, padding=0, output_padding=1):\n",
    "        super(UpSampling, self).__init__()\n",
    "        # self.up_sample = nn.Upsample(scale_factor=scale_factor, mode='bilinear')\n",
    "        # stride=2, kernel_size=2相当于宽高翻倍\n",
    "        self.up_sample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                            dilation=dilation, padding=padding, output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up_sample(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"编码器中的一个层次块\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    :param kernel_size: 下采样层（即最大池化层）的核大小\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, kernel_size):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv_block = ConvBlock(in_channels, mid_channels, out_channels)\n",
    "        self.down_sample = DownSampling(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_block(x)\n",
    "        return self.down_sample(x1), x1\n",
    "\n",
    "\n",
    "class ConcatLayer(nn.Module):\n",
    "    \"\"\"跳跃连接，在通道维上连接\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConcatLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        # 将从编码器传过来的特征图裁剪到与输入相同尺寸\n",
    "        x1 = functional.center_crop(skip_x, [x.shape[-2], x.shape[-1]])\n",
    "        if x1.shape != x.shape:\n",
    "            raise Exception('要连接的两个特征图尺寸不一致，skip_x.shape={}，x.shape={}'.format(skip_x.shape, x.shape))\n",
    "        # 通道维连接\n",
    "        return torch.cat([x, x1], dim=1)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中的层次块，每个层次块都是UpSampling -> Concat -> ConvBlock\n",
    "\n",
    "    :param in_channels: 层次块的输入通道数\n",
    "    :param mid_channels: 层次块中间一层卷积的通道数\n",
    "    :param out_channels: 层次块输出层的通道数\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.up_sample = UpSampling(in_channels, out_channels)\n",
    "        self.conv_block = ConvBlock(in_channels, mid_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        x1 = self.up_sample(x)\n",
    "        concat = ConcatLayer()\n",
    "        x2 = concat(x1, skip_x)\n",
    "        return self.conv_block(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e8c61b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T09:57:43.208726Z",
     "iopub.status.busy": "2022-05-26T09:57:43.208452Z",
     "iopub.status.idle": "2022-05-26T09:57:43.218864Z",
     "shell.execute_reply": "2022-05-26T09:57:43.218152Z"
    },
    "papermill": {
     "duration": 0.022899,
     "end_time": "2022-05-26T09:57:43.220810",
     "exception": false,
     "start_time": "2022-05-26T09:57:43.197911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(model, dataloader, optimizer, loss, epoch, device=None):\n",
    "    epoch_index_list = []\n",
    "    loss_change_list = []\n",
    "    for i in range(epoch):\n",
    "        total_loss = 0.0\n",
    "        for index, (image, label) in enumerate(dataloader):\n",
    "            segment_mask = model(image.to(device))\n",
    "            loss_value = loss(segment_mask, label.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            total_loss = total_loss + loss_value\n",
    "            if index == len(dataloader) - 1:\n",
    "                train_params = {\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': i,\n",
    "                    'loss': loss_value\n",
    "                }\n",
    "                path = '/kaggle/working/model_params/u_net_seg_' + str(i + 1) +  '.pth'\n",
    "                if not os.path.exists(path):\n",
    "                    index = path.rindex('/')\n",
    "                    dirs = path[:index]\n",
    "                    if not os.path.exists(dirs):\n",
    "                        os.mkdir(dirs)\n",
    "                    with open(path, 'w'):\n",
    "                        pass\n",
    "                torch.save(train_params, path)\n",
    "                print('epoch {} batch {}/{} average loss = {:.4f} last loss = {:.4f}'.format(i + 1, index + 1,\n",
    "                                                                                             len(dataloader),\n",
    "                                                                                             total_loss / len(dataloader),\n",
    "                                                                                             loss_value))\n",
    "            else:\n",
    "                print('epoch {} batch {}/{} loss = {:.4f}'.format(i + 1, index + 1, len(dataloader), loss_value))\n",
    "        epoch_index_list.append(i)\n",
    "        loss_change_list.append(total_loss / len(dataloader))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3bddef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T09:57:43.241091Z",
     "iopub.status.busy": "2022-05-26T09:57:43.240883Z",
     "iopub.status.idle": "2022-05-26T09:57:43.244173Z",
     "shell.execute_reply": "2022-05-26T09:57:43.243490Z"
    },
    "papermill": {
     "duration": 0.015474,
     "end_time": "2022-05-26T09:57:43.245744",
     "exception": false,
     "start_time": "2022-05-26T09:57:43.230270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'epoch': 20,\n",
    "    'lr': 3e-4,\n",
    "    'betas': (0.5, 0.999)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4882d742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T09:57:43.265491Z",
     "iopub.status.busy": "2022-05-26T09:57:43.265312Z",
     "iopub.status.idle": "2022-05-26T10:46:05.403481Z",
     "shell.execute_reply": "2022-05-26T10:46:05.401851Z"
    },
    "papermill": {
     "duration": 2902.152571,
     "end_time": "2022-05-26T10:46:05.407744",
     "exception": false,
     "start_time": "2022-05-26T09:57:43.255173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch 1 batch 1/123 loss = 0.7917\n",
      "epoch 1 batch 2/123 loss = 0.6371\n",
      "epoch 1 batch 3/123 loss = 0.4806\n",
      "epoch 1 batch 4/123 loss = 0.3798\n",
      "epoch 1 batch 5/123 loss = 0.3478\n",
      "epoch 1 batch 6/123 loss = 0.3134\n",
      "epoch 1 batch 7/123 loss = 0.2900\n",
      "epoch 1 batch 8/123 loss = 0.2575\n",
      "epoch 1 batch 9/123 loss = 0.2401\n",
      "epoch 1 batch 10/123 loss = 0.2307\n",
      "epoch 1 batch 11/123 loss = 0.2005\n",
      "epoch 1 batch 12/123 loss = 0.1962\n",
      "epoch 1 batch 13/123 loss = 0.1797\n",
      "epoch 1 batch 14/123 loss = 0.1843\n",
      "epoch 1 batch 15/123 loss = 0.1700\n",
      "epoch 1 batch 16/123 loss = 0.1643\n",
      "epoch 1 batch 17/123 loss = 0.1623\n",
      "epoch 1 batch 18/123 loss = 0.1577\n",
      "epoch 1 batch 19/123 loss = 0.1493\n",
      "epoch 1 batch 20/123 loss = 0.1731\n",
      "epoch 1 batch 21/123 loss = 0.1475\n",
      "epoch 1 batch 22/123 loss = 0.1506\n",
      "epoch 1 batch 23/123 loss = 0.1377\n",
      "epoch 1 batch 24/123 loss = 0.1338\n",
      "epoch 1 batch 25/123 loss = 0.1392\n",
      "epoch 1 batch 26/123 loss = 0.1403\n",
      "epoch 1 batch 27/123 loss = 0.1367\n",
      "epoch 1 batch 28/123 loss = 0.1364\n",
      "epoch 1 batch 29/123 loss = 0.1285\n",
      "epoch 1 batch 30/123 loss = 0.1301\n",
      "epoch 1 batch 31/123 loss = 0.1240\n",
      "epoch 1 batch 32/123 loss = 0.1290\n",
      "epoch 1 batch 33/123 loss = 0.1270\n",
      "epoch 1 batch 34/123 loss = 0.1176\n",
      "epoch 1 batch 35/123 loss = 0.1284\n",
      "epoch 1 batch 36/123 loss = 0.1222\n",
      "epoch 1 batch 37/123 loss = 0.1218\n",
      "epoch 1 batch 38/123 loss = 0.1096\n",
      "epoch 1 batch 39/123 loss = 0.1111\n",
      "epoch 1 batch 40/123 loss = 0.1141\n",
      "epoch 1 batch 41/123 loss = 0.1137\n",
      "epoch 1 batch 42/123 loss = 0.1192\n",
      "epoch 1 batch 43/123 loss = 0.1278\n",
      "epoch 1 batch 44/123 loss = 0.1234\n",
      "epoch 1 batch 45/123 loss = 0.1116\n",
      "epoch 1 batch 46/123 loss = 0.1119\n",
      "epoch 1 batch 47/123 loss = 0.1258\n",
      "epoch 1 batch 48/123 loss = 0.1101\n",
      "epoch 1 batch 49/123 loss = 0.1198\n",
      "epoch 1 batch 50/123 loss = 0.1147\n",
      "epoch 1 batch 51/123 loss = 0.1104\n",
      "epoch 1 batch 52/123 loss = 0.1184\n",
      "epoch 1 batch 53/123 loss = 0.1239\n",
      "epoch 1 batch 54/123 loss = 0.1155\n",
      "epoch 1 batch 55/123 loss = 0.1167\n",
      "epoch 1 batch 56/123 loss = 0.1046\n",
      "epoch 1 batch 57/123 loss = 0.1171\n",
      "epoch 1 batch 58/123 loss = 0.1122\n",
      "epoch 1 batch 59/123 loss = 0.1051\n",
      "epoch 1 batch 60/123 loss = 0.1174\n",
      "epoch 1 batch 61/123 loss = 0.1089\n",
      "epoch 1 batch 62/123 loss = 0.1067\n",
      "epoch 1 batch 63/123 loss = 0.1134\n",
      "epoch 1 batch 64/123 loss = 0.1156\n",
      "epoch 1 batch 65/123 loss = 0.1030\n",
      "epoch 1 batch 66/123 loss = 0.1124\n",
      "epoch 1 batch 67/123 loss = 0.1124\n",
      "epoch 1 batch 68/123 loss = 0.0978\n",
      "epoch 1 batch 69/123 loss = 0.1119\n",
      "epoch 1 batch 70/123 loss = 0.1071\n",
      "epoch 1 batch 71/123 loss = 0.1055\n",
      "epoch 1 batch 72/123 loss = 0.1118\n",
      "epoch 1 batch 73/123 loss = 0.1136\n",
      "epoch 1 batch 74/123 loss = 0.1155\n",
      "epoch 1 batch 75/123 loss = 0.1003\n",
      "epoch 1 batch 76/123 loss = 0.1001\n",
      "epoch 1 batch 77/123 loss = 0.1042\n",
      "epoch 1 batch 78/123 loss = 0.0977\n",
      "epoch 1 batch 79/123 loss = 0.1037\n",
      "epoch 1 batch 80/123 loss = 0.0988\n",
      "epoch 1 batch 81/123 loss = 0.1106\n",
      "epoch 1 batch 82/123 loss = 0.0915\n",
      "epoch 1 batch 83/123 loss = 0.1174\n",
      "epoch 1 batch 84/123 loss = 0.0913\n",
      "epoch 1 batch 85/123 loss = 0.0965\n",
      "epoch 1 batch 86/123 loss = 0.0932\n",
      "epoch 1 batch 87/123 loss = 0.0983\n",
      "epoch 1 batch 88/123 loss = 0.0868\n",
      "epoch 1 batch 89/123 loss = 0.0937\n",
      "epoch 1 batch 90/123 loss = 0.1033\n",
      "epoch 1 batch 91/123 loss = 0.0967\n",
      "epoch 1 batch 92/123 loss = 0.0959\n",
      "epoch 1 batch 93/123 loss = 0.0900\n",
      "epoch 1 batch 94/123 loss = 0.0903\n",
      "epoch 1 batch 95/123 loss = 0.1005\n",
      "epoch 1 batch 96/123 loss = 0.0933\n",
      "epoch 1 batch 97/123 loss = 0.0808\n",
      "epoch 1 batch 98/123 loss = 0.1072\n",
      "epoch 1 batch 99/123 loss = 0.0911\n",
      "epoch 1 batch 100/123 loss = 0.0886\n",
      "epoch 1 batch 101/123 loss = 0.0864\n",
      "epoch 1 batch 102/123 loss = 0.0896\n",
      "epoch 1 batch 103/123 loss = 0.1045\n",
      "epoch 1 batch 104/123 loss = 0.0957\n",
      "epoch 1 batch 105/123 loss = 0.0891\n",
      "epoch 1 batch 106/123 loss = 0.0813\n",
      "epoch 1 batch 107/123 loss = 0.0954\n",
      "epoch 1 batch 108/123 loss = 0.0979\n",
      "epoch 1 batch 109/123 loss = 0.0794\n",
      "epoch 1 batch 110/123 loss = 0.0752\n",
      "epoch 1 batch 111/123 loss = 0.0838\n",
      "epoch 1 batch 112/123 loss = 0.0914\n",
      "epoch 1 batch 113/123 loss = 0.0893\n",
      "epoch 1 batch 114/123 loss = 0.0823\n",
      "epoch 1 batch 115/123 loss = 0.0760\n",
      "epoch 1 batch 116/123 loss = 0.0884\n",
      "epoch 1 batch 117/123 loss = 0.0815\n",
      "epoch 1 batch 118/123 loss = 0.0922\n",
      "epoch 1 batch 119/123 loss = 0.0870\n",
      "epoch 1 batch 120/123 loss = 0.0871\n",
      "epoch 1 batch 121/123 loss = 0.0927\n",
      "epoch 1 batch 122/123 loss = 0.1023\n",
      "epoch 1 batch 29/123 average loss = 0.1359 last loss = 0.0937\n",
      "epoch 2 batch 1/123 loss = 0.0972\n",
      "epoch 2 batch 2/123 loss = 0.0842\n",
      "epoch 2 batch 3/123 loss = 0.0815\n",
      "epoch 2 batch 4/123 loss = 0.0838\n",
      "epoch 2 batch 5/123 loss = 0.0859\n",
      "epoch 2 batch 6/123 loss = 0.0862\n",
      "epoch 2 batch 7/123 loss = 0.0832\n",
      "epoch 2 batch 8/123 loss = 0.0789\n",
      "epoch 2 batch 9/123 loss = 0.0829\n",
      "epoch 2 batch 10/123 loss = 0.0796\n",
      "epoch 2 batch 11/123 loss = 0.0792\n",
      "epoch 2 batch 12/123 loss = 0.0801\n",
      "epoch 2 batch 13/123 loss = 0.0753\n",
      "epoch 2 batch 14/123 loss = 0.0936\n",
      "epoch 2 batch 15/123 loss = 0.0791\n",
      "epoch 2 batch 16/123 loss = 0.0850\n",
      "epoch 2 batch 17/123 loss = 0.0828\n",
      "epoch 2 batch 18/123 loss = 0.0781\n",
      "epoch 2 batch 19/123 loss = 0.0843\n",
      "epoch 2 batch 20/123 loss = 0.0875\n",
      "epoch 2 batch 21/123 loss = 0.0979\n",
      "epoch 2 batch 22/123 loss = 0.0848\n",
      "epoch 2 batch 23/123 loss = 0.0796\n",
      "epoch 2 batch 24/123 loss = 0.0713\n",
      "epoch 2 batch 25/123 loss = 0.0865\n",
      "epoch 2 batch 26/123 loss = 0.0720\n",
      "epoch 2 batch 27/123 loss = 0.0836\n",
      "epoch 2 batch 28/123 loss = 0.0800\n",
      "epoch 2 batch 29/123 loss = 0.0746\n",
      "epoch 2 batch 30/123 loss = 0.0765\n",
      "epoch 2 batch 31/123 loss = 0.0823\n",
      "epoch 2 batch 32/123 loss = 0.0778\n",
      "epoch 2 batch 33/123 loss = 0.0707\n",
      "epoch 2 batch 34/123 loss = 0.0844\n",
      "epoch 2 batch 35/123 loss = 0.0839\n",
      "epoch 2 batch 36/123 loss = 0.0805\n",
      "epoch 2 batch 37/123 loss = 0.0696\n",
      "epoch 2 batch 38/123 loss = 0.0730\n",
      "epoch 2 batch 39/123 loss = 0.0703\n",
      "epoch 2 batch 40/123 loss = 0.0788\n",
      "epoch 2 batch 41/123 loss = 0.0750\n",
      "epoch 2 batch 42/123 loss = 0.0768\n",
      "epoch 2 batch 43/123 loss = 0.0792\n",
      "epoch 2 batch 44/123 loss = 0.0839\n",
      "epoch 2 batch 45/123 loss = 0.0820\n",
      "epoch 2 batch 46/123 loss = 0.0806\n",
      "epoch 2 batch 47/123 loss = 0.0789\n",
      "epoch 2 batch 48/123 loss = 0.0666\n",
      "epoch 2 batch 49/123 loss = 0.0789\n",
      "epoch 2 batch 50/123 loss = 0.0724\n",
      "epoch 2 batch 51/123 loss = 0.0750\n",
      "epoch 2 batch 52/123 loss = 0.0628\n",
      "epoch 2 batch 53/123 loss = 0.0757\n",
      "epoch 2 batch 54/123 loss = 0.0849\n",
      "epoch 2 batch 55/123 loss = 0.0699\n",
      "epoch 2 batch 56/123 loss = 0.0731\n",
      "epoch 2 batch 57/123 loss = 0.0702\n",
      "epoch 2 batch 58/123 loss = 0.0745\n",
      "epoch 2 batch 59/123 loss = 0.0667\n",
      "epoch 2 batch 60/123 loss = 0.0634\n",
      "epoch 2 batch 61/123 loss = 0.0804\n",
      "epoch 2 batch 62/123 loss = 0.0740\n",
      "epoch 2 batch 63/123 loss = 0.0810\n",
      "epoch 2 batch 64/123 loss = 0.0682\n",
      "epoch 2 batch 65/123 loss = 0.0819\n",
      "epoch 2 batch 66/123 loss = 0.0718\n",
      "epoch 2 batch 67/123 loss = 0.0752\n",
      "epoch 2 batch 68/123 loss = 0.0734\n",
      "epoch 2 batch 69/123 loss = 0.0658\n",
      "epoch 2 batch 70/123 loss = 0.0725\n",
      "epoch 2 batch 71/123 loss = 0.0770\n",
      "epoch 2 batch 72/123 loss = 0.0666\n",
      "epoch 2 batch 73/123 loss = 0.0803\n",
      "epoch 2 batch 74/123 loss = 0.0700\n",
      "epoch 2 batch 75/123 loss = 0.0641\n",
      "epoch 2 batch 76/123 loss = 0.0609\n",
      "epoch 2 batch 77/123 loss = 0.0724\n",
      "epoch 2 batch 78/123 loss = 0.0627\n",
      "epoch 2 batch 79/123 loss = 0.0801\n",
      "epoch 2 batch 80/123 loss = 0.0827\n",
      "epoch 2 batch 81/123 loss = 0.0734\n",
      "epoch 2 batch 82/123 loss = 0.0682\n",
      "epoch 2 batch 83/123 loss = 0.0662\n",
      "epoch 2 batch 84/123 loss = 0.0759\n",
      "epoch 2 batch 85/123 loss = 0.0877\n",
      "epoch 2 batch 86/123 loss = 0.0700\n",
      "epoch 2 batch 87/123 loss = 0.0855\n",
      "epoch 2 batch 88/123 loss = 0.0786\n",
      "epoch 2 batch 89/123 loss = 0.0793\n",
      "epoch 2 batch 90/123 loss = 0.0793\n",
      "epoch 2 batch 91/123 loss = 0.0721\n",
      "epoch 2 batch 92/123 loss = 0.0709\n",
      "epoch 2 batch 93/123 loss = 0.0772\n",
      "epoch 2 batch 94/123 loss = 0.0669\n",
      "epoch 2 batch 95/123 loss = 0.0775\n",
      "epoch 2 batch 96/123 loss = 0.0702\n",
      "epoch 2 batch 97/123 loss = 0.0755\n",
      "epoch 2 batch 98/123 loss = 0.0787\n",
      "epoch 2 batch 99/123 loss = 0.0872\n",
      "epoch 2 batch 100/123 loss = 0.0758\n",
      "epoch 2 batch 101/123 loss = 0.0744\n",
      "epoch 2 batch 102/123 loss = 0.0688\n",
      "epoch 2 batch 103/123 loss = 0.0793\n",
      "epoch 2 batch 104/123 loss = 0.0835\n",
      "epoch 2 batch 105/123 loss = 0.0724\n",
      "epoch 2 batch 106/123 loss = 0.0703\n",
      "epoch 2 batch 107/123 loss = 0.0770\n",
      "epoch 2 batch 108/123 loss = 0.0735\n",
      "epoch 2 batch 109/123 loss = 0.0700\n",
      "epoch 2 batch 110/123 loss = 0.0765\n",
      "epoch 2 batch 111/123 loss = 0.0735\n",
      "epoch 2 batch 112/123 loss = 0.0641\n",
      "epoch 2 batch 113/123 loss = 0.0756\n",
      "epoch 2 batch 114/123 loss = 0.0778\n",
      "epoch 2 batch 115/123 loss = 0.0655\n",
      "epoch 2 batch 116/123 loss = 0.0659\n",
      "epoch 2 batch 117/123 loss = 0.0717\n",
      "epoch 2 batch 118/123 loss = 0.0735\n",
      "epoch 2 batch 119/123 loss = 0.0686\n",
      "epoch 2 batch 120/123 loss = 0.0704\n",
      "epoch 2 batch 121/123 loss = 0.0708\n",
      "epoch 2 batch 122/123 loss = 0.0661\n",
      "epoch 2 batch 29/123 average loss = 0.0763 last loss = 0.0757\n",
      "epoch 3 batch 1/123 loss = 0.0708\n",
      "epoch 3 batch 2/123 loss = 0.0745\n",
      "epoch 3 batch 3/123 loss = 0.0658\n",
      "epoch 3 batch 4/123 loss = 0.0669\n",
      "epoch 3 batch 5/123 loss = 0.0689\n",
      "epoch 3 batch 6/123 loss = 0.0703\n",
      "epoch 3 batch 7/123 loss = 0.0575\n",
      "epoch 3 batch 8/123 loss = 0.0755\n",
      "epoch 3 batch 9/123 loss = 0.0661\n",
      "epoch 3 batch 10/123 loss = 0.0807\n",
      "epoch 3 batch 11/123 loss = 0.0719\n",
      "epoch 3 batch 12/123 loss = 0.0709\n",
      "epoch 3 batch 13/123 loss = 0.0699\n",
      "epoch 3 batch 14/123 loss = 0.0615\n",
      "epoch 3 batch 15/123 loss = 0.0711\n",
      "epoch 3 batch 16/123 loss = 0.0649\n",
      "epoch 3 batch 17/123 loss = 0.0773\n",
      "epoch 3 batch 18/123 loss = 0.0739\n",
      "epoch 3 batch 19/123 loss = 0.0624\n",
      "epoch 3 batch 20/123 loss = 0.0642\n",
      "epoch 3 batch 21/123 loss = 0.0654\n",
      "epoch 3 batch 22/123 loss = 0.0622\n",
      "epoch 3 batch 23/123 loss = 0.0562\n",
      "epoch 3 batch 24/123 loss = 0.0668\n",
      "epoch 3 batch 25/123 loss = 0.0773\n",
      "epoch 3 batch 26/123 loss = 0.0793\n",
      "epoch 3 batch 27/123 loss = 0.0617\n",
      "epoch 3 batch 28/123 loss = 0.0714\n",
      "epoch 3 batch 29/123 loss = 0.0785\n",
      "epoch 3 batch 30/123 loss = 0.0625\n",
      "epoch 3 batch 31/123 loss = 0.0757\n",
      "epoch 3 batch 32/123 loss = 0.0687\n",
      "epoch 3 batch 33/123 loss = 0.0653\n",
      "epoch 3 batch 34/123 loss = 0.0651\n",
      "epoch 3 batch 35/123 loss = 0.0694\n",
      "epoch 3 batch 36/123 loss = 0.0764\n",
      "epoch 3 batch 37/123 loss = 0.0787\n",
      "epoch 3 batch 38/123 loss = 0.0860\n",
      "epoch 3 batch 39/123 loss = 0.0708\n",
      "epoch 3 batch 40/123 loss = 0.0612\n",
      "epoch 3 batch 41/123 loss = 0.0843\n",
      "epoch 3 batch 42/123 loss = 0.0695\n",
      "epoch 3 batch 43/123 loss = 0.0629\n",
      "epoch 3 batch 44/123 loss = 0.0628\n",
      "epoch 3 batch 45/123 loss = 0.0607\n",
      "epoch 3 batch 46/123 loss = 0.0628\n",
      "epoch 3 batch 47/123 loss = 0.0713\n",
      "epoch 3 batch 48/123 loss = 0.0602\n",
      "epoch 3 batch 49/123 loss = 0.0589\n",
      "epoch 3 batch 50/123 loss = 0.0587\n",
      "epoch 3 batch 51/123 loss = 0.0558\n",
      "epoch 3 batch 52/123 loss = 0.0565\n",
      "epoch 3 batch 53/123 loss = 0.0737\n",
      "epoch 3 batch 54/123 loss = 0.0739\n",
      "epoch 3 batch 55/123 loss = 0.0635\n",
      "epoch 3 batch 56/123 loss = 0.0562\n",
      "epoch 3 batch 57/123 loss = 0.0735\n",
      "epoch 3 batch 58/123 loss = 0.0610\n",
      "epoch 3 batch 59/123 loss = 0.0721\n",
      "epoch 3 batch 60/123 loss = 0.0669\n",
      "epoch 3 batch 61/123 loss = 0.0625\n",
      "epoch 3 batch 62/123 loss = 0.0606\n",
      "epoch 3 batch 63/123 loss = 0.0648\n",
      "epoch 3 batch 64/123 loss = 0.0624\n",
      "epoch 3 batch 65/123 loss = 0.0589\n",
      "epoch 3 batch 66/123 loss = 0.0623\n",
      "epoch 3 batch 67/123 loss = 0.0566\n",
      "epoch 3 batch 68/123 loss = 0.0647\n",
      "epoch 3 batch 69/123 loss = 0.0656\n",
      "epoch 3 batch 70/123 loss = 0.0749\n",
      "epoch 3 batch 71/123 loss = 0.0567\n",
      "epoch 3 batch 72/123 loss = 0.0538\n",
      "epoch 3 batch 73/123 loss = 0.0650\n",
      "epoch 3 batch 74/123 loss = 0.0656\n",
      "epoch 3 batch 75/123 loss = 0.0644\n",
      "epoch 3 batch 76/123 loss = 0.0671\n",
      "epoch 3 batch 77/123 loss = 0.0630\n",
      "epoch 3 batch 78/123 loss = 0.0683\n",
      "epoch 3 batch 79/123 loss = 0.0580\n",
      "epoch 3 batch 80/123 loss = 0.0681\n",
      "epoch 3 batch 81/123 loss = 0.0618\n",
      "epoch 3 batch 82/123 loss = 0.0689\n",
      "epoch 3 batch 83/123 loss = 0.0655\n",
      "epoch 3 batch 84/123 loss = 0.0593\n",
      "epoch 3 batch 85/123 loss = 0.0643\n",
      "epoch 3 batch 86/123 loss = 0.0638\n",
      "epoch 3 batch 87/123 loss = 0.0657\n",
      "epoch 3 batch 88/123 loss = 0.0637\n",
      "epoch 3 batch 89/123 loss = 0.0713\n",
      "epoch 3 batch 90/123 loss = 0.0615\n",
      "epoch 3 batch 91/123 loss = 0.0704\n",
      "epoch 3 batch 92/123 loss = 0.0640\n",
      "epoch 3 batch 93/123 loss = 0.0601\n",
      "epoch 3 batch 94/123 loss = 0.0557\n",
      "epoch 3 batch 95/123 loss = 0.0592\n",
      "epoch 3 batch 96/123 loss = 0.0645\n",
      "epoch 3 batch 97/123 loss = 0.0650\n",
      "epoch 3 batch 98/123 loss = 0.0619\n",
      "epoch 3 batch 99/123 loss = 0.0592\n",
      "epoch 3 batch 100/123 loss = 0.0618\n",
      "epoch 3 batch 101/123 loss = 0.0638\n",
      "epoch 3 batch 102/123 loss = 0.0821\n",
      "epoch 3 batch 103/123 loss = 0.0669\n",
      "epoch 3 batch 104/123 loss = 0.0715\n",
      "epoch 3 batch 105/123 loss = 0.0623\n",
      "epoch 3 batch 106/123 loss = 0.0607\n",
      "epoch 3 batch 107/123 loss = 0.0634\n",
      "epoch 3 batch 108/123 loss = 0.0685\n",
      "epoch 3 batch 109/123 loss = 0.0590\n",
      "epoch 3 batch 110/123 loss = 0.0545\n",
      "epoch 3 batch 111/123 loss = 0.0626\n",
      "epoch 3 batch 112/123 loss = 0.0548\n",
      "epoch 3 batch 113/123 loss = 0.0669\n",
      "epoch 3 batch 114/123 loss = 0.0519\n",
      "epoch 3 batch 115/123 loss = 0.0600\n",
      "epoch 3 batch 116/123 loss = 0.0518\n",
      "epoch 3 batch 117/123 loss = 0.0572\n",
      "epoch 3 batch 118/123 loss = 0.0708\n",
      "epoch 3 batch 119/123 loss = 0.0653\n",
      "epoch 3 batch 120/123 loss = 0.0668\n",
      "epoch 3 batch 121/123 loss = 0.0555\n",
      "epoch 3 batch 122/123 loss = 0.0535\n",
      "epoch 3 batch 29/123 average loss = 0.0655 last loss = 0.0610\n",
      "epoch 4 batch 1/123 loss = 0.0739\n",
      "epoch 4 batch 2/123 loss = 0.0555\n",
      "epoch 4 batch 3/123 loss = 0.0624\n",
      "epoch 4 batch 4/123 loss = 0.0601\n",
      "epoch 4 batch 5/123 loss = 0.0542\n",
      "epoch 4 batch 6/123 loss = 0.0615\n",
      "epoch 4 batch 7/123 loss = 0.0640\n",
      "epoch 4 batch 8/123 loss = 0.0553\n",
      "epoch 4 batch 9/123 loss = 0.0525\n",
      "epoch 4 batch 10/123 loss = 0.0677\n",
      "epoch 4 batch 11/123 loss = 0.0508\n",
      "epoch 4 batch 12/123 loss = 0.0574\n",
      "epoch 4 batch 13/123 loss = 0.0610\n",
      "epoch 4 batch 14/123 loss = 0.0639\n",
      "epoch 4 batch 15/123 loss = 0.0563\n",
      "epoch 4 batch 16/123 loss = 0.0653\n",
      "epoch 4 batch 17/123 loss = 0.0675\n",
      "epoch 4 batch 18/123 loss = 0.0702\n",
      "epoch 4 batch 19/123 loss = 0.0629\n",
      "epoch 4 batch 20/123 loss = 0.0582\n",
      "epoch 4 batch 21/123 loss = 0.0583\n",
      "epoch 4 batch 22/123 loss = 0.0570\n",
      "epoch 4 batch 23/123 loss = 0.0544\n",
      "epoch 4 batch 24/123 loss = 0.0646\n",
      "epoch 4 batch 25/123 loss = 0.0649\n",
      "epoch 4 batch 26/123 loss = 0.0704\n",
      "epoch 4 batch 27/123 loss = 0.0710\n",
      "epoch 4 batch 28/123 loss = 0.0590\n",
      "epoch 4 batch 29/123 loss = 0.0733\n",
      "epoch 4 batch 30/123 loss = 0.0642\n",
      "epoch 4 batch 31/123 loss = 0.0629\n",
      "epoch 4 batch 32/123 loss = 0.0676\n",
      "epoch 4 batch 33/123 loss = 0.0644\n",
      "epoch 4 batch 34/123 loss = 0.0520\n",
      "epoch 4 batch 35/123 loss = 0.0577\n",
      "epoch 4 batch 36/123 loss = 0.0641\n",
      "epoch 4 batch 37/123 loss = 0.0575\n",
      "epoch 4 batch 38/123 loss = 0.0528\n",
      "epoch 4 batch 39/123 loss = 0.0704\n",
      "epoch 4 batch 40/123 loss = 0.0625\n",
      "epoch 4 batch 41/123 loss = 0.0617\n",
      "epoch 4 batch 42/123 loss = 0.0614\n",
      "epoch 4 batch 43/123 loss = 0.0631\n",
      "epoch 4 batch 44/123 loss = 0.0548\n",
      "epoch 4 batch 45/123 loss = 0.0701\n",
      "epoch 4 batch 46/123 loss = 0.0671\n",
      "epoch 4 batch 47/123 loss = 0.0618\n",
      "epoch 4 batch 48/123 loss = 0.0684\n",
      "epoch 4 batch 49/123 loss = 0.0635\n",
      "epoch 4 batch 50/123 loss = 0.0599\n",
      "epoch 4 batch 51/123 loss = 0.0574\n",
      "epoch 4 batch 52/123 loss = 0.0591\n",
      "epoch 4 batch 53/123 loss = 0.0617\n",
      "epoch 4 batch 54/123 loss = 0.0634\n",
      "epoch 4 batch 55/123 loss = 0.0560\n",
      "epoch 4 batch 56/123 loss = 0.0572\n",
      "epoch 4 batch 57/123 loss = 0.0555\n",
      "epoch 4 batch 58/123 loss = 0.0518\n",
      "epoch 4 batch 59/123 loss = 0.0698\n",
      "epoch 4 batch 60/123 loss = 0.0605\n",
      "epoch 4 batch 61/123 loss = 0.0528\n",
      "epoch 4 batch 62/123 loss = 0.0540\n",
      "epoch 4 batch 63/123 loss = 0.0623\n",
      "epoch 4 batch 64/123 loss = 0.0585\n",
      "epoch 4 batch 65/123 loss = 0.0666\n",
      "epoch 4 batch 66/123 loss = 0.0625\n",
      "epoch 4 batch 67/123 loss = 0.0624\n",
      "epoch 4 batch 68/123 loss = 0.0624\n",
      "epoch 4 batch 69/123 loss = 0.0598\n",
      "epoch 4 batch 70/123 loss = 0.0615\n",
      "epoch 4 batch 71/123 loss = 0.0625\n",
      "epoch 4 batch 72/123 loss = 0.0644\n",
      "epoch 4 batch 73/123 loss = 0.0541\n",
      "epoch 4 batch 74/123 loss = 0.0560\n",
      "epoch 4 batch 75/123 loss = 0.0669\n",
      "epoch 4 batch 76/123 loss = 0.0556\n",
      "epoch 4 batch 77/123 loss = 0.0609\n",
      "epoch 4 batch 78/123 loss = 0.0570\n",
      "epoch 4 batch 79/123 loss = 0.0615\n",
      "epoch 4 batch 80/123 loss = 0.0553\n",
      "epoch 4 batch 81/123 loss = 0.0539\n",
      "epoch 4 batch 82/123 loss = 0.0475\n",
      "epoch 4 batch 83/123 loss = 0.0595\n",
      "epoch 4 batch 84/123 loss = 0.0459\n",
      "epoch 4 batch 85/123 loss = 0.0508\n",
      "epoch 4 batch 86/123 loss = 0.0582\n",
      "epoch 4 batch 87/123 loss = 0.0589\n",
      "epoch 4 batch 88/123 loss = 0.0663\n",
      "epoch 4 batch 89/123 loss = 0.0537\n",
      "epoch 4 batch 90/123 loss = 0.0524\n",
      "epoch 4 batch 91/123 loss = 0.0656\n",
      "epoch 4 batch 92/123 loss = 0.0692\n",
      "epoch 4 batch 93/123 loss = 0.0620\n",
      "epoch 4 batch 94/123 loss = 0.0573\n",
      "epoch 4 batch 95/123 loss = 0.0636\n",
      "epoch 4 batch 96/123 loss = 0.0558\n",
      "epoch 4 batch 97/123 loss = 0.0569\n",
      "epoch 4 batch 98/123 loss = 0.0646\n",
      "epoch 4 batch 99/123 loss = 0.0547\n",
      "epoch 4 batch 100/123 loss = 0.0680\n",
      "epoch 4 batch 101/123 loss = 0.0513\n",
      "epoch 4 batch 102/123 loss = 0.0569\n",
      "epoch 4 batch 103/123 loss = 0.0587\n",
      "epoch 4 batch 104/123 loss = 0.0590\n",
      "epoch 4 batch 105/123 loss = 0.0561\n",
      "epoch 4 batch 106/123 loss = 0.0523\n",
      "epoch 4 batch 107/123 loss = 0.0557\n",
      "epoch 4 batch 108/123 loss = 0.0485\n",
      "epoch 4 batch 109/123 loss = 0.0479\n",
      "epoch 4 batch 110/123 loss = 0.0548\n",
      "epoch 4 batch 111/123 loss = 0.0511\n",
      "epoch 4 batch 112/123 loss = 0.0644\n",
      "epoch 4 batch 113/123 loss = 0.0521\n",
      "epoch 4 batch 114/123 loss = 0.0600\n",
      "epoch 4 batch 115/123 loss = 0.0569\n",
      "epoch 4 batch 116/123 loss = 0.0547\n",
      "epoch 4 batch 117/123 loss = 0.0576\n",
      "epoch 4 batch 118/123 loss = 0.0616\n",
      "epoch 4 batch 119/123 loss = 0.0573\n",
      "epoch 4 batch 120/123 loss = 0.0540\n",
      "epoch 4 batch 121/123 loss = 0.0603\n",
      "epoch 4 batch 122/123 loss = 0.0568\n",
      "epoch 4 batch 29/123 average loss = 0.0596 last loss = 0.0523\n",
      "epoch 5 batch 1/123 loss = 0.0515\n",
      "epoch 5 batch 2/123 loss = 0.0571\n",
      "epoch 5 batch 3/123 loss = 0.0572\n",
      "epoch 5 batch 4/123 loss = 0.0668\n",
      "epoch 5 batch 5/123 loss = 0.0649\n",
      "epoch 5 batch 6/123 loss = 0.0545\n",
      "epoch 5 batch 7/123 loss = 0.0580\n",
      "epoch 5 batch 8/123 loss = 0.0586\n",
      "epoch 5 batch 9/123 loss = 0.0536\n",
      "epoch 5 batch 10/123 loss = 0.0634\n",
      "epoch 5 batch 11/123 loss = 0.0713\n",
      "epoch 5 batch 12/123 loss = 0.0583\n",
      "epoch 5 batch 13/123 loss = 0.0598\n",
      "epoch 5 batch 14/123 loss = 0.0510\n",
      "epoch 5 batch 15/123 loss = 0.0631\n",
      "epoch 5 batch 16/123 loss = 0.0530\n",
      "epoch 5 batch 17/123 loss = 0.0622\n",
      "epoch 5 batch 18/123 loss = 0.0483\n",
      "epoch 5 batch 19/123 loss = 0.0565\n",
      "epoch 5 batch 20/123 loss = 0.0456\n",
      "epoch 5 batch 21/123 loss = 0.0475\n",
      "epoch 5 batch 22/123 loss = 0.0559\n",
      "epoch 5 batch 23/123 loss = 0.0576\n",
      "epoch 5 batch 24/123 loss = 0.0553\n",
      "epoch 5 batch 25/123 loss = 0.0616\n",
      "epoch 5 batch 26/123 loss = 0.0708\n",
      "epoch 5 batch 27/123 loss = 0.0623\n",
      "epoch 5 batch 28/123 loss = 0.0574\n",
      "epoch 5 batch 29/123 loss = 0.0554\n",
      "epoch 5 batch 30/123 loss = 0.0514\n",
      "epoch 5 batch 31/123 loss = 0.0649\n",
      "epoch 5 batch 32/123 loss = 0.0622\n",
      "epoch 5 batch 33/123 loss = 0.0497\n",
      "epoch 5 batch 34/123 loss = 0.0560\n",
      "epoch 5 batch 35/123 loss = 0.0535\n",
      "epoch 5 batch 36/123 loss = 0.0522\n",
      "epoch 5 batch 37/123 loss = 0.0505\n",
      "epoch 5 batch 38/123 loss = 0.0540\n",
      "epoch 5 batch 39/123 loss = 0.0707\n",
      "epoch 5 batch 40/123 loss = 0.0570\n",
      "epoch 5 batch 41/123 loss = 0.0590\n",
      "epoch 5 batch 42/123 loss = 0.0626\n",
      "epoch 5 batch 43/123 loss = 0.0534\n",
      "epoch 5 batch 44/123 loss = 0.0558\n",
      "epoch 5 batch 45/123 loss = 0.0567\n",
      "epoch 5 batch 46/123 loss = 0.0593\n",
      "epoch 5 batch 47/123 loss = 0.0557\n",
      "epoch 5 batch 48/123 loss = 0.0600\n",
      "epoch 5 batch 49/123 loss = 0.0543\n",
      "epoch 5 batch 50/123 loss = 0.0464\n",
      "epoch 5 batch 51/123 loss = 0.0514\n",
      "epoch 5 batch 52/123 loss = 0.0592\n",
      "epoch 5 batch 53/123 loss = 0.0582\n",
      "epoch 5 batch 54/123 loss = 0.0620\n",
      "epoch 5 batch 55/123 loss = 0.0529\n",
      "epoch 5 batch 56/123 loss = 0.0540\n",
      "epoch 5 batch 57/123 loss = 0.0592\n",
      "epoch 5 batch 58/123 loss = 0.0608\n",
      "epoch 5 batch 59/123 loss = 0.0590\n",
      "epoch 5 batch 60/123 loss = 0.0599\n",
      "epoch 5 batch 61/123 loss = 0.0506\n",
      "epoch 5 batch 62/123 loss = 0.0580\n",
      "epoch 5 batch 63/123 loss = 0.0597\n",
      "epoch 5 batch 64/123 loss = 0.0603\n",
      "epoch 5 batch 65/123 loss = 0.0639\n",
      "epoch 5 batch 66/123 loss = 0.0518\n",
      "epoch 5 batch 67/123 loss = 0.0561\n",
      "epoch 5 batch 68/123 loss = 0.0586\n",
      "epoch 5 batch 69/123 loss = 0.0568\n",
      "epoch 5 batch 70/123 loss = 0.0692\n",
      "epoch 5 batch 71/123 loss = 0.0528\n",
      "epoch 5 batch 72/123 loss = 0.0457\n",
      "epoch 5 batch 73/123 loss = 0.0486\n",
      "epoch 5 batch 74/123 loss = 0.0596\n",
      "epoch 5 batch 75/123 loss = 0.0584\n",
      "epoch 5 batch 76/123 loss = 0.0547\n",
      "epoch 5 batch 77/123 loss = 0.0623\n",
      "epoch 5 batch 78/123 loss = 0.0513\n",
      "epoch 5 batch 79/123 loss = 0.0523\n",
      "epoch 5 batch 80/123 loss = 0.0562\n",
      "epoch 5 batch 81/123 loss = 0.0605\n",
      "epoch 5 batch 82/123 loss = 0.0498\n",
      "epoch 5 batch 83/123 loss = 0.0657\n",
      "epoch 5 batch 84/123 loss = 0.0551\n",
      "epoch 5 batch 85/123 loss = 0.0598\n",
      "epoch 5 batch 86/123 loss = 0.0642\n",
      "epoch 5 batch 87/123 loss = 0.0536\n",
      "epoch 5 batch 88/123 loss = 0.0561\n",
      "epoch 5 batch 89/123 loss = 0.0587\n",
      "epoch 5 batch 90/123 loss = 0.0515\n",
      "epoch 5 batch 91/123 loss = 0.0587\n",
      "epoch 5 batch 92/123 loss = 0.0665\n",
      "epoch 5 batch 93/123 loss = 0.0580\n",
      "epoch 5 batch 94/123 loss = 0.0498\n",
      "epoch 5 batch 95/123 loss = 0.0564\n",
      "epoch 5 batch 96/123 loss = 0.0687\n",
      "epoch 5 batch 97/123 loss = 0.0602\n",
      "epoch 5 batch 98/123 loss = 0.0518\n",
      "epoch 5 batch 99/123 loss = 0.0626\n",
      "epoch 5 batch 100/123 loss = 0.0581\n",
      "epoch 5 batch 101/123 loss = 0.0589\n",
      "epoch 5 batch 102/123 loss = 0.0568\n",
      "epoch 5 batch 103/123 loss = 0.0570\n",
      "epoch 5 batch 104/123 loss = 0.0538\n",
      "epoch 5 batch 105/123 loss = 0.0660\n",
      "epoch 5 batch 106/123 loss = 0.0594\n",
      "epoch 5 batch 107/123 loss = 0.0521\n",
      "epoch 5 batch 108/123 loss = 0.0619\n",
      "epoch 5 batch 109/123 loss = 0.0568\n",
      "epoch 5 batch 110/123 loss = 0.0571\n",
      "epoch 5 batch 111/123 loss = 0.0520\n",
      "epoch 5 batch 112/123 loss = 0.0575\n",
      "epoch 5 batch 113/123 loss = 0.0596\n",
      "epoch 5 batch 114/123 loss = 0.0555\n",
      "epoch 5 batch 115/123 loss = 0.0580\n",
      "epoch 5 batch 116/123 loss = 0.0538\n",
      "epoch 5 batch 117/123 loss = 0.0537\n",
      "epoch 5 batch 118/123 loss = 0.0524\n",
      "epoch 5 batch 119/123 loss = 0.0455\n",
      "epoch 5 batch 120/123 loss = 0.0568\n",
      "epoch 5 batch 121/123 loss = 0.0585\n",
      "epoch 5 batch 122/123 loss = 0.0535\n",
      "epoch 5 batch 29/123 average loss = 0.0572 last loss = 0.0628\n",
      "epoch 6 batch 1/123 loss = 0.0586\n",
      "epoch 6 batch 2/123 loss = 0.0626\n",
      "epoch 6 batch 3/123 loss = 0.0500\n",
      "epoch 6 batch 4/123 loss = 0.0580\n",
      "epoch 6 batch 5/123 loss = 0.0485\n",
      "epoch 6 batch 6/123 loss = 0.0541\n",
      "epoch 6 batch 7/123 loss = 0.0564\n",
      "epoch 6 batch 8/123 loss = 0.0503\n",
      "epoch 6 batch 9/123 loss = 0.0562\n",
      "epoch 6 batch 10/123 loss = 0.0613\n",
      "epoch 6 batch 11/123 loss = 0.0613\n",
      "epoch 6 batch 12/123 loss = 0.0550\n",
      "epoch 6 batch 13/123 loss = 0.0533\n",
      "epoch 6 batch 14/123 loss = 0.0678\n",
      "epoch 6 batch 15/123 loss = 0.0510\n",
      "epoch 6 batch 16/123 loss = 0.0457\n",
      "epoch 6 batch 17/123 loss = 0.0486\n",
      "epoch 6 batch 18/123 loss = 0.0479\n",
      "epoch 6 batch 19/123 loss = 0.0500\n",
      "epoch 6 batch 20/123 loss = 0.0456\n",
      "epoch 6 batch 21/123 loss = 0.0625\n",
      "epoch 6 batch 22/123 loss = 0.0497\n",
      "epoch 6 batch 23/123 loss = 0.0510\n",
      "epoch 6 batch 24/123 loss = 0.0452\n",
      "epoch 6 batch 25/123 loss = 0.0563\n",
      "epoch 6 batch 26/123 loss = 0.0647\n",
      "epoch 6 batch 27/123 loss = 0.0590\n",
      "epoch 6 batch 28/123 loss = 0.0594\n",
      "epoch 6 batch 29/123 loss = 0.0593\n",
      "epoch 6 batch 30/123 loss = 0.0650\n",
      "epoch 6 batch 31/123 loss = 0.0597\n",
      "epoch 6 batch 32/123 loss = 0.0559\n",
      "epoch 6 batch 33/123 loss = 0.0532\n",
      "epoch 6 batch 34/123 loss = 0.0604\n",
      "epoch 6 batch 35/123 loss = 0.0549\n",
      "epoch 6 batch 36/123 loss = 0.0588\n",
      "epoch 6 batch 37/123 loss = 0.0584\n",
      "epoch 6 batch 38/123 loss = 0.0528\n",
      "epoch 6 batch 39/123 loss = 0.0666\n",
      "epoch 6 batch 40/123 loss = 0.0525\n",
      "epoch 6 batch 41/123 loss = 0.0454\n",
      "epoch 6 batch 42/123 loss = 0.0488\n",
      "epoch 6 batch 43/123 loss = 0.0591\n",
      "epoch 6 batch 44/123 loss = 0.0499\n",
      "epoch 6 batch 45/123 loss = 0.0484\n",
      "epoch 6 batch 46/123 loss = 0.0636\n",
      "epoch 6 batch 47/123 loss = 0.0439\n",
      "epoch 6 batch 48/123 loss = 0.0530\n",
      "epoch 6 batch 49/123 loss = 0.0574\n",
      "epoch 6 batch 50/123 loss = 0.0514\n",
      "epoch 6 batch 51/123 loss = 0.0571\n",
      "epoch 6 batch 52/123 loss = 0.0552\n",
      "epoch 6 batch 53/123 loss = 0.0439\n",
      "epoch 6 batch 54/123 loss = 0.0563\n",
      "epoch 6 batch 55/123 loss = 0.0442\n",
      "epoch 6 batch 56/123 loss = 0.0601\n",
      "epoch 6 batch 57/123 loss = 0.0492\n",
      "epoch 6 batch 58/123 loss = 0.0495\n",
      "epoch 6 batch 59/123 loss = 0.0569\n",
      "epoch 6 batch 60/123 loss = 0.0492\n",
      "epoch 6 batch 61/123 loss = 0.0527\n",
      "epoch 6 batch 62/123 loss = 0.0515\n",
      "epoch 6 batch 63/123 loss = 0.0492\n",
      "epoch 6 batch 64/123 loss = 0.0605\n",
      "epoch 6 batch 65/123 loss = 0.0591\n",
      "epoch 6 batch 66/123 loss = 0.0522\n",
      "epoch 6 batch 67/123 loss = 0.0533\n",
      "epoch 6 batch 68/123 loss = 0.0517\n",
      "epoch 6 batch 69/123 loss = 0.0610\n",
      "epoch 6 batch 70/123 loss = 0.0508\n",
      "epoch 6 batch 71/123 loss = 0.0542\n",
      "epoch 6 batch 72/123 loss = 0.0561\n",
      "epoch 6 batch 73/123 loss = 0.0537\n",
      "epoch 6 batch 74/123 loss = 0.0629\n",
      "epoch 6 batch 75/123 loss = 0.0556\n",
      "epoch 6 batch 76/123 loss = 0.0547\n",
      "epoch 6 batch 77/123 loss = 0.0620\n",
      "epoch 6 batch 78/123 loss = 0.0613\n",
      "epoch 6 batch 79/123 loss = 0.0500\n",
      "epoch 6 batch 80/123 loss = 0.0511\n",
      "epoch 6 batch 81/123 loss = 0.0573\n",
      "epoch 6 batch 82/123 loss = 0.0542\n",
      "epoch 6 batch 83/123 loss = 0.0492\n",
      "epoch 6 batch 84/123 loss = 0.0725\n",
      "epoch 6 batch 85/123 loss = 0.0663\n",
      "epoch 6 batch 86/123 loss = 0.0579\n",
      "epoch 6 batch 87/123 loss = 0.0550\n",
      "epoch 6 batch 88/123 loss = 0.0610\n",
      "epoch 6 batch 89/123 loss = 0.0569\n",
      "epoch 6 batch 90/123 loss = 0.0516\n",
      "epoch 6 batch 91/123 loss = 0.0538\n",
      "epoch 6 batch 92/123 loss = 0.0466\n",
      "epoch 6 batch 93/123 loss = 0.0480\n",
      "epoch 6 batch 94/123 loss = 0.0468\n",
      "epoch 6 batch 95/123 loss = 0.0680\n",
      "epoch 6 batch 96/123 loss = 0.0544\n",
      "epoch 6 batch 97/123 loss = 0.0474\n",
      "epoch 6 batch 98/123 loss = 0.0595\n",
      "epoch 6 batch 99/123 loss = 0.0526\n",
      "epoch 6 batch 100/123 loss = 0.0546\n",
      "epoch 6 batch 101/123 loss = 0.0626\n",
      "epoch 6 batch 102/123 loss = 0.0537\n",
      "epoch 6 batch 103/123 loss = 0.0539\n",
      "epoch 6 batch 104/123 loss = 0.0472\n",
      "epoch 6 batch 105/123 loss = 0.0551\n",
      "epoch 6 batch 106/123 loss = 0.0466\n",
      "epoch 6 batch 107/123 loss = 0.0584\n",
      "epoch 6 batch 108/123 loss = 0.0536\n",
      "epoch 6 batch 109/123 loss = 0.0562\n",
      "epoch 6 batch 110/123 loss = 0.0630\n",
      "epoch 6 batch 111/123 loss = 0.0575\n",
      "epoch 6 batch 112/123 loss = 0.0458\n",
      "epoch 6 batch 113/123 loss = 0.0600\n",
      "epoch 6 batch 114/123 loss = 0.0681\n",
      "epoch 6 batch 115/123 loss = 0.0581\n",
      "epoch 6 batch 116/123 loss = 0.0457\n",
      "epoch 6 batch 117/123 loss = 0.0542\n",
      "epoch 6 batch 118/123 loss = 0.0607\n",
      "epoch 6 batch 119/123 loss = 0.0569\n",
      "epoch 6 batch 120/123 loss = 0.0583\n",
      "epoch 6 batch 121/123 loss = 0.0504\n",
      "epoch 6 batch 122/123 loss = 0.0500\n",
      "epoch 6 batch 29/123 average loss = 0.0550 last loss = 0.0592\n",
      "epoch 7 batch 1/123 loss = 0.0474\n",
      "epoch 7 batch 2/123 loss = 0.0459\n",
      "epoch 7 batch 3/123 loss = 0.0587\n",
      "epoch 7 batch 4/123 loss = 0.0584\n",
      "epoch 7 batch 5/123 loss = 0.0640\n",
      "epoch 7 batch 6/123 loss = 0.0527\n",
      "epoch 7 batch 7/123 loss = 0.0508\n",
      "epoch 7 batch 8/123 loss = 0.0533\n",
      "epoch 7 batch 9/123 loss = 0.0656\n",
      "epoch 7 batch 10/123 loss = 0.0554\n",
      "epoch 7 batch 11/123 loss = 0.0646\n",
      "epoch 7 batch 12/123 loss = 0.0496\n",
      "epoch 7 batch 13/123 loss = 0.0515\n",
      "epoch 7 batch 14/123 loss = 0.0462\n",
      "epoch 7 batch 15/123 loss = 0.0522\n",
      "epoch 7 batch 16/123 loss = 0.0523\n",
      "epoch 7 batch 17/123 loss = 0.0559\n",
      "epoch 7 batch 18/123 loss = 0.0507\n",
      "epoch 7 batch 19/123 loss = 0.0582\n",
      "epoch 7 batch 20/123 loss = 0.0593\n",
      "epoch 7 batch 21/123 loss = 0.0539\n",
      "epoch 7 batch 22/123 loss = 0.0515\n",
      "epoch 7 batch 23/123 loss = 0.0577\n",
      "epoch 7 batch 24/123 loss = 0.0485\n",
      "epoch 7 batch 25/123 loss = 0.0521\n",
      "epoch 7 batch 26/123 loss = 0.0584\n",
      "epoch 7 batch 27/123 loss = 0.0517\n",
      "epoch 7 batch 28/123 loss = 0.0487\n",
      "epoch 7 batch 29/123 loss = 0.0490\n",
      "epoch 7 batch 30/123 loss = 0.0554\n",
      "epoch 7 batch 31/123 loss = 0.0535\n",
      "epoch 7 batch 32/123 loss = 0.0442\n",
      "epoch 7 batch 33/123 loss = 0.0516\n",
      "epoch 7 batch 34/123 loss = 0.0556\n",
      "epoch 7 batch 35/123 loss = 0.0558\n",
      "epoch 7 batch 36/123 loss = 0.0550\n",
      "epoch 7 batch 37/123 loss = 0.0544\n",
      "epoch 7 batch 38/123 loss = 0.0528\n",
      "epoch 7 batch 39/123 loss = 0.0533\n",
      "epoch 7 batch 40/123 loss = 0.0625\n",
      "epoch 7 batch 41/123 loss = 0.0508\n",
      "epoch 7 batch 42/123 loss = 0.0519\n",
      "epoch 7 batch 43/123 loss = 0.0499\n",
      "epoch 7 batch 44/123 loss = 0.0479\n",
      "epoch 7 batch 45/123 loss = 0.0529\n",
      "epoch 7 batch 46/123 loss = 0.0536\n",
      "epoch 7 batch 47/123 loss = 0.0490\n",
      "epoch 7 batch 48/123 loss = 0.0548\n",
      "epoch 7 batch 49/123 loss = 0.0584\n",
      "epoch 7 batch 50/123 loss = 0.0490\n",
      "epoch 7 batch 51/123 loss = 0.0525\n",
      "epoch 7 batch 52/123 loss = 0.0442\n",
      "epoch 7 batch 53/123 loss = 0.0627\n",
      "epoch 7 batch 54/123 loss = 0.0651\n",
      "epoch 7 batch 55/123 loss = 0.0456\n",
      "epoch 7 batch 56/123 loss = 0.0472\n",
      "epoch 7 batch 57/123 loss = 0.0549\n",
      "epoch 7 batch 58/123 loss = 0.0528\n",
      "epoch 7 batch 59/123 loss = 0.0479\n",
      "epoch 7 batch 60/123 loss = 0.0556\n",
      "epoch 7 batch 61/123 loss = 0.0469\n",
      "epoch 7 batch 62/123 loss = 0.0484\n",
      "epoch 7 batch 63/123 loss = 0.0466\n",
      "epoch 7 batch 64/123 loss = 0.0494\n",
      "epoch 7 batch 65/123 loss = 0.0467\n",
      "epoch 7 batch 66/123 loss = 0.0518\n",
      "epoch 7 batch 67/123 loss = 0.0448\n",
      "epoch 7 batch 68/123 loss = 0.0494\n",
      "epoch 7 batch 69/123 loss = 0.0518\n",
      "epoch 7 batch 70/123 loss = 0.0431\n",
      "epoch 7 batch 71/123 loss = 0.0499\n",
      "epoch 7 batch 72/123 loss = 0.0496\n",
      "epoch 7 batch 73/123 loss = 0.0507\n",
      "epoch 7 batch 74/123 loss = 0.0512\n",
      "epoch 7 batch 75/123 loss = 0.0563\n",
      "epoch 7 batch 76/123 loss = 0.0533\n",
      "epoch 7 batch 77/123 loss = 0.0447\n",
      "epoch 7 batch 78/123 loss = 0.0526\n",
      "epoch 7 batch 79/123 loss = 0.0483\n",
      "epoch 7 batch 80/123 loss = 0.0549\n",
      "epoch 7 batch 81/123 loss = 0.0594\n",
      "epoch 7 batch 82/123 loss = 0.0489\n",
      "epoch 7 batch 83/123 loss = 0.0567\n",
      "epoch 7 batch 84/123 loss = 0.0517\n",
      "epoch 7 batch 85/123 loss = 0.0582\n",
      "epoch 7 batch 86/123 loss = 0.0480\n",
      "epoch 7 batch 87/123 loss = 0.0634\n",
      "epoch 7 batch 88/123 loss = 0.0459\n",
      "epoch 7 batch 89/123 loss = 0.0574\n",
      "epoch 7 batch 90/123 loss = 0.0458\n",
      "epoch 7 batch 91/123 loss = 0.0415\n",
      "epoch 7 batch 92/123 loss = 0.0538\n",
      "epoch 7 batch 93/123 loss = 0.0512\n",
      "epoch 7 batch 94/123 loss = 0.0458\n",
      "epoch 7 batch 95/123 loss = 0.0478\n",
      "epoch 7 batch 96/123 loss = 0.0471\n",
      "epoch 7 batch 97/123 loss = 0.0551\n",
      "epoch 7 batch 98/123 loss = 0.0508\n",
      "epoch 7 batch 99/123 loss = 0.0505\n",
      "epoch 7 batch 100/123 loss = 0.0580\n",
      "epoch 7 batch 101/123 loss = 0.0416\n",
      "epoch 7 batch 102/123 loss = 0.0481\n",
      "epoch 7 batch 103/123 loss = 0.0558\n",
      "epoch 7 batch 104/123 loss = 0.0473\n",
      "epoch 7 batch 105/123 loss = 0.0589\n",
      "epoch 7 batch 106/123 loss = 0.0618\n",
      "epoch 7 batch 107/123 loss = 0.0539\n",
      "epoch 7 batch 108/123 loss = 0.0494\n",
      "epoch 7 batch 109/123 loss = 0.0673\n",
      "epoch 7 batch 110/123 loss = 0.0541\n",
      "epoch 7 batch 111/123 loss = 0.0595\n",
      "epoch 7 batch 112/123 loss = 0.0597\n",
      "epoch 7 batch 113/123 loss = 0.0476\n",
      "epoch 7 batch 114/123 loss = 0.0501\n",
      "epoch 7 batch 115/123 loss = 0.0597\n",
      "epoch 7 batch 116/123 loss = 0.0514\n",
      "epoch 7 batch 117/123 loss = 0.0548\n",
      "epoch 7 batch 118/123 loss = 0.0477\n",
      "epoch 7 batch 119/123 loss = 0.0538\n",
      "epoch 7 batch 120/123 loss = 0.0532\n",
      "epoch 7 batch 121/123 loss = 0.0574\n",
      "epoch 7 batch 122/123 loss = 0.0415\n",
      "epoch 7 batch 29/123 average loss = 0.0525 last loss = 0.0548\n",
      "epoch 8 batch 1/123 loss = 0.0485\n",
      "epoch 8 batch 2/123 loss = 0.0562\n",
      "epoch 8 batch 3/123 loss = 0.0521\n",
      "epoch 8 batch 4/123 loss = 0.0451\n",
      "epoch 8 batch 5/123 loss = 0.0504\n",
      "epoch 8 batch 6/123 loss = 0.0419\n",
      "epoch 8 batch 7/123 loss = 0.0418\n",
      "epoch 8 batch 8/123 loss = 0.0427\n",
      "epoch 8 batch 9/123 loss = 0.0552\n",
      "epoch 8 batch 10/123 loss = 0.0580\n",
      "epoch 8 batch 11/123 loss = 0.0507\n",
      "epoch 8 batch 12/123 loss = 0.0490\n",
      "epoch 8 batch 13/123 loss = 0.0451\n",
      "epoch 8 batch 14/123 loss = 0.0432\n",
      "epoch 8 batch 15/123 loss = 0.0478\n",
      "epoch 8 batch 16/123 loss = 0.0559\n",
      "epoch 8 batch 17/123 loss = 0.0486\n",
      "epoch 8 batch 18/123 loss = 0.0465\n",
      "epoch 8 batch 19/123 loss = 0.0565\n",
      "epoch 8 batch 20/123 loss = 0.0541\n",
      "epoch 8 batch 21/123 loss = 0.0491\n",
      "epoch 8 batch 22/123 loss = 0.0466\n",
      "epoch 8 batch 23/123 loss = 0.0417\n",
      "epoch 8 batch 24/123 loss = 0.0523\n",
      "epoch 8 batch 25/123 loss = 0.0534\n",
      "epoch 8 batch 26/123 loss = 0.0550\n",
      "epoch 8 batch 27/123 loss = 0.0462\n",
      "epoch 8 batch 28/123 loss = 0.0396\n",
      "epoch 8 batch 29/123 loss = 0.0546\n",
      "epoch 8 batch 30/123 loss = 0.0507\n",
      "epoch 8 batch 31/123 loss = 0.0479\n",
      "epoch 8 batch 32/123 loss = 0.0566\n",
      "epoch 8 batch 33/123 loss = 0.0546\n",
      "epoch 8 batch 34/123 loss = 0.0486\n",
      "epoch 8 batch 35/123 loss = 0.0516\n",
      "epoch 8 batch 36/123 loss = 0.0588\n",
      "epoch 8 batch 37/123 loss = 0.0536\n",
      "epoch 8 batch 38/123 loss = 0.0563\n",
      "epoch 8 batch 39/123 loss = 0.0455\n",
      "epoch 8 batch 40/123 loss = 0.0558\n",
      "epoch 8 batch 41/123 loss = 0.0531\n",
      "epoch 8 batch 42/123 loss = 0.0559\n",
      "epoch 8 batch 43/123 loss = 0.0540\n",
      "epoch 8 batch 44/123 loss = 0.0551\n",
      "epoch 8 batch 45/123 loss = 0.0592\n",
      "epoch 8 batch 46/123 loss = 0.0457\n",
      "epoch 8 batch 47/123 loss = 0.0569\n",
      "epoch 8 batch 48/123 loss = 0.0519\n",
      "epoch 8 batch 49/123 loss = 0.0583\n",
      "epoch 8 batch 50/123 loss = 0.0519\n",
      "epoch 8 batch 51/123 loss = 0.0474\n",
      "epoch 8 batch 52/123 loss = 0.0472\n",
      "epoch 8 batch 53/123 loss = 0.0452\n",
      "epoch 8 batch 54/123 loss = 0.0507\n",
      "epoch 8 batch 55/123 loss = 0.0510\n",
      "epoch 8 batch 56/123 loss = 0.0478\n",
      "epoch 8 batch 57/123 loss = 0.0488\n",
      "epoch 8 batch 58/123 loss = 0.0513\n",
      "epoch 8 batch 59/123 loss = 0.0501\n",
      "epoch 8 batch 60/123 loss = 0.0457\n",
      "epoch 8 batch 61/123 loss = 0.0546\n",
      "epoch 8 batch 62/123 loss = 0.0604\n",
      "epoch 8 batch 63/123 loss = 0.0465\n",
      "epoch 8 batch 64/123 loss = 0.0513\n",
      "epoch 8 batch 65/123 loss = 0.0493\n",
      "epoch 8 batch 66/123 loss = 0.0545\n",
      "epoch 8 batch 67/123 loss = 0.0533\n",
      "epoch 8 batch 68/123 loss = 0.0518\n",
      "epoch 8 batch 69/123 loss = 0.0511\n",
      "epoch 8 batch 70/123 loss = 0.0491\n",
      "epoch 8 batch 71/123 loss = 0.0454\n",
      "epoch 8 batch 72/123 loss = 0.0497\n",
      "epoch 8 batch 73/123 loss = 0.0447\n",
      "epoch 8 batch 74/123 loss = 0.0500\n",
      "epoch 8 batch 75/123 loss = 0.0612\n",
      "epoch 8 batch 76/123 loss = 0.0446\n",
      "epoch 8 batch 77/123 loss = 0.0448\n",
      "epoch 8 batch 78/123 loss = 0.0493\n",
      "epoch 8 batch 79/123 loss = 0.0458\n",
      "epoch 8 batch 80/123 loss = 0.0552\n",
      "epoch 8 batch 81/123 loss = 0.0528\n",
      "epoch 8 batch 82/123 loss = 0.0528\n",
      "epoch 8 batch 83/123 loss = 0.0538\n",
      "epoch 8 batch 84/123 loss = 0.0477\n",
      "epoch 8 batch 85/123 loss = 0.0527\n",
      "epoch 8 batch 86/123 loss = 0.0487\n",
      "epoch 8 batch 87/123 loss = 0.0538\n",
      "epoch 8 batch 88/123 loss = 0.0487\n",
      "epoch 8 batch 89/123 loss = 0.0516\n",
      "epoch 8 batch 90/123 loss = 0.0414\n",
      "epoch 8 batch 91/123 loss = 0.0550\n",
      "epoch 8 batch 92/123 loss = 0.0455\n",
      "epoch 8 batch 93/123 loss = 0.0610\n",
      "epoch 8 batch 94/123 loss = 0.0566\n",
      "epoch 8 batch 95/123 loss = 0.0516\n",
      "epoch 8 batch 96/123 loss = 0.0522\n",
      "epoch 8 batch 97/123 loss = 0.0469\n",
      "epoch 8 batch 98/123 loss = 0.0490\n",
      "epoch 8 batch 99/123 loss = 0.0443\n",
      "epoch 8 batch 100/123 loss = 0.0620\n",
      "epoch 8 batch 101/123 loss = 0.0500\n",
      "epoch 8 batch 102/123 loss = 0.0439\n",
      "epoch 8 batch 103/123 loss = 0.0497\n",
      "epoch 8 batch 104/123 loss = 0.0566\n",
      "epoch 8 batch 105/123 loss = 0.0576\n",
      "epoch 8 batch 106/123 loss = 0.0496\n",
      "epoch 8 batch 107/123 loss = 0.0450\n",
      "epoch 8 batch 108/123 loss = 0.0518\n",
      "epoch 8 batch 109/123 loss = 0.0521\n",
      "epoch 8 batch 110/123 loss = 0.0473\n",
      "epoch 8 batch 111/123 loss = 0.0603\n",
      "epoch 8 batch 112/123 loss = 0.0468\n",
      "epoch 8 batch 113/123 loss = 0.0452\n",
      "epoch 8 batch 114/123 loss = 0.0606\n",
      "epoch 8 batch 115/123 loss = 0.0515\n",
      "epoch 8 batch 116/123 loss = 0.0549\n",
      "epoch 8 batch 117/123 loss = 0.0492\n",
      "epoch 8 batch 118/123 loss = 0.0503\n",
      "epoch 8 batch 119/123 loss = 0.0543\n",
      "epoch 8 batch 120/123 loss = 0.0471\n",
      "epoch 8 batch 121/123 loss = 0.0465\n",
      "epoch 8 batch 122/123 loss = 0.0442\n",
      "epoch 8 batch 29/123 average loss = 0.0507 last loss = 0.0467\n",
      "epoch 9 batch 1/123 loss = 0.0446\n",
      "epoch 9 batch 2/123 loss = 0.0528\n",
      "epoch 9 batch 3/123 loss = 0.0549\n",
      "epoch 9 batch 4/123 loss = 0.0471\n",
      "epoch 9 batch 5/123 loss = 0.0550\n",
      "epoch 9 batch 6/123 loss = 0.0544\n",
      "epoch 9 batch 7/123 loss = 0.0389\n",
      "epoch 9 batch 8/123 loss = 0.0486\n",
      "epoch 9 batch 9/123 loss = 0.0576\n",
      "epoch 9 batch 10/123 loss = 0.0480\n",
      "epoch 9 batch 11/123 loss = 0.0475\n",
      "epoch 9 batch 12/123 loss = 0.0470\n",
      "epoch 9 batch 13/123 loss = 0.0457\n",
      "epoch 9 batch 14/123 loss = 0.0426\n",
      "epoch 9 batch 15/123 loss = 0.0475\n",
      "epoch 9 batch 16/123 loss = 0.0504\n",
      "epoch 9 batch 17/123 loss = 0.0534\n",
      "epoch 9 batch 18/123 loss = 0.0529\n",
      "epoch 9 batch 19/123 loss = 0.0487\n",
      "epoch 9 batch 20/123 loss = 0.0535\n",
      "epoch 9 batch 21/123 loss = 0.0548\n",
      "epoch 9 batch 22/123 loss = 0.0476\n",
      "epoch 9 batch 23/123 loss = 0.0481\n",
      "epoch 9 batch 24/123 loss = 0.0589\n",
      "epoch 9 batch 25/123 loss = 0.0500\n",
      "epoch 9 batch 26/123 loss = 0.0604\n",
      "epoch 9 batch 27/123 loss = 0.0507\n",
      "epoch 9 batch 28/123 loss = 0.0426\n",
      "epoch 9 batch 29/123 loss = 0.0545\n",
      "epoch 9 batch 30/123 loss = 0.0430\n",
      "epoch 9 batch 31/123 loss = 0.0431\n",
      "epoch 9 batch 32/123 loss = 0.0475\n",
      "epoch 9 batch 33/123 loss = 0.0453\n",
      "epoch 9 batch 34/123 loss = 0.0459\n",
      "epoch 9 batch 35/123 loss = 0.0490\n",
      "epoch 9 batch 36/123 loss = 0.0507\n",
      "epoch 9 batch 37/123 loss = 0.0446\n",
      "epoch 9 batch 38/123 loss = 0.0554\n",
      "epoch 9 batch 39/123 loss = 0.0599\n",
      "epoch 9 batch 40/123 loss = 0.0501\n",
      "epoch 9 batch 41/123 loss = 0.0498\n",
      "epoch 9 batch 42/123 loss = 0.0479\n",
      "epoch 9 batch 43/123 loss = 0.0518\n",
      "epoch 9 batch 44/123 loss = 0.0491\n",
      "epoch 9 batch 45/123 loss = 0.0502\n",
      "epoch 9 batch 46/123 loss = 0.0572\n",
      "epoch 9 batch 47/123 loss = 0.0519\n",
      "epoch 9 batch 48/123 loss = 0.0547\n",
      "epoch 9 batch 49/123 loss = 0.0491\n",
      "epoch 9 batch 50/123 loss = 0.0484\n",
      "epoch 9 batch 51/123 loss = 0.0555\n",
      "epoch 9 batch 52/123 loss = 0.0495\n",
      "epoch 9 batch 53/123 loss = 0.0524\n",
      "epoch 9 batch 54/123 loss = 0.0495\n",
      "epoch 9 batch 55/123 loss = 0.0520\n",
      "epoch 9 batch 56/123 loss = 0.0489\n",
      "epoch 9 batch 57/123 loss = 0.0559\n",
      "epoch 9 batch 58/123 loss = 0.0455\n",
      "epoch 9 batch 59/123 loss = 0.0475\n",
      "epoch 9 batch 60/123 loss = 0.0456\n",
      "epoch 9 batch 61/123 loss = 0.0489\n",
      "epoch 9 batch 62/123 loss = 0.0396\n",
      "epoch 9 batch 63/123 loss = 0.0536\n",
      "epoch 9 batch 64/123 loss = 0.0540\n",
      "epoch 9 batch 65/123 loss = 0.0513\n",
      "epoch 9 batch 66/123 loss = 0.0526\n",
      "epoch 9 batch 67/123 loss = 0.0517\n",
      "epoch 9 batch 68/123 loss = 0.0521\n",
      "epoch 9 batch 69/123 loss = 0.0472\n",
      "epoch 9 batch 70/123 loss = 0.0407\n",
      "epoch 9 batch 71/123 loss = 0.0432\n",
      "epoch 9 batch 72/123 loss = 0.0493\n",
      "epoch 9 batch 73/123 loss = 0.0398\n",
      "epoch 9 batch 74/123 loss = 0.0451\n",
      "epoch 9 batch 75/123 loss = 0.0520\n",
      "epoch 9 batch 76/123 loss = 0.0502\n",
      "epoch 9 batch 77/123 loss = 0.0400\n",
      "epoch 9 batch 78/123 loss = 0.0443\n",
      "epoch 9 batch 79/123 loss = 0.0569\n",
      "epoch 9 batch 80/123 loss = 0.0382\n",
      "epoch 9 batch 81/123 loss = 0.0483\n",
      "epoch 9 batch 82/123 loss = 0.0434\n",
      "epoch 9 batch 83/123 loss = 0.0544\n",
      "epoch 9 batch 84/123 loss = 0.0582\n",
      "epoch 9 batch 85/123 loss = 0.0525\n",
      "epoch 9 batch 86/123 loss = 0.0473\n",
      "epoch 9 batch 87/123 loss = 0.0511\n",
      "epoch 9 batch 88/123 loss = 0.0570\n",
      "epoch 9 batch 89/123 loss = 0.0560\n",
      "epoch 9 batch 90/123 loss = 0.0563\n",
      "epoch 9 batch 91/123 loss = 0.0442\n",
      "epoch 9 batch 92/123 loss = 0.0455\n",
      "epoch 9 batch 93/123 loss = 0.0435\n",
      "epoch 9 batch 94/123 loss = 0.0416\n",
      "epoch 9 batch 95/123 loss = 0.0503\n",
      "epoch 9 batch 96/123 loss = 0.0409\n",
      "epoch 9 batch 97/123 loss = 0.0540\n",
      "epoch 9 batch 98/123 loss = 0.0463\n",
      "epoch 9 batch 99/123 loss = 0.0452\n",
      "epoch 9 batch 100/123 loss = 0.0448\n",
      "epoch 9 batch 101/123 loss = 0.0440\n",
      "epoch 9 batch 102/123 loss = 0.0447\n",
      "epoch 9 batch 103/123 loss = 0.0508\n",
      "epoch 9 batch 104/123 loss = 0.0482\n",
      "epoch 9 batch 105/123 loss = 0.0492\n",
      "epoch 9 batch 106/123 loss = 0.0483\n",
      "epoch 9 batch 107/123 loss = 0.0480\n",
      "epoch 9 batch 108/123 loss = 0.0528\n",
      "epoch 9 batch 109/123 loss = 0.0494\n",
      "epoch 9 batch 110/123 loss = 0.0494\n",
      "epoch 9 batch 111/123 loss = 0.0474\n",
      "epoch 9 batch 112/123 loss = 0.0422\n",
      "epoch 9 batch 113/123 loss = 0.0498\n",
      "epoch 9 batch 114/123 loss = 0.0459\n",
      "epoch 9 batch 115/123 loss = 0.0526\n",
      "epoch 9 batch 116/123 loss = 0.0469\n",
      "epoch 9 batch 117/123 loss = 0.0565\n",
      "epoch 9 batch 118/123 loss = 0.0460\n",
      "epoch 9 batch 119/123 loss = 0.0418\n",
      "epoch 9 batch 120/123 loss = 0.0497\n",
      "epoch 9 batch 121/123 loss = 0.0482\n",
      "epoch 9 batch 122/123 loss = 0.0554\n",
      "epoch 9 batch 29/123 average loss = 0.0492 last loss = 0.0485\n",
      "epoch 10 batch 1/123 loss = 0.0425\n",
      "epoch 10 batch 2/123 loss = 0.0449\n",
      "epoch 10 batch 3/123 loss = 0.0485\n",
      "epoch 10 batch 4/123 loss = 0.0568\n",
      "epoch 10 batch 5/123 loss = 0.0468\n",
      "epoch 10 batch 6/123 loss = 0.0531\n",
      "epoch 10 batch 7/123 loss = 0.0482\n",
      "epoch 10 batch 8/123 loss = 0.0500\n",
      "epoch 10 batch 9/123 loss = 0.0459\n",
      "epoch 10 batch 10/123 loss = 0.0532\n",
      "epoch 10 batch 11/123 loss = 0.0508\n",
      "epoch 10 batch 12/123 loss = 0.0362\n",
      "epoch 10 batch 13/123 loss = 0.0502\n",
      "epoch 10 batch 14/123 loss = 0.0652\n",
      "epoch 10 batch 15/123 loss = 0.0488\n",
      "epoch 10 batch 16/123 loss = 0.0437\n",
      "epoch 10 batch 17/123 loss = 0.0515\n",
      "epoch 10 batch 18/123 loss = 0.0441\n",
      "epoch 10 batch 19/123 loss = 0.0491\n",
      "epoch 10 batch 20/123 loss = 0.0565\n",
      "epoch 10 batch 21/123 loss = 0.0576\n",
      "epoch 10 batch 22/123 loss = 0.0533\n",
      "epoch 10 batch 23/123 loss = 0.0491\n",
      "epoch 10 batch 24/123 loss = 0.0417\n",
      "epoch 10 batch 25/123 loss = 0.0507\n",
      "epoch 10 batch 26/123 loss = 0.0445\n",
      "epoch 10 batch 27/123 loss = 0.0403\n",
      "epoch 10 batch 28/123 loss = 0.0493\n",
      "epoch 10 batch 29/123 loss = 0.0526\n",
      "epoch 10 batch 30/123 loss = 0.0409\n",
      "epoch 10 batch 31/123 loss = 0.0479\n",
      "epoch 10 batch 32/123 loss = 0.0472\n",
      "epoch 10 batch 33/123 loss = 0.0453\n",
      "epoch 10 batch 34/123 loss = 0.0440\n",
      "epoch 10 batch 35/123 loss = 0.0436\n",
      "epoch 10 batch 36/123 loss = 0.0476\n",
      "epoch 10 batch 37/123 loss = 0.0535\n",
      "epoch 10 batch 38/123 loss = 0.0486\n",
      "epoch 10 batch 39/123 loss = 0.0452\n",
      "epoch 10 batch 40/123 loss = 0.0574\n",
      "epoch 10 batch 41/123 loss = 0.0416\n",
      "epoch 10 batch 42/123 loss = 0.0500\n",
      "epoch 10 batch 43/123 loss = 0.0473\n",
      "epoch 10 batch 44/123 loss = 0.0456\n",
      "epoch 10 batch 45/123 loss = 0.0524\n",
      "epoch 10 batch 46/123 loss = 0.0507\n",
      "epoch 10 batch 47/123 loss = 0.0451\n",
      "epoch 10 batch 48/123 loss = 0.0551\n",
      "epoch 10 batch 49/123 loss = 0.0534\n",
      "epoch 10 batch 50/123 loss = 0.0439\n",
      "epoch 10 batch 51/123 loss = 0.0398\n",
      "epoch 10 batch 52/123 loss = 0.0556\n",
      "epoch 10 batch 53/123 loss = 0.0466\n",
      "epoch 10 batch 54/123 loss = 0.0566\n",
      "epoch 10 batch 55/123 loss = 0.0415\n",
      "epoch 10 batch 56/123 loss = 0.0429\n",
      "epoch 10 batch 57/123 loss = 0.0385\n",
      "epoch 10 batch 58/123 loss = 0.0522\n",
      "epoch 10 batch 59/123 loss = 0.0478\n",
      "epoch 10 batch 60/123 loss = 0.0432\n",
      "epoch 10 batch 61/123 loss = 0.0435\n",
      "epoch 10 batch 62/123 loss = 0.0498\n",
      "epoch 10 batch 63/123 loss = 0.0454\n",
      "epoch 10 batch 64/123 loss = 0.0443\n",
      "epoch 10 batch 65/123 loss = 0.0567\n",
      "epoch 10 batch 66/123 loss = 0.0538\n",
      "epoch 10 batch 67/123 loss = 0.0498\n",
      "epoch 10 batch 68/123 loss = 0.0564\n",
      "epoch 10 batch 69/123 loss = 0.0521\n",
      "epoch 10 batch 70/123 loss = 0.0426\n",
      "epoch 10 batch 71/123 loss = 0.0449\n",
      "epoch 10 batch 72/123 loss = 0.0485\n",
      "epoch 10 batch 73/123 loss = 0.0446\n",
      "epoch 10 batch 74/123 loss = 0.0564\n",
      "epoch 10 batch 75/123 loss = 0.0431\n",
      "epoch 10 batch 76/123 loss = 0.0504\n",
      "epoch 10 batch 77/123 loss = 0.0519\n",
      "epoch 10 batch 78/123 loss = 0.0474\n",
      "epoch 10 batch 79/123 loss = 0.0440\n",
      "epoch 10 batch 80/123 loss = 0.0477\n",
      "epoch 10 batch 81/123 loss = 0.0440\n",
      "epoch 10 batch 82/123 loss = 0.0513\n",
      "epoch 10 batch 83/123 loss = 0.0418\n",
      "epoch 10 batch 84/123 loss = 0.0465\n",
      "epoch 10 batch 85/123 loss = 0.0536\n",
      "epoch 10 batch 86/123 loss = 0.0481\n",
      "epoch 10 batch 87/123 loss = 0.0499\n",
      "epoch 10 batch 88/123 loss = 0.0469\n",
      "epoch 10 batch 89/123 loss = 0.0485\n",
      "epoch 10 batch 90/123 loss = 0.0545\n",
      "epoch 10 batch 91/123 loss = 0.0483\n",
      "epoch 10 batch 92/123 loss = 0.0465\n",
      "epoch 10 batch 93/123 loss = 0.0474\n",
      "epoch 10 batch 94/123 loss = 0.0441\n",
      "epoch 10 batch 95/123 loss = 0.0442\n",
      "epoch 10 batch 96/123 loss = 0.0615\n",
      "epoch 10 batch 97/123 loss = 0.0609\n",
      "epoch 10 batch 98/123 loss = 0.0478\n",
      "epoch 10 batch 99/123 loss = 0.0506\n",
      "epoch 10 batch 100/123 loss = 0.0432\n",
      "epoch 10 batch 101/123 loss = 0.0498\n",
      "epoch 10 batch 102/123 loss = 0.0480\n",
      "epoch 10 batch 103/123 loss = 0.0423\n",
      "epoch 10 batch 104/123 loss = 0.0484\n",
      "epoch 10 batch 105/123 loss = 0.0450\n",
      "epoch 10 batch 106/123 loss = 0.0496\n",
      "epoch 10 batch 107/123 loss = 0.0409\n",
      "epoch 10 batch 108/123 loss = 0.0441\n",
      "epoch 10 batch 109/123 loss = 0.0443\n",
      "epoch 10 batch 110/123 loss = 0.0514\n",
      "epoch 10 batch 111/123 loss = 0.0388\n",
      "epoch 10 batch 112/123 loss = 0.0474\n",
      "epoch 10 batch 113/123 loss = 0.0495\n",
      "epoch 10 batch 114/123 loss = 0.0548\n",
      "epoch 10 batch 115/123 loss = 0.0392\n",
      "epoch 10 batch 116/123 loss = 0.0411\n",
      "epoch 10 batch 117/123 loss = 0.0420\n",
      "epoch 10 batch 118/123 loss = 0.0468\n",
      "epoch 10 batch 119/123 loss = 0.0493\n",
      "epoch 10 batch 120/123 loss = 0.0484\n",
      "epoch 10 batch 121/123 loss = 0.0457\n",
      "epoch 10 batch 122/123 loss = 0.0499\n",
      "epoch 10 batch 29/123 average loss = 0.0481 last loss = 0.0517\n",
      "epoch 11 batch 1/123 loss = 0.0454\n",
      "epoch 11 batch 2/123 loss = 0.0472\n",
      "epoch 11 batch 3/123 loss = 0.0610\n",
      "epoch 11 batch 4/123 loss = 0.0547\n",
      "epoch 11 batch 5/123 loss = 0.0538\n",
      "epoch 11 batch 6/123 loss = 0.0513\n",
      "epoch 11 batch 7/123 loss = 0.0562\n",
      "epoch 11 batch 8/123 loss = 0.0508\n",
      "epoch 11 batch 9/123 loss = 0.0404\n",
      "epoch 11 batch 10/123 loss = 0.0467\n",
      "epoch 11 batch 11/123 loss = 0.0472\n",
      "epoch 11 batch 12/123 loss = 0.0438\n",
      "epoch 11 batch 13/123 loss = 0.0387\n",
      "epoch 11 batch 14/123 loss = 0.0413\n",
      "epoch 11 batch 15/123 loss = 0.0477\n",
      "epoch 11 batch 16/123 loss = 0.0512\n",
      "epoch 11 batch 17/123 loss = 0.0528\n",
      "epoch 11 batch 18/123 loss = 0.0439\n",
      "epoch 11 batch 19/123 loss = 0.0509\n",
      "epoch 11 batch 20/123 loss = 0.0471\n",
      "epoch 11 batch 21/123 loss = 0.0456\n",
      "epoch 11 batch 22/123 loss = 0.0442\n",
      "epoch 11 batch 23/123 loss = 0.0467\n",
      "epoch 11 batch 24/123 loss = 0.0499\n",
      "epoch 11 batch 25/123 loss = 0.0427\n",
      "epoch 11 batch 26/123 loss = 0.0510\n",
      "epoch 11 batch 27/123 loss = 0.0447\n",
      "epoch 11 batch 28/123 loss = 0.0490\n",
      "epoch 11 batch 29/123 loss = 0.0457\n",
      "epoch 11 batch 30/123 loss = 0.0433\n",
      "epoch 11 batch 31/123 loss = 0.0501\n",
      "epoch 11 batch 32/123 loss = 0.0478\n",
      "epoch 11 batch 33/123 loss = 0.0485\n",
      "epoch 11 batch 34/123 loss = 0.0446\n",
      "epoch 11 batch 35/123 loss = 0.0427\n",
      "epoch 11 batch 36/123 loss = 0.0441\n",
      "epoch 11 batch 37/123 loss = 0.0450\n",
      "epoch 11 batch 38/123 loss = 0.0431\n",
      "epoch 11 batch 39/123 loss = 0.0527\n",
      "epoch 11 batch 40/123 loss = 0.0486\n",
      "epoch 11 batch 41/123 loss = 0.0471\n",
      "epoch 11 batch 42/123 loss = 0.0433\n",
      "epoch 11 batch 43/123 loss = 0.0421\n",
      "epoch 11 batch 44/123 loss = 0.0447\n",
      "epoch 11 batch 45/123 loss = 0.0481\n",
      "epoch 11 batch 46/123 loss = 0.0500\n",
      "epoch 11 batch 47/123 loss = 0.0469\n",
      "epoch 11 batch 48/123 loss = 0.0531\n",
      "epoch 11 batch 49/123 loss = 0.0522\n",
      "epoch 11 batch 50/123 loss = 0.0442\n",
      "epoch 11 batch 51/123 loss = 0.0459\n",
      "epoch 11 batch 52/123 loss = 0.0440\n",
      "epoch 11 batch 53/123 loss = 0.0511\n",
      "epoch 11 batch 54/123 loss = 0.0654\n",
      "epoch 11 batch 55/123 loss = 0.0527\n",
      "epoch 11 batch 56/123 loss = 0.0482\n",
      "epoch 11 batch 57/123 loss = 0.0447\n",
      "epoch 11 batch 58/123 loss = 0.0522\n",
      "epoch 11 batch 59/123 loss = 0.0479\n",
      "epoch 11 batch 60/123 loss = 0.0424\n",
      "epoch 11 batch 61/123 loss = 0.0360\n",
      "epoch 11 batch 62/123 loss = 0.0528\n",
      "epoch 11 batch 63/123 loss = 0.0446\n",
      "epoch 11 batch 64/123 loss = 0.0444\n",
      "epoch 11 batch 65/123 loss = 0.0476\n",
      "epoch 11 batch 66/123 loss = 0.0488\n",
      "epoch 11 batch 67/123 loss = 0.0521\n",
      "epoch 11 batch 68/123 loss = 0.0545\n",
      "epoch 11 batch 69/123 loss = 0.0456\n",
      "epoch 11 batch 70/123 loss = 0.0402\n",
      "epoch 11 batch 71/123 loss = 0.0412\n",
      "epoch 11 batch 72/123 loss = 0.0475\n",
      "epoch 11 batch 73/123 loss = 0.0389\n",
      "epoch 11 batch 74/123 loss = 0.0542\n",
      "epoch 11 batch 75/123 loss = 0.0441\n",
      "epoch 11 batch 76/123 loss = 0.0613\n",
      "epoch 11 batch 77/123 loss = 0.0450\n",
      "epoch 11 batch 78/123 loss = 0.0525\n",
      "epoch 11 batch 79/123 loss = 0.0604\n",
      "epoch 11 batch 80/123 loss = 0.0479\n",
      "epoch 11 batch 81/123 loss = 0.0475\n",
      "epoch 11 batch 82/123 loss = 0.0513\n",
      "epoch 11 batch 83/123 loss = 0.0471\n",
      "epoch 11 batch 84/123 loss = 0.0445\n",
      "epoch 11 batch 85/123 loss = 0.0510\n",
      "epoch 11 batch 86/123 loss = 0.0400\n",
      "epoch 11 batch 87/123 loss = 0.0428\n",
      "epoch 11 batch 88/123 loss = 0.0449\n",
      "epoch 11 batch 89/123 loss = 0.0445\n",
      "epoch 11 batch 90/123 loss = 0.0449\n",
      "epoch 11 batch 91/123 loss = 0.0500\n",
      "epoch 11 batch 92/123 loss = 0.0375\n",
      "epoch 11 batch 93/123 loss = 0.0396\n",
      "epoch 11 batch 94/123 loss = 0.0393\n",
      "epoch 11 batch 95/123 loss = 0.0373\n",
      "epoch 11 batch 96/123 loss = 0.0457\n",
      "epoch 11 batch 97/123 loss = 0.0480\n",
      "epoch 11 batch 98/123 loss = 0.0478\n",
      "epoch 11 batch 99/123 loss = 0.0395\n",
      "epoch 11 batch 100/123 loss = 0.0469\n",
      "epoch 11 batch 101/123 loss = 0.0416\n",
      "epoch 11 batch 102/123 loss = 0.0413\n",
      "epoch 11 batch 103/123 loss = 0.0381\n",
      "epoch 11 batch 104/123 loss = 0.0503\n",
      "epoch 11 batch 105/123 loss = 0.0441\n",
      "epoch 11 batch 106/123 loss = 0.0490\n",
      "epoch 11 batch 107/123 loss = 0.0493\n",
      "epoch 11 batch 108/123 loss = 0.0451\n",
      "epoch 11 batch 109/123 loss = 0.0487\n",
      "epoch 11 batch 110/123 loss = 0.0497\n",
      "epoch 11 batch 111/123 loss = 0.0505\n",
      "epoch 11 batch 112/123 loss = 0.0425\n",
      "epoch 11 batch 113/123 loss = 0.0444\n",
      "epoch 11 batch 114/123 loss = 0.0477\n",
      "epoch 11 batch 115/123 loss = 0.0513\n",
      "epoch 11 batch 116/123 loss = 0.0587\n",
      "epoch 11 batch 117/123 loss = 0.0538\n",
      "epoch 11 batch 118/123 loss = 0.0505\n",
      "epoch 11 batch 119/123 loss = 0.0452\n",
      "epoch 11 batch 120/123 loss = 0.0398\n",
      "epoch 11 batch 121/123 loss = 0.0492\n",
      "epoch 11 batch 122/123 loss = 0.0427\n",
      "epoch 11 batch 29/123 average loss = 0.0471 last loss = 0.0427\n",
      "epoch 12 batch 1/123 loss = 0.0428\n",
      "epoch 12 batch 2/123 loss = 0.0482\n",
      "epoch 12 batch 3/123 loss = 0.0517\n",
      "epoch 12 batch 4/123 loss = 0.0412\n",
      "epoch 12 batch 5/123 loss = 0.0436\n",
      "epoch 12 batch 6/123 loss = 0.0368\n",
      "epoch 12 batch 7/123 loss = 0.0450\n",
      "epoch 12 batch 8/123 loss = 0.0438\n",
      "epoch 12 batch 9/123 loss = 0.0450\n",
      "epoch 12 batch 10/123 loss = 0.0481\n",
      "epoch 12 batch 11/123 loss = 0.0475\n",
      "epoch 12 batch 12/123 loss = 0.0467\n",
      "epoch 12 batch 13/123 loss = 0.0379\n",
      "epoch 12 batch 14/123 loss = 0.0445\n",
      "epoch 12 batch 15/123 loss = 0.0524\n",
      "epoch 12 batch 16/123 loss = 0.0494\n",
      "epoch 12 batch 17/123 loss = 0.0358\n",
      "epoch 12 batch 18/123 loss = 0.0469\n",
      "epoch 12 batch 19/123 loss = 0.0422\n",
      "epoch 12 batch 20/123 loss = 0.0450\n",
      "epoch 12 batch 21/123 loss = 0.0445\n",
      "epoch 12 batch 22/123 loss = 0.0396\n",
      "epoch 12 batch 23/123 loss = 0.0467\n",
      "epoch 12 batch 24/123 loss = 0.0461\n",
      "epoch 12 batch 25/123 loss = 0.0482\n",
      "epoch 12 batch 26/123 loss = 0.0420\n",
      "epoch 12 batch 27/123 loss = 0.0458\n",
      "epoch 12 batch 28/123 loss = 0.0415\n",
      "epoch 12 batch 29/123 loss = 0.0429\n",
      "epoch 12 batch 30/123 loss = 0.0415\n",
      "epoch 12 batch 31/123 loss = 0.0437\n",
      "epoch 12 batch 32/123 loss = 0.0465\n",
      "epoch 12 batch 33/123 loss = 0.0474\n",
      "epoch 12 batch 34/123 loss = 0.0460\n",
      "epoch 12 batch 35/123 loss = 0.0440\n",
      "epoch 12 batch 36/123 loss = 0.0387\n",
      "epoch 12 batch 37/123 loss = 0.0427\n",
      "epoch 12 batch 38/123 loss = 0.0462\n",
      "epoch 12 batch 39/123 loss = 0.0342\n",
      "epoch 12 batch 40/123 loss = 0.0442\n",
      "epoch 12 batch 41/123 loss = 0.0454\n",
      "epoch 12 batch 42/123 loss = 0.0408\n",
      "epoch 12 batch 43/123 loss = 0.0535\n",
      "epoch 12 batch 44/123 loss = 0.0522\n",
      "epoch 12 batch 45/123 loss = 0.0431\n",
      "epoch 12 batch 46/123 loss = 0.0532\n",
      "epoch 12 batch 47/123 loss = 0.0432\n",
      "epoch 12 batch 48/123 loss = 0.0458\n",
      "epoch 12 batch 49/123 loss = 0.0419\n",
      "epoch 12 batch 50/123 loss = 0.0568\n",
      "epoch 12 batch 51/123 loss = 0.0481\n",
      "epoch 12 batch 52/123 loss = 0.0484\n",
      "epoch 12 batch 53/123 loss = 0.0419\n",
      "epoch 12 batch 54/123 loss = 0.0469\n",
      "epoch 12 batch 55/123 loss = 0.0476\n",
      "epoch 12 batch 56/123 loss = 0.0443\n",
      "epoch 12 batch 57/123 loss = 0.0431\n",
      "epoch 12 batch 58/123 loss = 0.0392\n",
      "epoch 12 batch 59/123 loss = 0.0555\n",
      "epoch 12 batch 60/123 loss = 0.0527\n",
      "epoch 12 batch 61/123 loss = 0.0447\n",
      "epoch 12 batch 62/123 loss = 0.0431\n",
      "epoch 12 batch 63/123 loss = 0.0451\n",
      "epoch 12 batch 64/123 loss = 0.0474\n",
      "epoch 12 batch 65/123 loss = 0.0427\n",
      "epoch 12 batch 66/123 loss = 0.0543\n",
      "epoch 12 batch 67/123 loss = 0.0482\n",
      "epoch 12 batch 68/123 loss = 0.0391\n",
      "epoch 12 batch 69/123 loss = 0.0509\n",
      "epoch 12 batch 70/123 loss = 0.0468\n",
      "epoch 12 batch 71/123 loss = 0.0478\n",
      "epoch 12 batch 72/123 loss = 0.0443\n",
      "epoch 12 batch 73/123 loss = 0.0481\n",
      "epoch 12 batch 74/123 loss = 0.0475\n",
      "epoch 12 batch 75/123 loss = 0.0469\n",
      "epoch 12 batch 76/123 loss = 0.0433\n",
      "epoch 12 batch 77/123 loss = 0.0458\n",
      "epoch 12 batch 78/123 loss = 0.0458\n",
      "epoch 12 batch 79/123 loss = 0.0559\n",
      "epoch 12 batch 80/123 loss = 0.0457\n",
      "epoch 12 batch 81/123 loss = 0.0377\n",
      "epoch 12 batch 82/123 loss = 0.0443\n",
      "epoch 12 batch 83/123 loss = 0.0479\n",
      "epoch 12 batch 84/123 loss = 0.0407\n",
      "epoch 12 batch 85/123 loss = 0.0330\n",
      "epoch 12 batch 86/123 loss = 0.0468\n",
      "epoch 12 batch 87/123 loss = 0.0436\n",
      "epoch 12 batch 88/123 loss = 0.0402\n",
      "epoch 12 batch 89/123 loss = 0.0471\n",
      "epoch 12 batch 90/123 loss = 0.0399\n",
      "epoch 12 batch 91/123 loss = 0.0440\n",
      "epoch 12 batch 92/123 loss = 0.0511\n",
      "epoch 12 batch 93/123 loss = 0.0432\n",
      "epoch 12 batch 94/123 loss = 0.0518\n",
      "epoch 12 batch 95/123 loss = 0.0440\n",
      "epoch 12 batch 96/123 loss = 0.0425\n",
      "epoch 12 batch 97/123 loss = 0.0392\n",
      "epoch 12 batch 98/123 loss = 0.0445\n",
      "epoch 12 batch 99/123 loss = 0.0492\n",
      "epoch 12 batch 100/123 loss = 0.0437\n",
      "epoch 12 batch 101/123 loss = 0.0484\n",
      "epoch 12 batch 102/123 loss = 0.0467\n",
      "epoch 12 batch 103/123 loss = 0.0486\n",
      "epoch 12 batch 104/123 loss = 0.0535\n",
      "epoch 12 batch 105/123 loss = 0.0509\n",
      "epoch 12 batch 106/123 loss = 0.0533\n",
      "epoch 12 batch 107/123 loss = 0.0450\n",
      "epoch 12 batch 108/123 loss = 0.0456\n",
      "epoch 12 batch 109/123 loss = 0.0465\n",
      "epoch 12 batch 110/123 loss = 0.0460\n",
      "epoch 12 batch 111/123 loss = 0.0570\n",
      "epoch 12 batch 112/123 loss = 0.0437\n",
      "epoch 12 batch 113/123 loss = 0.0421\n",
      "epoch 12 batch 114/123 loss = 0.0441\n",
      "epoch 12 batch 115/123 loss = 0.0482\n",
      "epoch 12 batch 116/123 loss = 0.0465\n",
      "epoch 12 batch 117/123 loss = 0.0439\n",
      "epoch 12 batch 118/123 loss = 0.0444\n",
      "epoch 12 batch 119/123 loss = 0.0512\n",
      "epoch 12 batch 120/123 loss = 0.0610\n",
      "epoch 12 batch 121/123 loss = 0.0456\n",
      "epoch 12 batch 122/123 loss = 0.0448\n",
      "epoch 12 batch 29/123 average loss = 0.0457 last loss = 0.0441\n",
      "epoch 13 batch 1/123 loss = 0.0538\n",
      "epoch 13 batch 2/123 loss = 0.0449\n",
      "epoch 13 batch 3/123 loss = 0.0409\n",
      "epoch 13 batch 4/123 loss = 0.0370\n",
      "epoch 13 batch 5/123 loss = 0.0417\n",
      "epoch 13 batch 6/123 loss = 0.0471\n",
      "epoch 13 batch 7/123 loss = 0.0488\n",
      "epoch 13 batch 8/123 loss = 0.0567\n",
      "epoch 13 batch 9/123 loss = 0.0401\n",
      "epoch 13 batch 10/123 loss = 0.0414\n",
      "epoch 13 batch 11/123 loss = 0.0479\n",
      "epoch 13 batch 12/123 loss = 0.0474\n",
      "epoch 13 batch 13/123 loss = 0.0563\n",
      "epoch 13 batch 14/123 loss = 0.0499\n",
      "epoch 13 batch 15/123 loss = 0.0467\n",
      "epoch 13 batch 16/123 loss = 0.0460\n",
      "epoch 13 batch 17/123 loss = 0.0415\n",
      "epoch 13 batch 18/123 loss = 0.0480\n",
      "epoch 13 batch 19/123 loss = 0.0375\n",
      "epoch 13 batch 20/123 loss = 0.0439\n",
      "epoch 13 batch 21/123 loss = 0.0576\n",
      "epoch 13 batch 22/123 loss = 0.0405\n",
      "epoch 13 batch 23/123 loss = 0.0440\n",
      "epoch 13 batch 24/123 loss = 0.0404\n",
      "epoch 13 batch 25/123 loss = 0.0430\n",
      "epoch 13 batch 26/123 loss = 0.0401\n",
      "epoch 13 batch 27/123 loss = 0.0493\n",
      "epoch 13 batch 28/123 loss = 0.0508\n",
      "epoch 13 batch 29/123 loss = 0.0468\n",
      "epoch 13 batch 30/123 loss = 0.0426\n",
      "epoch 13 batch 31/123 loss = 0.0421\n",
      "epoch 13 batch 32/123 loss = 0.0410\n",
      "epoch 13 batch 33/123 loss = 0.0395\n",
      "epoch 13 batch 34/123 loss = 0.0430\n",
      "epoch 13 batch 35/123 loss = 0.0373\n",
      "epoch 13 batch 36/123 loss = 0.0335\n",
      "epoch 13 batch 37/123 loss = 0.0408\n",
      "epoch 13 batch 38/123 loss = 0.0420\n",
      "epoch 13 batch 39/123 loss = 0.0430\n",
      "epoch 13 batch 40/123 loss = 0.0472\n",
      "epoch 13 batch 41/123 loss = 0.0454\n",
      "epoch 13 batch 42/123 loss = 0.0427\n",
      "epoch 13 batch 43/123 loss = 0.0458\n",
      "epoch 13 batch 44/123 loss = 0.0493\n",
      "epoch 13 batch 45/123 loss = 0.0440\n",
      "epoch 13 batch 46/123 loss = 0.0432\n",
      "epoch 13 batch 47/123 loss = 0.0479\n",
      "epoch 13 batch 48/123 loss = 0.0454\n",
      "epoch 13 batch 49/123 loss = 0.0402\n",
      "epoch 13 batch 50/123 loss = 0.0378\n",
      "epoch 13 batch 51/123 loss = 0.0415\n",
      "epoch 13 batch 52/123 loss = 0.0426\n",
      "epoch 13 batch 53/123 loss = 0.0516\n",
      "epoch 13 batch 54/123 loss = 0.0431\n",
      "epoch 13 batch 55/123 loss = 0.0506\n",
      "epoch 13 batch 56/123 loss = 0.0477\n",
      "epoch 13 batch 57/123 loss = 0.0426\n",
      "epoch 13 batch 58/123 loss = 0.0418\n",
      "epoch 13 batch 59/123 loss = 0.0493\n",
      "epoch 13 batch 60/123 loss = 0.0433\n",
      "epoch 13 batch 61/123 loss = 0.0449\n",
      "epoch 13 batch 62/123 loss = 0.0414\n",
      "epoch 13 batch 63/123 loss = 0.0394\n",
      "epoch 13 batch 64/123 loss = 0.0477\n",
      "epoch 13 batch 65/123 loss = 0.0555\n",
      "epoch 13 batch 66/123 loss = 0.0439\n",
      "epoch 13 batch 67/123 loss = 0.0419\n",
      "epoch 13 batch 68/123 loss = 0.0574\n",
      "epoch 13 batch 69/123 loss = 0.0427\n",
      "epoch 13 batch 70/123 loss = 0.0424\n",
      "epoch 13 batch 71/123 loss = 0.0482\n",
      "epoch 13 batch 72/123 loss = 0.0359\n",
      "epoch 13 batch 73/123 loss = 0.0396\n",
      "epoch 13 batch 74/123 loss = 0.0465\n",
      "epoch 13 batch 75/123 loss = 0.0471\n",
      "epoch 13 batch 76/123 loss = 0.0435\n",
      "epoch 13 batch 77/123 loss = 0.0376\n",
      "epoch 13 batch 78/123 loss = 0.0441\n",
      "epoch 13 batch 79/123 loss = 0.0386\n",
      "epoch 13 batch 80/123 loss = 0.0453\n",
      "epoch 13 batch 81/123 loss = 0.0403\n",
      "epoch 13 batch 82/123 loss = 0.0408\n",
      "epoch 13 batch 83/123 loss = 0.0445\n",
      "epoch 13 batch 84/123 loss = 0.0416\n",
      "epoch 13 batch 85/123 loss = 0.0397\n",
      "epoch 13 batch 86/123 loss = 0.0530\n",
      "epoch 13 batch 87/123 loss = 0.0420\n",
      "epoch 13 batch 88/123 loss = 0.0477\n",
      "epoch 13 batch 89/123 loss = 0.0504\n",
      "epoch 13 batch 90/123 loss = 0.0423\n",
      "epoch 13 batch 91/123 loss = 0.0389\n",
      "epoch 13 batch 92/123 loss = 0.0426\n",
      "epoch 13 batch 93/123 loss = 0.0428\n",
      "epoch 13 batch 94/123 loss = 0.0411\n",
      "epoch 13 batch 95/123 loss = 0.0450\n",
      "epoch 13 batch 96/123 loss = 0.0424\n",
      "epoch 13 batch 97/123 loss = 0.0459\n",
      "epoch 13 batch 98/123 loss = 0.0404\n",
      "epoch 13 batch 99/123 loss = 0.0407\n",
      "epoch 13 batch 100/123 loss = 0.0421\n",
      "epoch 13 batch 101/123 loss = 0.0430\n",
      "epoch 13 batch 102/123 loss = 0.0396\n",
      "epoch 13 batch 103/123 loss = 0.0443\n",
      "epoch 13 batch 104/123 loss = 0.0488\n",
      "epoch 13 batch 105/123 loss = 0.0439\n",
      "epoch 13 batch 106/123 loss = 0.0424\n",
      "epoch 13 batch 107/123 loss = 0.0459\n",
      "epoch 13 batch 108/123 loss = 0.0541\n",
      "epoch 13 batch 109/123 loss = 0.0405\n",
      "epoch 13 batch 110/123 loss = 0.0469\n",
      "epoch 13 batch 111/123 loss = 0.0474\n",
      "epoch 13 batch 112/123 loss = 0.0406\n",
      "epoch 13 batch 113/123 loss = 0.0435\n",
      "epoch 13 batch 114/123 loss = 0.0456\n",
      "epoch 13 batch 115/123 loss = 0.0517\n",
      "epoch 13 batch 116/123 loss = 0.0412\n",
      "epoch 13 batch 117/123 loss = 0.0374\n",
      "epoch 13 batch 118/123 loss = 0.0458\n",
      "epoch 13 batch 119/123 loss = 0.0432\n",
      "epoch 13 batch 120/123 loss = 0.0414\n",
      "epoch 13 batch 121/123 loss = 0.0425\n",
      "epoch 13 batch 122/123 loss = 0.0484\n",
      "epoch 13 batch 29/123 average loss = 0.0443 last loss = 0.0447\n",
      "epoch 14 batch 1/123 loss = 0.0374\n",
      "epoch 14 batch 2/123 loss = 0.0434\n",
      "epoch 14 batch 3/123 loss = 0.0374\n",
      "epoch 14 batch 4/123 loss = 0.0456\n",
      "epoch 14 batch 5/123 loss = 0.0379\n",
      "epoch 14 batch 6/123 loss = 0.0369\n",
      "epoch 14 batch 7/123 loss = 0.0533\n",
      "epoch 14 batch 8/123 loss = 0.0438\n",
      "epoch 14 batch 9/123 loss = 0.0456\n",
      "epoch 14 batch 10/123 loss = 0.0493\n",
      "epoch 14 batch 11/123 loss = 0.0424\n",
      "epoch 14 batch 12/123 loss = 0.0433\n",
      "epoch 14 batch 13/123 loss = 0.0447\n",
      "epoch 14 batch 14/123 loss = 0.0388\n",
      "epoch 14 batch 15/123 loss = 0.0395\n",
      "epoch 14 batch 16/123 loss = 0.0407\n",
      "epoch 14 batch 17/123 loss = 0.0520\n",
      "epoch 14 batch 18/123 loss = 0.0402\n",
      "epoch 14 batch 19/123 loss = 0.0452\n",
      "epoch 14 batch 20/123 loss = 0.0397\n",
      "epoch 14 batch 21/123 loss = 0.0412\n",
      "epoch 14 batch 22/123 loss = 0.0448\n",
      "epoch 14 batch 23/123 loss = 0.0496\n",
      "epoch 14 batch 24/123 loss = 0.0383\n",
      "epoch 14 batch 25/123 loss = 0.0382\n",
      "epoch 14 batch 26/123 loss = 0.0405\n",
      "epoch 14 batch 27/123 loss = 0.0441\n",
      "epoch 14 batch 28/123 loss = 0.0430\n",
      "epoch 14 batch 29/123 loss = 0.0411\n",
      "epoch 14 batch 30/123 loss = 0.0439\n",
      "epoch 14 batch 31/123 loss = 0.0402\n",
      "epoch 14 batch 32/123 loss = 0.0421\n",
      "epoch 14 batch 33/123 loss = 0.0418\n",
      "epoch 14 batch 34/123 loss = 0.0421\n",
      "epoch 14 batch 35/123 loss = 0.0440\n",
      "epoch 14 batch 36/123 loss = 0.0364\n",
      "epoch 14 batch 37/123 loss = 0.0587\n",
      "epoch 14 batch 38/123 loss = 0.0486\n",
      "epoch 14 batch 39/123 loss = 0.0474\n",
      "epoch 14 batch 40/123 loss = 0.0462\n",
      "epoch 14 batch 41/123 loss = 0.0423\n",
      "epoch 14 batch 42/123 loss = 0.0403\n",
      "epoch 14 batch 43/123 loss = 0.0446\n",
      "epoch 14 batch 44/123 loss = 0.0451\n",
      "epoch 14 batch 45/123 loss = 0.0410\n",
      "epoch 14 batch 46/123 loss = 0.0412\n",
      "epoch 14 batch 47/123 loss = 0.0463\n",
      "epoch 14 batch 48/123 loss = 0.0473\n",
      "epoch 14 batch 49/123 loss = 0.0389\n",
      "epoch 14 batch 50/123 loss = 0.0382\n",
      "epoch 14 batch 51/123 loss = 0.0459\n",
      "epoch 14 batch 52/123 loss = 0.0457\n",
      "epoch 14 batch 53/123 loss = 0.0413\n",
      "epoch 14 batch 54/123 loss = 0.0511\n",
      "epoch 14 batch 55/123 loss = 0.0418\n",
      "epoch 14 batch 56/123 loss = 0.0455\n",
      "epoch 14 batch 57/123 loss = 0.0494\n",
      "epoch 14 batch 58/123 loss = 0.0364\n",
      "epoch 14 batch 59/123 loss = 0.0393\n",
      "epoch 14 batch 60/123 loss = 0.0447\n",
      "epoch 14 batch 61/123 loss = 0.0433\n",
      "epoch 14 batch 62/123 loss = 0.0526\n",
      "epoch 14 batch 63/123 loss = 0.0433\n",
      "epoch 14 batch 64/123 loss = 0.0422\n",
      "epoch 14 batch 65/123 loss = 0.0415\n",
      "epoch 14 batch 66/123 loss = 0.0518\n",
      "epoch 14 batch 67/123 loss = 0.0432\n",
      "epoch 14 batch 68/123 loss = 0.0456\n",
      "epoch 14 batch 69/123 loss = 0.0474\n",
      "epoch 14 batch 70/123 loss = 0.0490\n",
      "epoch 14 batch 71/123 loss = 0.0457\n",
      "epoch 14 batch 72/123 loss = 0.0506\n",
      "epoch 14 batch 73/123 loss = 0.0443\n",
      "epoch 14 batch 74/123 loss = 0.0468\n",
      "epoch 14 batch 75/123 loss = 0.0414\n",
      "epoch 14 batch 76/123 loss = 0.0485\n",
      "epoch 14 batch 77/123 loss = 0.0422\n",
      "epoch 14 batch 78/123 loss = 0.0400\n",
      "epoch 14 batch 79/123 loss = 0.0385\n",
      "epoch 14 batch 80/123 loss = 0.0404\n",
      "epoch 14 batch 81/123 loss = 0.0392\n",
      "epoch 14 batch 82/123 loss = 0.0460\n",
      "epoch 14 batch 83/123 loss = 0.0496\n",
      "epoch 14 batch 84/123 loss = 0.0448\n",
      "epoch 14 batch 85/123 loss = 0.0496\n",
      "epoch 14 batch 86/123 loss = 0.0384\n",
      "epoch 14 batch 87/123 loss = 0.0463\n",
      "epoch 14 batch 88/123 loss = 0.0348\n",
      "epoch 14 batch 89/123 loss = 0.0420\n",
      "epoch 14 batch 90/123 loss = 0.0397\n",
      "epoch 14 batch 91/123 loss = 0.0421\n",
      "epoch 14 batch 92/123 loss = 0.0471\n",
      "epoch 14 batch 93/123 loss = 0.0382\n",
      "epoch 14 batch 94/123 loss = 0.0444\n",
      "epoch 14 batch 95/123 loss = 0.0454\n",
      "epoch 14 batch 96/123 loss = 0.0397\n",
      "epoch 14 batch 97/123 loss = 0.0400\n",
      "epoch 14 batch 98/123 loss = 0.0401\n",
      "epoch 14 batch 99/123 loss = 0.0341\n",
      "epoch 14 batch 100/123 loss = 0.0432\n",
      "epoch 14 batch 101/123 loss = 0.0409\n",
      "epoch 14 batch 102/123 loss = 0.0409\n",
      "epoch 14 batch 103/123 loss = 0.0491\n",
      "epoch 14 batch 104/123 loss = 0.0458\n",
      "epoch 14 batch 105/123 loss = 0.0459\n",
      "epoch 14 batch 106/123 loss = 0.0445\n",
      "epoch 14 batch 107/123 loss = 0.0409\n",
      "epoch 14 batch 108/123 loss = 0.0421\n",
      "epoch 14 batch 109/123 loss = 0.0492\n",
      "epoch 14 batch 110/123 loss = 0.0466\n",
      "epoch 14 batch 111/123 loss = 0.0417\n",
      "epoch 14 batch 112/123 loss = 0.0386\n",
      "epoch 14 batch 113/123 loss = 0.0463\n",
      "epoch 14 batch 114/123 loss = 0.0549\n",
      "epoch 14 batch 115/123 loss = 0.0388\n",
      "epoch 14 batch 116/123 loss = 0.0458\n",
      "epoch 14 batch 117/123 loss = 0.0441\n",
      "epoch 14 batch 118/123 loss = 0.0376\n",
      "epoch 14 batch 119/123 loss = 0.0326\n",
      "epoch 14 batch 120/123 loss = 0.0393\n",
      "epoch 14 batch 121/123 loss = 0.0588\n",
      "epoch 14 batch 122/123 loss = 0.0418\n",
      "epoch 14 batch 29/123 average loss = 0.0435 last loss = 0.0493\n",
      "epoch 15 batch 1/123 loss = 0.0440\n",
      "epoch 15 batch 2/123 loss = 0.0425\n",
      "epoch 15 batch 3/123 loss = 0.0461\n",
      "epoch 15 batch 4/123 loss = 0.0432\n",
      "epoch 15 batch 5/123 loss = 0.0396\n",
      "epoch 15 batch 6/123 loss = 0.0446\n",
      "epoch 15 batch 7/123 loss = 0.0357\n",
      "epoch 15 batch 8/123 loss = 0.0478\n",
      "epoch 15 batch 9/123 loss = 0.0467\n",
      "epoch 15 batch 10/123 loss = 0.0365\n",
      "epoch 15 batch 11/123 loss = 0.0445\n",
      "epoch 15 batch 12/123 loss = 0.0429\n",
      "epoch 15 batch 13/123 loss = 0.0427\n",
      "epoch 15 batch 14/123 loss = 0.0381\n",
      "epoch 15 batch 15/123 loss = 0.0523\n",
      "epoch 15 batch 16/123 loss = 0.0477\n",
      "epoch 15 batch 17/123 loss = 0.0354\n",
      "epoch 15 batch 18/123 loss = 0.0412\n",
      "epoch 15 batch 19/123 loss = 0.0439\n",
      "epoch 15 batch 20/123 loss = 0.0452\n",
      "epoch 15 batch 21/123 loss = 0.0451\n",
      "epoch 15 batch 22/123 loss = 0.0409\n",
      "epoch 15 batch 23/123 loss = 0.0473\n",
      "epoch 15 batch 24/123 loss = 0.0424\n",
      "epoch 15 batch 25/123 loss = 0.0451\n",
      "epoch 15 batch 26/123 loss = 0.0415\n",
      "epoch 15 batch 27/123 loss = 0.0386\n",
      "epoch 15 batch 28/123 loss = 0.0448\n",
      "epoch 15 batch 29/123 loss = 0.0523\n",
      "epoch 15 batch 30/123 loss = 0.0392\n",
      "epoch 15 batch 31/123 loss = 0.0422\n",
      "epoch 15 batch 32/123 loss = 0.0463\n",
      "epoch 15 batch 33/123 loss = 0.0480\n",
      "epoch 15 batch 34/123 loss = 0.0470\n",
      "epoch 15 batch 35/123 loss = 0.0392\n",
      "epoch 15 batch 36/123 loss = 0.0450\n",
      "epoch 15 batch 37/123 loss = 0.0446\n",
      "epoch 15 batch 38/123 loss = 0.0477\n",
      "epoch 15 batch 39/123 loss = 0.0403\n",
      "epoch 15 batch 40/123 loss = 0.0442\n",
      "epoch 15 batch 41/123 loss = 0.0407\n",
      "epoch 15 batch 42/123 loss = 0.0394\n",
      "epoch 15 batch 43/123 loss = 0.0393\n",
      "epoch 15 batch 44/123 loss = 0.0371\n",
      "epoch 15 batch 45/123 loss = 0.0452\n",
      "epoch 15 batch 46/123 loss = 0.0404\n",
      "epoch 15 batch 47/123 loss = 0.0380\n",
      "epoch 15 batch 48/123 loss = 0.0394\n",
      "epoch 15 batch 49/123 loss = 0.0534\n",
      "epoch 15 batch 50/123 loss = 0.0441\n",
      "epoch 15 batch 51/123 loss = 0.0459\n",
      "epoch 15 batch 52/123 loss = 0.0352\n",
      "epoch 15 batch 53/123 loss = 0.0431\n",
      "epoch 15 batch 54/123 loss = 0.0455\n",
      "epoch 15 batch 55/123 loss = 0.0460\n",
      "epoch 15 batch 56/123 loss = 0.0380\n",
      "epoch 15 batch 57/123 loss = 0.0433\n",
      "epoch 15 batch 58/123 loss = 0.0379\n",
      "epoch 15 batch 59/123 loss = 0.0403\n",
      "epoch 15 batch 60/123 loss = 0.0348\n",
      "epoch 15 batch 61/123 loss = 0.0450\n",
      "epoch 15 batch 62/123 loss = 0.0355\n",
      "epoch 15 batch 63/123 loss = 0.0434\n",
      "epoch 15 batch 64/123 loss = 0.0365\n",
      "epoch 15 batch 65/123 loss = 0.0445\n",
      "epoch 15 batch 66/123 loss = 0.0402\n",
      "epoch 15 batch 67/123 loss = 0.0404\n",
      "epoch 15 batch 68/123 loss = 0.0400\n",
      "epoch 15 batch 69/123 loss = 0.0470\n",
      "epoch 15 batch 70/123 loss = 0.0446\n",
      "epoch 15 batch 71/123 loss = 0.0462\n",
      "epoch 15 batch 72/123 loss = 0.0421\n",
      "epoch 15 batch 73/123 loss = 0.0320\n",
      "epoch 15 batch 74/123 loss = 0.0364\n",
      "epoch 15 batch 75/123 loss = 0.0403\n",
      "epoch 15 batch 76/123 loss = 0.0452\n",
      "epoch 15 batch 77/123 loss = 0.0406\n",
      "epoch 15 batch 78/123 loss = 0.0391\n",
      "epoch 15 batch 79/123 loss = 0.0350\n",
      "epoch 15 batch 80/123 loss = 0.0404\n",
      "epoch 15 batch 81/123 loss = 0.0379\n",
      "epoch 15 batch 82/123 loss = 0.0455\n",
      "epoch 15 batch 83/123 loss = 0.0376\n",
      "epoch 15 batch 84/123 loss = 0.0494\n",
      "epoch 15 batch 85/123 loss = 0.0393\n",
      "epoch 15 batch 86/123 loss = 0.0462\n",
      "epoch 15 batch 87/123 loss = 0.0444\n",
      "epoch 15 batch 88/123 loss = 0.0421\n",
      "epoch 15 batch 89/123 loss = 0.0388\n",
      "epoch 15 batch 90/123 loss = 0.0391\n",
      "epoch 15 batch 91/123 loss = 0.0427\n",
      "epoch 15 batch 92/123 loss = 0.0409\n",
      "epoch 15 batch 93/123 loss = 0.0376\n",
      "epoch 15 batch 94/123 loss = 0.0443\n",
      "epoch 15 batch 95/123 loss = 0.0501\n",
      "epoch 15 batch 96/123 loss = 0.0389\n",
      "epoch 15 batch 97/123 loss = 0.0460\n",
      "epoch 15 batch 98/123 loss = 0.0341\n",
      "epoch 15 batch 99/123 loss = 0.0520\n",
      "epoch 15 batch 100/123 loss = 0.0388\n",
      "epoch 15 batch 101/123 loss = 0.0381\n",
      "epoch 15 batch 102/123 loss = 0.0389\n",
      "epoch 15 batch 103/123 loss = 0.0476\n",
      "epoch 15 batch 104/123 loss = 0.0440\n",
      "epoch 15 batch 105/123 loss = 0.0423\n",
      "epoch 15 batch 106/123 loss = 0.0425\n",
      "epoch 15 batch 107/123 loss = 0.0374\n",
      "epoch 15 batch 108/123 loss = 0.0451\n",
      "epoch 15 batch 109/123 loss = 0.0439\n",
      "epoch 15 batch 110/123 loss = 0.0407\n",
      "epoch 15 batch 111/123 loss = 0.0433\n",
      "epoch 15 batch 112/123 loss = 0.0465\n",
      "epoch 15 batch 113/123 loss = 0.0384\n",
      "epoch 15 batch 114/123 loss = 0.0451\n",
      "epoch 15 batch 115/123 loss = 0.0415\n",
      "epoch 15 batch 116/123 loss = 0.0468\n",
      "epoch 15 batch 117/123 loss = 0.0376\n",
      "epoch 15 batch 118/123 loss = 0.0349\n",
      "epoch 15 batch 119/123 loss = 0.0388\n",
      "epoch 15 batch 120/123 loss = 0.0400\n",
      "epoch 15 batch 121/123 loss = 0.0446\n",
      "epoch 15 batch 122/123 loss = 0.0422\n",
      "epoch 15 batch 29/123 average loss = 0.0422 last loss = 0.0411\n",
      "epoch 16 batch 1/123 loss = 0.0454\n",
      "epoch 16 batch 2/123 loss = 0.0382\n",
      "epoch 16 batch 3/123 loss = 0.0442\n",
      "epoch 16 batch 4/123 loss = 0.0406\n",
      "epoch 16 batch 5/123 loss = 0.0393\n",
      "epoch 16 batch 6/123 loss = 0.0391\n",
      "epoch 16 batch 7/123 loss = 0.0299\n",
      "epoch 16 batch 8/123 loss = 0.0382\n",
      "epoch 16 batch 9/123 loss = 0.0341\n",
      "epoch 16 batch 10/123 loss = 0.0343\n",
      "epoch 16 batch 11/123 loss = 0.0382\n",
      "epoch 16 batch 12/123 loss = 0.0403\n",
      "epoch 16 batch 13/123 loss = 0.0427\n",
      "epoch 16 batch 14/123 loss = 0.0345\n",
      "epoch 16 batch 15/123 loss = 0.0344\n",
      "epoch 16 batch 16/123 loss = 0.0389\n",
      "epoch 16 batch 17/123 loss = 0.0450\n",
      "epoch 16 batch 18/123 loss = 0.0335\n",
      "epoch 16 batch 19/123 loss = 0.0431\n",
      "epoch 16 batch 20/123 loss = 0.0434\n",
      "epoch 16 batch 21/123 loss = 0.0430\n",
      "epoch 16 batch 22/123 loss = 0.0438\n",
      "epoch 16 batch 23/123 loss = 0.0430\n",
      "epoch 16 batch 24/123 loss = 0.0446\n",
      "epoch 16 batch 25/123 loss = 0.0399\n",
      "epoch 16 batch 26/123 loss = 0.0368\n",
      "epoch 16 batch 27/123 loss = 0.0374\n",
      "epoch 16 batch 28/123 loss = 0.0376\n",
      "epoch 16 batch 29/123 loss = 0.0459\n",
      "epoch 16 batch 30/123 loss = 0.0498\n",
      "epoch 16 batch 31/123 loss = 0.0369\n",
      "epoch 16 batch 32/123 loss = 0.0439\n",
      "epoch 16 batch 33/123 loss = 0.0428\n",
      "epoch 16 batch 34/123 loss = 0.0392\n",
      "epoch 16 batch 35/123 loss = 0.0401\n",
      "epoch 16 batch 36/123 loss = 0.0396\n",
      "epoch 16 batch 37/123 loss = 0.0418\n",
      "epoch 16 batch 38/123 loss = 0.0410\n",
      "epoch 16 batch 39/123 loss = 0.0481\n",
      "epoch 16 batch 40/123 loss = 0.0465\n",
      "epoch 16 batch 41/123 loss = 0.0485\n",
      "epoch 16 batch 42/123 loss = 0.0420\n",
      "epoch 16 batch 43/123 loss = 0.0398\n",
      "epoch 16 batch 44/123 loss = 0.0423\n",
      "epoch 16 batch 45/123 loss = 0.0396\n",
      "epoch 16 batch 46/123 loss = 0.0389\n",
      "epoch 16 batch 47/123 loss = 0.0423\n",
      "epoch 16 batch 48/123 loss = 0.0410\n",
      "epoch 16 batch 49/123 loss = 0.0397\n",
      "epoch 16 batch 50/123 loss = 0.0350\n",
      "epoch 16 batch 51/123 loss = 0.0394\n",
      "epoch 16 batch 52/123 loss = 0.0328\n",
      "epoch 16 batch 53/123 loss = 0.0408\n",
      "epoch 16 batch 54/123 loss = 0.0364\n",
      "epoch 16 batch 55/123 loss = 0.0467\n",
      "epoch 16 batch 56/123 loss = 0.0351\n",
      "epoch 16 batch 57/123 loss = 0.0442\n",
      "epoch 16 batch 58/123 loss = 0.0464\n",
      "epoch 16 batch 59/123 loss = 0.0424\n",
      "epoch 16 batch 60/123 loss = 0.0439\n",
      "epoch 16 batch 61/123 loss = 0.0431\n",
      "epoch 16 batch 62/123 loss = 0.0383\n",
      "epoch 16 batch 63/123 loss = 0.0468\n",
      "epoch 16 batch 64/123 loss = 0.0400\n",
      "epoch 16 batch 65/123 loss = 0.0383\n",
      "epoch 16 batch 66/123 loss = 0.0391\n",
      "epoch 16 batch 67/123 loss = 0.0417\n",
      "epoch 16 batch 68/123 loss = 0.0404\n",
      "epoch 16 batch 69/123 loss = 0.0453\n",
      "epoch 16 batch 70/123 loss = 0.0415\n",
      "epoch 16 batch 71/123 loss = 0.0393\n",
      "epoch 16 batch 72/123 loss = 0.0463\n",
      "epoch 16 batch 73/123 loss = 0.0428\n",
      "epoch 16 batch 74/123 loss = 0.0374\n",
      "epoch 16 batch 75/123 loss = 0.0449\n",
      "epoch 16 batch 76/123 loss = 0.0433\n",
      "epoch 16 batch 77/123 loss = 0.0408\n",
      "epoch 16 batch 78/123 loss = 0.0383\n",
      "epoch 16 batch 79/123 loss = 0.0423\n",
      "epoch 16 batch 80/123 loss = 0.0403\n",
      "epoch 16 batch 81/123 loss = 0.0346\n",
      "epoch 16 batch 82/123 loss = 0.0403\n",
      "epoch 16 batch 83/123 loss = 0.0419\n",
      "epoch 16 batch 84/123 loss = 0.0489\n",
      "epoch 16 batch 85/123 loss = 0.0515\n",
      "epoch 16 batch 86/123 loss = 0.0428\n",
      "epoch 16 batch 87/123 loss = 0.0469\n",
      "epoch 16 batch 88/123 loss = 0.0434\n",
      "epoch 16 batch 89/123 loss = 0.0523\n",
      "epoch 16 batch 90/123 loss = 0.0363\n",
      "epoch 16 batch 91/123 loss = 0.0414\n",
      "epoch 16 batch 92/123 loss = 0.0370\n",
      "epoch 16 batch 93/123 loss = 0.0405\n",
      "epoch 16 batch 94/123 loss = 0.0426\n",
      "epoch 16 batch 95/123 loss = 0.0436\n",
      "epoch 16 batch 96/123 loss = 0.0442\n",
      "epoch 16 batch 97/123 loss = 0.0468\n",
      "epoch 16 batch 98/123 loss = 0.0400\n",
      "epoch 16 batch 99/123 loss = 0.0525\n",
      "epoch 16 batch 100/123 loss = 0.0370\n",
      "epoch 16 batch 101/123 loss = 0.0420\n",
      "epoch 16 batch 102/123 loss = 0.0367\n",
      "epoch 16 batch 103/123 loss = 0.0430\n",
      "epoch 16 batch 104/123 loss = 0.0395\n",
      "epoch 16 batch 105/123 loss = 0.0434\n",
      "epoch 16 batch 106/123 loss = 0.0362\n",
      "epoch 16 batch 107/123 loss = 0.0458\n",
      "epoch 16 batch 108/123 loss = 0.0435\n",
      "epoch 16 batch 109/123 loss = 0.0362\n",
      "epoch 16 batch 110/123 loss = 0.0389\n",
      "epoch 16 batch 111/123 loss = 0.0393\n",
      "epoch 16 batch 112/123 loss = 0.0358\n",
      "epoch 16 batch 113/123 loss = 0.0522\n",
      "epoch 16 batch 114/123 loss = 0.0390\n",
      "epoch 16 batch 115/123 loss = 0.0436\n",
      "epoch 16 batch 116/123 loss = 0.0465\n",
      "epoch 16 batch 117/123 loss = 0.0429\n",
      "epoch 16 batch 118/123 loss = 0.0412\n",
      "epoch 16 batch 119/123 loss = 0.0436\n",
      "epoch 16 batch 120/123 loss = 0.0408\n",
      "epoch 16 batch 121/123 loss = 0.0396\n",
      "epoch 16 batch 122/123 loss = 0.0376\n",
      "epoch 16 batch 29/123 average loss = 0.0413 last loss = 0.0421\n",
      "epoch 17 batch 1/123 loss = 0.0380\n",
      "epoch 17 batch 2/123 loss = 0.0404\n",
      "epoch 17 batch 3/123 loss = 0.0406\n",
      "epoch 17 batch 4/123 loss = 0.0489\n",
      "epoch 17 batch 5/123 loss = 0.0369\n",
      "epoch 17 batch 6/123 loss = 0.0415\n",
      "epoch 17 batch 7/123 loss = 0.0421\n",
      "epoch 17 batch 8/123 loss = 0.0424\n",
      "epoch 17 batch 9/123 loss = 0.0378\n",
      "epoch 17 batch 10/123 loss = 0.0427\n",
      "epoch 17 batch 11/123 loss = 0.0421\n",
      "epoch 17 batch 12/123 loss = 0.0339\n",
      "epoch 17 batch 13/123 loss = 0.0391\n",
      "epoch 17 batch 14/123 loss = 0.0418\n",
      "epoch 17 batch 15/123 loss = 0.0404\n",
      "epoch 17 batch 16/123 loss = 0.0461\n",
      "epoch 17 batch 17/123 loss = 0.0474\n",
      "epoch 17 batch 18/123 loss = 0.0368\n",
      "epoch 17 batch 19/123 loss = 0.0364\n",
      "epoch 17 batch 20/123 loss = 0.0391\n",
      "epoch 17 batch 21/123 loss = 0.0342\n",
      "epoch 17 batch 22/123 loss = 0.0385\n",
      "epoch 17 batch 23/123 loss = 0.0407\n",
      "epoch 17 batch 24/123 loss = 0.0419\n",
      "epoch 17 batch 25/123 loss = 0.0421\n",
      "epoch 17 batch 26/123 loss = 0.0417\n",
      "epoch 17 batch 27/123 loss = 0.0373\n",
      "epoch 17 batch 28/123 loss = 0.0350\n",
      "epoch 17 batch 29/123 loss = 0.0339\n",
      "epoch 17 batch 30/123 loss = 0.0375\n",
      "epoch 17 batch 31/123 loss = 0.0361\n",
      "epoch 17 batch 32/123 loss = 0.0395\n",
      "epoch 17 batch 33/123 loss = 0.0369\n",
      "epoch 17 batch 34/123 loss = 0.0321\n",
      "epoch 17 batch 35/123 loss = 0.0433\n",
      "epoch 17 batch 36/123 loss = 0.0416\n",
      "epoch 17 batch 37/123 loss = 0.0424\n",
      "epoch 17 batch 38/123 loss = 0.0401\n",
      "epoch 17 batch 39/123 loss = 0.0376\n",
      "epoch 17 batch 40/123 loss = 0.0392\n",
      "epoch 17 batch 41/123 loss = 0.0412\n",
      "epoch 17 batch 42/123 loss = 0.0453\n",
      "epoch 17 batch 43/123 loss = 0.0368\n",
      "epoch 17 batch 44/123 loss = 0.0401\n",
      "epoch 17 batch 45/123 loss = 0.0386\n",
      "epoch 17 batch 46/123 loss = 0.0416\n",
      "epoch 17 batch 47/123 loss = 0.0345\n",
      "epoch 17 batch 48/123 loss = 0.0395\n",
      "epoch 17 batch 49/123 loss = 0.0358\n",
      "epoch 17 batch 50/123 loss = 0.0459\n",
      "epoch 17 batch 51/123 loss = 0.0330\n",
      "epoch 17 batch 52/123 loss = 0.0404\n",
      "epoch 17 batch 53/123 loss = 0.0393\n",
      "epoch 17 batch 54/123 loss = 0.0390\n",
      "epoch 17 batch 55/123 loss = 0.0462\n",
      "epoch 17 batch 56/123 loss = 0.0410\n",
      "epoch 17 batch 57/123 loss = 0.0430\n",
      "epoch 17 batch 58/123 loss = 0.0436\n",
      "epoch 17 batch 59/123 loss = 0.0317\n",
      "epoch 17 batch 60/123 loss = 0.0443\n",
      "epoch 17 batch 61/123 loss = 0.0392\n",
      "epoch 17 batch 62/123 loss = 0.0404\n",
      "epoch 17 batch 63/123 loss = 0.0406\n",
      "epoch 17 batch 64/123 loss = 0.0433\n",
      "epoch 17 batch 65/123 loss = 0.0382\n",
      "epoch 17 batch 66/123 loss = 0.0384\n",
      "epoch 17 batch 67/123 loss = 0.0379\n",
      "epoch 17 batch 68/123 loss = 0.0434\n",
      "epoch 17 batch 69/123 loss = 0.0403\n",
      "epoch 17 batch 70/123 loss = 0.0321\n",
      "epoch 17 batch 71/123 loss = 0.0401\n",
      "epoch 17 batch 72/123 loss = 0.0379\n",
      "epoch 17 batch 73/123 loss = 0.0363\n",
      "epoch 17 batch 74/123 loss = 0.0407\n",
      "epoch 17 batch 75/123 loss = 0.0416\n",
      "epoch 17 batch 76/123 loss = 0.0425\n",
      "epoch 17 batch 77/123 loss = 0.0375\n",
      "epoch 17 batch 78/123 loss = 0.0387\n",
      "epoch 17 batch 79/123 loss = 0.0445\n",
      "epoch 17 batch 80/123 loss = 0.0415\n",
      "epoch 17 batch 81/123 loss = 0.0408\n",
      "epoch 17 batch 82/123 loss = 0.0355\n",
      "epoch 17 batch 83/123 loss = 0.0405\n",
      "epoch 17 batch 84/123 loss = 0.0358\n",
      "epoch 17 batch 85/123 loss = 0.0539\n",
      "epoch 17 batch 86/123 loss = 0.0433\n",
      "epoch 17 batch 87/123 loss = 0.0379\n",
      "epoch 17 batch 88/123 loss = 0.0391\n",
      "epoch 17 batch 89/123 loss = 0.0379\n",
      "epoch 17 batch 90/123 loss = 0.0375\n",
      "epoch 17 batch 91/123 loss = 0.0418\n",
      "epoch 17 batch 92/123 loss = 0.0415\n",
      "epoch 17 batch 93/123 loss = 0.0391\n",
      "epoch 17 batch 94/123 loss = 0.0382\n",
      "epoch 17 batch 95/123 loss = 0.0407\n",
      "epoch 17 batch 96/123 loss = 0.0355\n",
      "epoch 17 batch 97/123 loss = 0.0376\n",
      "epoch 17 batch 98/123 loss = 0.0352\n",
      "epoch 17 batch 99/123 loss = 0.0366\n",
      "epoch 17 batch 100/123 loss = 0.0381\n",
      "epoch 17 batch 101/123 loss = 0.0383\n",
      "epoch 17 batch 102/123 loss = 0.0450\n",
      "epoch 17 batch 103/123 loss = 0.0399\n",
      "epoch 17 batch 104/123 loss = 0.0413\n",
      "epoch 17 batch 105/123 loss = 0.0353\n",
      "epoch 17 batch 106/123 loss = 0.0354\n",
      "epoch 17 batch 107/123 loss = 0.0374\n",
      "epoch 17 batch 108/123 loss = 0.0412\n",
      "epoch 17 batch 109/123 loss = 0.0389\n",
      "epoch 17 batch 110/123 loss = 0.0420\n",
      "epoch 17 batch 111/123 loss = 0.0410\n",
      "epoch 17 batch 112/123 loss = 0.0372\n",
      "epoch 17 batch 113/123 loss = 0.0463\n",
      "epoch 17 batch 114/123 loss = 0.0383\n",
      "epoch 17 batch 115/123 loss = 0.0514\n",
      "epoch 17 batch 116/123 loss = 0.0466\n",
      "epoch 17 batch 117/123 loss = 0.0451\n",
      "epoch 17 batch 118/123 loss = 0.0384\n",
      "epoch 17 batch 119/123 loss = 0.0413\n",
      "epoch 17 batch 120/123 loss = 0.0452\n",
      "epoch 17 batch 121/123 loss = 0.0378\n",
      "epoch 17 batch 122/123 loss = 0.0470\n",
      "epoch 17 batch 29/123 average loss = 0.0400 last loss = 0.0400\n",
      "epoch 18 batch 1/123 loss = 0.0397\n",
      "epoch 18 batch 2/123 loss = 0.0380\n",
      "epoch 18 batch 3/123 loss = 0.0370\n",
      "epoch 18 batch 4/123 loss = 0.0391\n",
      "epoch 18 batch 5/123 loss = 0.0367\n",
      "epoch 18 batch 6/123 loss = 0.0366\n",
      "epoch 18 batch 7/123 loss = 0.0382\n",
      "epoch 18 batch 8/123 loss = 0.0338\n",
      "epoch 18 batch 9/123 loss = 0.0388\n",
      "epoch 18 batch 10/123 loss = 0.0375\n",
      "epoch 18 batch 11/123 loss = 0.0367\n",
      "epoch 18 batch 12/123 loss = 0.0361\n",
      "epoch 18 batch 13/123 loss = 0.0402\n",
      "epoch 18 batch 14/123 loss = 0.0359\n",
      "epoch 18 batch 15/123 loss = 0.0385\n",
      "epoch 18 batch 16/123 loss = 0.0402\n",
      "epoch 18 batch 17/123 loss = 0.0396\n",
      "epoch 18 batch 18/123 loss = 0.0314\n",
      "epoch 18 batch 19/123 loss = 0.0340\n",
      "epoch 18 batch 20/123 loss = 0.0371\n",
      "epoch 18 batch 21/123 loss = 0.0369\n",
      "epoch 18 batch 22/123 loss = 0.0347\n",
      "epoch 18 batch 23/123 loss = 0.0412\n",
      "epoch 18 batch 24/123 loss = 0.0425\n",
      "epoch 18 batch 25/123 loss = 0.0341\n",
      "epoch 18 batch 26/123 loss = 0.0371\n",
      "epoch 18 batch 27/123 loss = 0.0426\n",
      "epoch 18 batch 28/123 loss = 0.0357\n",
      "epoch 18 batch 29/123 loss = 0.0363\n",
      "epoch 18 batch 30/123 loss = 0.0417\n",
      "epoch 18 batch 31/123 loss = 0.0359\n",
      "epoch 18 batch 32/123 loss = 0.0351\n",
      "epoch 18 batch 33/123 loss = 0.0360\n",
      "epoch 18 batch 34/123 loss = 0.0393\n",
      "epoch 18 batch 35/123 loss = 0.0390\n",
      "epoch 18 batch 36/123 loss = 0.0343\n",
      "epoch 18 batch 37/123 loss = 0.0424\n",
      "epoch 18 batch 38/123 loss = 0.0354\n",
      "epoch 18 batch 39/123 loss = 0.0343\n",
      "epoch 18 batch 40/123 loss = 0.0320\n",
      "epoch 18 batch 41/123 loss = 0.0411\n",
      "epoch 18 batch 42/123 loss = 0.0377\n",
      "epoch 18 batch 43/123 loss = 0.0430\n",
      "epoch 18 batch 44/123 loss = 0.0401\n",
      "epoch 18 batch 45/123 loss = 0.0349\n",
      "epoch 18 batch 46/123 loss = 0.0395\n",
      "epoch 18 batch 47/123 loss = 0.0334\n",
      "epoch 18 batch 48/123 loss = 0.0315\n",
      "epoch 18 batch 49/123 loss = 0.0411\n",
      "epoch 18 batch 50/123 loss = 0.0319\n",
      "epoch 18 batch 51/123 loss = 0.0374\n",
      "epoch 18 batch 52/123 loss = 0.0392\n",
      "epoch 18 batch 53/123 loss = 0.0377\n",
      "epoch 18 batch 54/123 loss = 0.0343\n",
      "epoch 18 batch 55/123 loss = 0.0363\n",
      "epoch 18 batch 56/123 loss = 0.0401\n",
      "epoch 18 batch 57/123 loss = 0.0393\n",
      "epoch 18 batch 58/123 loss = 0.0312\n",
      "epoch 18 batch 59/123 loss = 0.0339\n",
      "epoch 18 batch 60/123 loss = 0.0495\n",
      "epoch 18 batch 61/123 loss = 0.0355\n",
      "epoch 18 batch 62/123 loss = 0.0340\n",
      "epoch 18 batch 63/123 loss = 0.0430\n",
      "epoch 18 batch 64/123 loss = 0.0346\n",
      "epoch 18 batch 65/123 loss = 0.0395\n",
      "epoch 18 batch 66/123 loss = 0.0389\n",
      "epoch 18 batch 67/123 loss = 0.0350\n",
      "epoch 18 batch 68/123 loss = 0.0400\n",
      "epoch 18 batch 69/123 loss = 0.0387\n",
      "epoch 18 batch 70/123 loss = 0.0381\n",
      "epoch 18 batch 71/123 loss = 0.0397\n",
      "epoch 18 batch 72/123 loss = 0.0430\n",
      "epoch 18 batch 73/123 loss = 0.0397\n",
      "epoch 18 batch 74/123 loss = 0.0441\n",
      "epoch 18 batch 75/123 loss = 0.0379\n",
      "epoch 18 batch 76/123 loss = 0.0420\n",
      "epoch 18 batch 77/123 loss = 0.0380\n",
      "epoch 18 batch 78/123 loss = 0.0436\n",
      "epoch 18 batch 79/123 loss = 0.0438\n",
      "epoch 18 batch 80/123 loss = 0.0468\n",
      "epoch 18 batch 81/123 loss = 0.0427\n",
      "epoch 18 batch 82/123 loss = 0.0350\n",
      "epoch 18 batch 83/123 loss = 0.0400\n",
      "epoch 18 batch 84/123 loss = 0.0385\n",
      "epoch 18 batch 85/123 loss = 0.0404\n",
      "epoch 18 batch 86/123 loss = 0.0375\n",
      "epoch 18 batch 87/123 loss = 0.0332\n",
      "epoch 18 batch 88/123 loss = 0.0475\n",
      "epoch 18 batch 89/123 loss = 0.0387\n",
      "epoch 18 batch 90/123 loss = 0.0414\n",
      "epoch 18 batch 91/123 loss = 0.0459\n",
      "epoch 18 batch 92/123 loss = 0.0495\n",
      "epoch 18 batch 93/123 loss = 0.0399\n",
      "epoch 18 batch 94/123 loss = 0.0382\n",
      "epoch 18 batch 95/123 loss = 0.0423\n",
      "epoch 18 batch 96/123 loss = 0.0366\n",
      "epoch 18 batch 97/123 loss = 0.0351\n",
      "epoch 18 batch 98/123 loss = 0.0415\n",
      "epoch 18 batch 99/123 loss = 0.0447\n",
      "epoch 18 batch 100/123 loss = 0.0410\n",
      "epoch 18 batch 101/123 loss = 0.0422\n",
      "epoch 18 batch 102/123 loss = 0.0411\n",
      "epoch 18 batch 103/123 loss = 0.0404\n",
      "epoch 18 batch 104/123 loss = 0.0497\n",
      "epoch 18 batch 105/123 loss = 0.0479\n",
      "epoch 18 batch 106/123 loss = 0.0370\n",
      "epoch 18 batch 107/123 loss = 0.0373\n",
      "epoch 18 batch 108/123 loss = 0.0448\n",
      "epoch 18 batch 109/123 loss = 0.0439\n",
      "epoch 18 batch 110/123 loss = 0.0376\n",
      "epoch 18 batch 111/123 loss = 0.0362\n",
      "epoch 18 batch 112/123 loss = 0.0427\n",
      "epoch 18 batch 113/123 loss = 0.0395\n",
      "epoch 18 batch 114/123 loss = 0.0372\n",
      "epoch 18 batch 115/123 loss = 0.0374\n",
      "epoch 18 batch 116/123 loss = 0.0404\n",
      "epoch 18 batch 117/123 loss = 0.0487\n",
      "epoch 18 batch 118/123 loss = 0.0425\n",
      "epoch 18 batch 119/123 loss = 0.0379\n",
      "epoch 18 batch 120/123 loss = 0.0370\n",
      "epoch 18 batch 121/123 loss = 0.0409\n",
      "epoch 18 batch 122/123 loss = 0.0349\n",
      "epoch 18 batch 29/123 average loss = 0.0389 last loss = 0.0351\n",
      "epoch 19 batch 1/123 loss = 0.0366\n",
      "epoch 19 batch 2/123 loss = 0.0358\n",
      "epoch 19 batch 3/123 loss = 0.0438\n",
      "epoch 19 batch 4/123 loss = 0.0395\n",
      "epoch 19 batch 5/123 loss = 0.0341\n",
      "epoch 19 batch 6/123 loss = 0.0367\n",
      "epoch 19 batch 7/123 loss = 0.0352\n",
      "epoch 19 batch 8/123 loss = 0.0336\n",
      "epoch 19 batch 9/123 loss = 0.0344\n",
      "epoch 19 batch 10/123 loss = 0.0368\n",
      "epoch 19 batch 11/123 loss = 0.0319\n",
      "epoch 19 batch 12/123 loss = 0.0378\n",
      "epoch 19 batch 13/123 loss = 0.0333\n",
      "epoch 19 batch 14/123 loss = 0.0332\n",
      "epoch 19 batch 15/123 loss = 0.0358\n",
      "epoch 19 batch 16/123 loss = 0.0357\n",
      "epoch 19 batch 17/123 loss = 0.0372\n",
      "epoch 19 batch 18/123 loss = 0.0411\n",
      "epoch 19 batch 19/123 loss = 0.0457\n",
      "epoch 19 batch 20/123 loss = 0.0366\n",
      "epoch 19 batch 21/123 loss = 0.0345\n",
      "epoch 19 batch 22/123 loss = 0.0367\n",
      "epoch 19 batch 23/123 loss = 0.0397\n",
      "epoch 19 batch 24/123 loss = 0.0312\n",
      "epoch 19 batch 25/123 loss = 0.0335\n",
      "epoch 19 batch 26/123 loss = 0.0349\n",
      "epoch 19 batch 27/123 loss = 0.0387\n",
      "epoch 19 batch 28/123 loss = 0.0390\n",
      "epoch 19 batch 29/123 loss = 0.0378\n",
      "epoch 19 batch 30/123 loss = 0.0338\n",
      "epoch 19 batch 31/123 loss = 0.0440\n",
      "epoch 19 batch 32/123 loss = 0.0353\n",
      "epoch 19 batch 33/123 loss = 0.0420\n",
      "epoch 19 batch 34/123 loss = 0.0432\n",
      "epoch 19 batch 35/123 loss = 0.0373\n",
      "epoch 19 batch 36/123 loss = 0.0349\n",
      "epoch 19 batch 37/123 loss = 0.0379\n",
      "epoch 19 batch 38/123 loss = 0.0335\n",
      "epoch 19 batch 39/123 loss = 0.0398\n",
      "epoch 19 batch 40/123 loss = 0.0404\n",
      "epoch 19 batch 41/123 loss = 0.0401\n",
      "epoch 19 batch 42/123 loss = 0.0390\n",
      "epoch 19 batch 43/123 loss = 0.0360\n",
      "epoch 19 batch 44/123 loss = 0.0357\n",
      "epoch 19 batch 45/123 loss = 0.0413\n",
      "epoch 19 batch 46/123 loss = 0.0403\n",
      "epoch 19 batch 47/123 loss = 0.0406\n",
      "epoch 19 batch 48/123 loss = 0.0347\n",
      "epoch 19 batch 49/123 loss = 0.0396\n",
      "epoch 19 batch 50/123 loss = 0.0336\n",
      "epoch 19 batch 51/123 loss = 0.0288\n",
      "epoch 19 batch 52/123 loss = 0.0400\n",
      "epoch 19 batch 53/123 loss = 0.0372\n",
      "epoch 19 batch 54/123 loss = 0.0348\n",
      "epoch 19 batch 55/123 loss = 0.0369\n",
      "epoch 19 batch 56/123 loss = 0.0392\n",
      "epoch 19 batch 57/123 loss = 0.0333\n",
      "epoch 19 batch 58/123 loss = 0.0416\n",
      "epoch 19 batch 59/123 loss = 0.0431\n",
      "epoch 19 batch 60/123 loss = 0.0367\n",
      "epoch 19 batch 61/123 loss = 0.0450\n",
      "epoch 19 batch 62/123 loss = 0.0376\n",
      "epoch 19 batch 63/123 loss = 0.0306\n",
      "epoch 19 batch 64/123 loss = 0.0369\n",
      "epoch 19 batch 65/123 loss = 0.0334\n",
      "epoch 19 batch 66/123 loss = 0.0418\n",
      "epoch 19 batch 67/123 loss = 0.0364\n",
      "epoch 19 batch 68/123 loss = 0.0342\n",
      "epoch 19 batch 69/123 loss = 0.0364\n",
      "epoch 19 batch 70/123 loss = 0.0375\n",
      "epoch 19 batch 71/123 loss = 0.0388\n",
      "epoch 19 batch 72/123 loss = 0.0400\n",
      "epoch 19 batch 73/123 loss = 0.0367\n",
      "epoch 19 batch 74/123 loss = 0.0415\n",
      "epoch 19 batch 75/123 loss = 0.0377\n",
      "epoch 19 batch 76/123 loss = 0.0349\n",
      "epoch 19 batch 77/123 loss = 0.0329\n",
      "epoch 19 batch 78/123 loss = 0.0419\n",
      "epoch 19 batch 79/123 loss = 0.0351\n",
      "epoch 19 batch 80/123 loss = 0.0373\n",
      "epoch 19 batch 81/123 loss = 0.0418\n",
      "epoch 19 batch 82/123 loss = 0.0353\n",
      "epoch 19 batch 83/123 loss = 0.0414\n",
      "epoch 19 batch 84/123 loss = 0.0443\n",
      "epoch 19 batch 85/123 loss = 0.0369\n",
      "epoch 19 batch 86/123 loss = 0.0286\n",
      "epoch 19 batch 87/123 loss = 0.0511\n",
      "epoch 19 batch 88/123 loss = 0.0463\n",
      "epoch 19 batch 89/123 loss = 0.0383\n",
      "epoch 19 batch 90/123 loss = 0.0409\n",
      "epoch 19 batch 91/123 loss = 0.0373\n",
      "epoch 19 batch 92/123 loss = 0.0380\n",
      "epoch 19 batch 93/123 loss = 0.0443\n",
      "epoch 19 batch 94/123 loss = 0.0345\n",
      "epoch 19 batch 95/123 loss = 0.0399\n",
      "epoch 19 batch 96/123 loss = 0.0396\n",
      "epoch 19 batch 97/123 loss = 0.0332\n",
      "epoch 19 batch 98/123 loss = 0.0418\n",
      "epoch 19 batch 99/123 loss = 0.0454\n",
      "epoch 19 batch 100/123 loss = 0.0336\n",
      "epoch 19 batch 101/123 loss = 0.0424\n",
      "epoch 19 batch 102/123 loss = 0.0366\n",
      "epoch 19 batch 103/123 loss = 0.0366\n",
      "epoch 19 batch 104/123 loss = 0.0379\n",
      "epoch 19 batch 105/123 loss = 0.0369\n",
      "epoch 19 batch 106/123 loss = 0.0490\n",
      "epoch 19 batch 107/123 loss = 0.0449\n",
      "epoch 19 batch 108/123 loss = 0.0490\n",
      "epoch 19 batch 109/123 loss = 0.0437\n",
      "epoch 19 batch 110/123 loss = 0.0450\n",
      "epoch 19 batch 111/123 loss = 0.0389\n",
      "epoch 19 batch 112/123 loss = 0.0459\n",
      "epoch 19 batch 113/123 loss = 0.0415\n",
      "epoch 19 batch 114/123 loss = 0.0410\n",
      "epoch 19 batch 115/123 loss = 0.0419\n",
      "epoch 19 batch 116/123 loss = 0.0399\n",
      "epoch 19 batch 117/123 loss = 0.0398\n",
      "epoch 19 batch 118/123 loss = 0.0381\n",
      "epoch 19 batch 119/123 loss = 0.0377\n",
      "epoch 19 batch 120/123 loss = 0.0393\n",
      "epoch 19 batch 121/123 loss = 0.0380\n",
      "epoch 19 batch 122/123 loss = 0.0361\n",
      "epoch 19 batch 29/123 average loss = 0.0383 last loss = 0.0334\n",
      "epoch 20 batch 1/123 loss = 0.0414\n",
      "epoch 20 batch 2/123 loss = 0.0413\n",
      "epoch 20 batch 3/123 loss = 0.0386\n",
      "epoch 20 batch 4/123 loss = 0.0429\n",
      "epoch 20 batch 5/123 loss = 0.0391\n",
      "epoch 20 batch 6/123 loss = 0.0302\n",
      "epoch 20 batch 7/123 loss = 0.0350\n",
      "epoch 20 batch 8/123 loss = 0.0376\n",
      "epoch 20 batch 9/123 loss = 0.0391\n",
      "epoch 20 batch 10/123 loss = 0.0428\n",
      "epoch 20 batch 11/123 loss = 0.0297\n",
      "epoch 20 batch 12/123 loss = 0.0336\n",
      "epoch 20 batch 13/123 loss = 0.0409\n",
      "epoch 20 batch 14/123 loss = 0.0415\n",
      "epoch 20 batch 15/123 loss = 0.0388\n",
      "epoch 20 batch 16/123 loss = 0.0350\n",
      "epoch 20 batch 17/123 loss = 0.0433\n",
      "epoch 20 batch 18/123 loss = 0.0386\n",
      "epoch 20 batch 19/123 loss = 0.0369\n",
      "epoch 20 batch 20/123 loss = 0.0375\n",
      "epoch 20 batch 21/123 loss = 0.0399\n",
      "epoch 20 batch 22/123 loss = 0.0378\n",
      "epoch 20 batch 23/123 loss = 0.0369\n",
      "epoch 20 batch 24/123 loss = 0.0331\n",
      "epoch 20 batch 25/123 loss = 0.0319\n",
      "epoch 20 batch 26/123 loss = 0.0335\n",
      "epoch 20 batch 27/123 loss = 0.0344\n",
      "epoch 20 batch 28/123 loss = 0.0335\n",
      "epoch 20 batch 29/123 loss = 0.0356\n",
      "epoch 20 batch 30/123 loss = 0.0432\n",
      "epoch 20 batch 31/123 loss = 0.0390\n",
      "epoch 20 batch 32/123 loss = 0.0390\n",
      "epoch 20 batch 33/123 loss = 0.0337\n",
      "epoch 20 batch 34/123 loss = 0.0385\n",
      "epoch 20 batch 35/123 loss = 0.0374\n",
      "epoch 20 batch 36/123 loss = 0.0388\n",
      "epoch 20 batch 37/123 loss = 0.0374\n",
      "epoch 20 batch 38/123 loss = 0.0407\n",
      "epoch 20 batch 39/123 loss = 0.0378\n",
      "epoch 20 batch 40/123 loss = 0.0382\n",
      "epoch 20 batch 41/123 loss = 0.0379\n",
      "epoch 20 batch 42/123 loss = 0.0360\n",
      "epoch 20 batch 43/123 loss = 0.0316\n",
      "epoch 20 batch 44/123 loss = 0.0348\n",
      "epoch 20 batch 45/123 loss = 0.0305\n",
      "epoch 20 batch 46/123 loss = 0.0353\n",
      "epoch 20 batch 47/123 loss = 0.0342\n",
      "epoch 20 batch 48/123 loss = 0.0316\n",
      "epoch 20 batch 49/123 loss = 0.0346\n",
      "epoch 20 batch 50/123 loss = 0.0307\n",
      "epoch 20 batch 51/123 loss = 0.0377\n",
      "epoch 20 batch 52/123 loss = 0.0324\n",
      "epoch 20 batch 53/123 loss = 0.0346\n",
      "epoch 20 batch 54/123 loss = 0.0316\n",
      "epoch 20 batch 55/123 loss = 0.0404\n",
      "epoch 20 batch 56/123 loss = 0.0439\n",
      "epoch 20 batch 57/123 loss = 0.0361\n",
      "epoch 20 batch 58/123 loss = 0.0402\n",
      "epoch 20 batch 59/123 loss = 0.0367\n",
      "epoch 20 batch 60/123 loss = 0.0384\n",
      "epoch 20 batch 61/123 loss = 0.0381\n",
      "epoch 20 batch 62/123 loss = 0.0365\n",
      "epoch 20 batch 63/123 loss = 0.0353\n",
      "epoch 20 batch 64/123 loss = 0.0334\n",
      "epoch 20 batch 65/123 loss = 0.0435\n",
      "epoch 20 batch 66/123 loss = 0.0356\n",
      "epoch 20 batch 67/123 loss = 0.0440\n",
      "epoch 20 batch 68/123 loss = 0.0297\n",
      "epoch 20 batch 69/123 loss = 0.0373\n",
      "epoch 20 batch 70/123 loss = 0.0442\n",
      "epoch 20 batch 71/123 loss = 0.0378\n",
      "epoch 20 batch 72/123 loss = 0.0343\n",
      "epoch 20 batch 73/123 loss = 0.0361\n",
      "epoch 20 batch 74/123 loss = 0.0424\n",
      "epoch 20 batch 75/123 loss = 0.0404\n",
      "epoch 20 batch 76/123 loss = 0.0357\n",
      "epoch 20 batch 77/123 loss = 0.0333\n",
      "epoch 20 batch 78/123 loss = 0.0374\n",
      "epoch 20 batch 79/123 loss = 0.0405\n",
      "epoch 20 batch 80/123 loss = 0.0404\n",
      "epoch 20 batch 81/123 loss = 0.0403\n",
      "epoch 20 batch 82/123 loss = 0.0322\n",
      "epoch 20 batch 83/123 loss = 0.0443\n",
      "epoch 20 batch 84/123 loss = 0.0348\n",
      "epoch 20 batch 85/123 loss = 0.0367\n",
      "epoch 20 batch 86/123 loss = 0.0365\n",
      "epoch 20 batch 87/123 loss = 0.0450\n",
      "epoch 20 batch 88/123 loss = 0.0360\n",
      "epoch 20 batch 89/123 loss = 0.0335\n",
      "epoch 20 batch 90/123 loss = 0.0321\n",
      "epoch 20 batch 91/123 loss = 0.0378\n",
      "epoch 20 batch 92/123 loss = 0.0323\n",
      "epoch 20 batch 93/123 loss = 0.0387\n",
      "epoch 20 batch 94/123 loss = 0.0443\n",
      "epoch 20 batch 95/123 loss = 0.0368\n",
      "epoch 20 batch 96/123 loss = 0.0381\n",
      "epoch 20 batch 97/123 loss = 0.0346\n",
      "epoch 20 batch 98/123 loss = 0.0416\n",
      "epoch 20 batch 99/123 loss = 0.0363\n",
      "epoch 20 batch 100/123 loss = 0.0350\n",
      "epoch 20 batch 101/123 loss = 0.0391\n",
      "epoch 20 batch 102/123 loss = 0.0346\n",
      "epoch 20 batch 103/123 loss = 0.0391\n",
      "epoch 20 batch 104/123 loss = 0.0371\n",
      "epoch 20 batch 105/123 loss = 0.0363\n",
      "epoch 20 batch 106/123 loss = 0.0357\n",
      "epoch 20 batch 107/123 loss = 0.0403\n",
      "epoch 20 batch 108/123 loss = 0.0396\n",
      "epoch 20 batch 109/123 loss = 0.0375\n",
      "epoch 20 batch 110/123 loss = 0.0308\n",
      "epoch 20 batch 111/123 loss = 0.0446\n",
      "epoch 20 batch 112/123 loss = 0.0360\n",
      "epoch 20 batch 113/123 loss = 0.0342\n",
      "epoch 20 batch 114/123 loss = 0.0369\n",
      "epoch 20 batch 115/123 loss = 0.0423\n",
      "epoch 20 batch 116/123 loss = 0.0377\n",
      "epoch 20 batch 117/123 loss = 0.0374\n",
      "epoch 20 batch 118/123 loss = 0.0397\n",
      "epoch 20 batch 119/123 loss = 0.0403\n",
      "epoch 20 batch 120/123 loss = 0.0341\n",
      "epoch 20 batch 121/123 loss = 0.0387\n",
      "epoch 20 batch 122/123 loss = 0.0336\n",
      "epoch 20 batch 29/123 average loss = 0.0372 last loss = 0.0332\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    current_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(current_device)\n",
    "    data_loader = load_data('/kaggle/working/datasets/cityscapes.pth')\n",
    "    unet = UNet().to(current_device)\n",
    "    mse_loss = nn.MSELoss().to(current_device)\n",
    "    optimizer_adam = optim.Adam(unet.parameters(), lr=params.get(\"lr\"), betas=params.get(\"betas\"))\n",
    "    unet = trainer(unet, data_loader, optimizer_adam, mse_loss, params.get('epoch'), device=current_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2917.090965,
   "end_time": "2022-05-26T10:46:09.440038",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-26T09:57:32.349073",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
